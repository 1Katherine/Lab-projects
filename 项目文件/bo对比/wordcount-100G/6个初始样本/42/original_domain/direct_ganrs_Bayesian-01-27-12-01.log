nohup: 忽略输入
/usr/local/home/yyq/bo/ganrs_bo
=============== start wordcount-100G ===============
2022年 01月 27日 星期四 08:57:34 CST
=============== start wordcount-100G ===============
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo/config/wordcount-100G" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo/logs.json" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo/generationConf.csv" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo/ganrs_target.png" 的文件状态(stat): 没有那个文件或目录
================= config1 =================
2022年 01月 27日 星期四 08:57:35 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (94679) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (268622) - 没有那个进程
--sampleType = firstngroup	 --ganrsGroup = 6	 --niters = 44	 --initFile = /usr/local/home/yyq/bo/ganrs_bo/wordcount-100G-GAN-42.csv
vital_params_name = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
vital_params_list = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold', 'runtime']
取出前6个样本
取出前两组样本作为初始样本：, shape = (6, 28)
interations：44
------------使用ganrs生成初始样本点------------
Tmax = 157.14285714285714
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[3.20000000e+01 6.40000000e+01]
 [0.00000000e+00 1.00000000e+00]
 [0.00000000e+00 1.00000000e+00]
 [2.00000000e+02 5.00000000e+02]
 [1.00000000e+00 4.00000000e+00]
 [4.00000000e+00 8.00000000e+00]
 [3.00000000e+00 7.00000000e+00]
 [3.84000000e+02 8.77000000e+02]
 [3.20000000e+01 1.28000000e+02]
 [3.20000000e+01 1.28000000e+02]
 [4.00000000e+00 1.00000000e+01]
 [1.07374157e+09 2.14748314e+09]
 [5.00000000e-01 9.00000000e-01]
 [0.00000000e+00 1.00000000e+00]
 [5.12000000e+02 1.02400000e+03]
 [5.00000000e-01 9.00000000e-01]
 [0.00000000e+00 1.00000000e+00]
 [1.07374182e+09 2.14748365e+09]
 [1.07374182e+09 2.14748365e+09]
 [2.40000000e+01 7.20000000e+01]
 [0.00000000e+00 1.00000000e+00]
 [5.00000000e+02 1.00000000e+03]
 [0.00000000e+00 1.00000000e+00]
 [1.60000000e+01 4.80000000e+01]
 [1.00000000e+00 5.00000000e+00]
 [1.50000000e+02 3.50000000e+02]
 [1.00000000e+00 4.00000000e+00]]
before probe, param.shape = (6, 27)
before probe, target = (6,)
black_box_function conf = spark.broadcast.blockSize
black_box_function conf = spark.broadcast.checksum
black_box_function conf = spark.broadcast.compress
black_box_function conf = spark.default.parallelism
black_box_function conf = spark.executor.cores
black_box_function conf = spark.executor.instances
black_box_function conf = spark.executor.memory
black_box_function conf = spark.executor.memoryOverhead
black_box_function conf = spark.kryoserializer.buffer
black_box_function conf = spark.kryoserializer.buffer.max
black_box_function conf = spark.locality.wait
black_box_function conf = spark.maxRemoteBlockSizeFetchToMem
black_box_function conf = spark.memory.fraction
black_box_function conf = spark.memory.offHeap.enabled
black_box_function conf = spark.memory.offHeap.size
black_box_function conf = spark.memory.storageFraction
black_box_function conf = spark.rdd.compress
black_box_function conf = spark.reducer.maxBlocksInFlightPerAddress
black_box_function conf = spark.reducer.maxReqsInFlight
black_box_function conf = spark.reducer.maxSizeInFlight
black_box_function conf = spark.scheduler.mode
black_box_function conf = spark.scheduler.revive.interval
black_box_function conf = spark.shuffle.compress
black_box_function conf = spark.shuffle.file.buffer
black_box_function conf = spark.shuffle.io.numConnectionsPerPeer
black_box_function conf = spark.shuffle.sort.bypassMergeThreshold
black_box_function conf = spark.storage.memoryMapThreshold
probe说：没见过！帮你计算target！x = [5.33193788e+01 4.07692862e-01 4.08721627e-01 4.28573144e+02
 2.61681428e+00 7.44145873e+00 5.98991718e+00 7.76821731e+02
 3.91799337e+01 1.10590173e+02 5.96060994e+00 1.82095099e+09
 5.81948713e-01 3.17771678e-01 7.90174229e+02 6.43787850e-01
 1.78195136e-01 1.65623174e+09 1.20597168e+09 2.84707905e+01
 7.42507504e-01 5.95686389e+02 8.94925600e-01 4.73415083e+01
 3.02497738e+00 2.51394667e+02 1.39051369e+00] target = -258.852
x_probe target = -258.852
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[3.20000000e+01 5.04600000e+01]
 [6.20000000e-01 1.00000000e+00]
 [0.00000000e+00 3.80000000e-01]
 [2.28500000e+02 4.71500000e+02]
 [1.81000000e+00 4.00000000e+00]
 [5.43000000e+00 8.00000000e+00]
 [4.43000000e+00 7.00000000e+00]
 [3.84000000e+02 6.75290000e+02]
 [4.74200000e+01 1.24580000e+02]
 [8.31200000e+01 1.28000000e+02]
 [7.72000000e+00 1.00000000e+01]
 [1.34411514e+09 2.14748314e+09]
 [5.00000000e-01 6.52000000e-01]
 [0.00000000e+00 3.80000000e-01]
 [5.12000000e+02 1.68960000e+02]
 [6.43000000e-01 9.00000000e-01]
 [0.00000000e+00 3.80000000e-01]
 [1.62162951e+09 2.14748365e+09]
 [1.07374182e+09 1.57594519e+09]
 [2.40000000e+01 6.11400000e+01]
 [6.20000000e-01 1.00000000e+00]
 [5.67450000e+02 9.70550000e+02]
 [6.20000000e-01 1.00000000e+00]
 [1.60000000e+01 3.86600000e+01]
 [3.48000000e+00 5.00000000e+00]
 [1.51900000e+02 3.12100000e+02]
 [1.00000000e+00 3.19000000e+00]]
before probe, param.shape = (7, 27)
before probe, target = (7,)
Traceback (most recent call last):
  File "/usr/local/home/yyq/bo/ganrs_bo/ganrs_Bayesian_Optimization_server.py", line 248, in <module>
    optimizer.maximize(init_points=init_points, n_iter=n_iter, acq='ei')
  File "/usr/local/home/yyq/bo/ganrs_bo/bayes_scode/bayesian_optimization.py", line 203, in maximize
    x_probe = self.suggest(util)
  File "/usr/local/home/yyq/bo/ganrs_bo/bayes_scode/bayesian_optimization.py", line 130, in suggest
    suggestion = acq_max(
  File "/usr/local/home/yyq/bo/ganrs_bo/bayes_scode/util.py", line 55, in acq_max
    res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp, y_max=y_max),
  File "/usr/local/home/python3/python3/lib/python3.8/site-packages/scipy/optimize/_minimize.py", line 623, in minimize
    return _minimize_lbfgsb(fun, x0, args, jac, bounds,
  File "/usr/local/home/python3/python3/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py", line 294, in _minimize_lbfgsb
    raise ValueError("LBFGSB - one of the lower bounds is greater than an upper bound.")
ValueError: LBFGSB - one of the lower bounds is greater than an upper bound.
nohup: 忽略输入
/usr/local/home/yyq/bo/ganrs_bo
=============== start wordcount-100G ===============
2022年 01月 27日 星期四 09:14:48 CST
=============== start wordcount-100G ===============
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo/generationConf.csv" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo/ganrs_target.png" 的文件状态(stat): 没有那个文件或目录
================= config1 =================
2022年 01月 27日 星期四 09:14:50 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (104449) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (277578) - 没有那个进程
================= config2 =================
2022年 01月 27日 星期四 09:18:06 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (107235) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (279916) - 没有那个进程
--sampleType = firstngroup	 --ganrsGroup = 6	 --niters = 44	 --initFile = /usr/local/home/yyq/bo/ganrs_bo/wordcount-100G-GAN-42.csv
重要参数列表（将贝叶斯的x_probe按照重要参数列表顺序转成配置文件实际运行:
spark.broadcast.blockSize
spark.broadcast.checksum
spark.broadcast.compress
spark.default.parallelism
spark.executor.cores
spark.executor.instances
spark.executor.memory
spark.executor.memoryOverhead
spark.kryoserializer.buffer
spark.kryoserializer.buffer.max
spark.locality.wait
spark.maxRemoteBlockSizeFetchToMem
spark.memory.fraction
spark.memory.offHeap.enabled
spark.memory.offHeap.size
spark.memory.storageFraction
spark.rdd.compress
spark.reducer.maxBlocksInFlightPerAddress
spark.reducer.maxReqsInFlight
spark.reducer.maxSizeInFlight
spark.scheduler.mode
spark.scheduler.revive.interval
spark.shuffle.compress
spark.shuffle.file.buffer
spark.shuffle.io.numConnectionsPerPeer
spark.shuffle.sort.bypassMergeThreshold
spark.storage.memoryMapThreshold
获取初始样本时，按照贝叶斯内部的key顺序传初始样本和已有的执行时间：
vital_params_name = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
vital_params_list = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold', 'runtime']
取出前6个样本
取出前两组样本作为初始样本：, shape = (6, 28)
interations：44
------------使用ganrs生成初始样本点------------
Tmax = 157.14285714285714
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[3.20000000e+01 6.40000000e+01]
 [0.00000000e+00 1.00000000e+00]
 [0.00000000e+00 1.00000000e+00]
 [2.00000000e+02 5.00000000e+02]
 [1.00000000e+00 4.00000000e+00]
 [4.00000000e+00 8.00000000e+00]
 [3.00000000e+00 7.00000000e+00]
 [3.84000000e+02 8.77000000e+02]
 [3.20000000e+01 1.28000000e+02]
 [3.20000000e+01 1.28000000e+02]
 [4.00000000e+00 1.00000000e+01]
 [1.07374157e+09 2.14748314e+09]
 [5.00000000e-01 9.00000000e-01]
 [0.00000000e+00 1.00000000e+00]
 [5.12000000e+02 1.02400000e+03]
 [5.00000000e-01 9.00000000e-01]
 [0.00000000e+00 1.00000000e+00]
 [1.07374182e+09 2.14748365e+09]
 [1.07374182e+09 2.14748365e+09]
 [2.40000000e+01 7.20000000e+01]
 [0.00000000e+00 1.00000000e+00]
 [5.00000000e+02 1.00000000e+03]
 [0.00000000e+00 1.00000000e+00]
 [1.60000000e+01 4.80000000e+01]
 [1.00000000e+00 5.00000000e+00]
 [1.50000000e+02 3.50000000e+02]
 [1.00000000e+00 4.00000000e+00]]
before probe, param.shape = (6, 27)
before probe, target = (6,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 53.319378844037786, 'spark.broadcast.checksum': 0.4076928616011777, 'spark.broadcast.compress': 0.40872162742522244, 'spark.default.parallelism': 428.5731435615442, 'spark.executor.cores': 2.6168142799516043, 'spark.executor.instances': 7.441458726774114, 'spark.executor.memory': 5.989917180549119, 'spark.executor.memoryOverhead': 776.8217305977164, 'spark.kryoserializer.buffer': 39.179933665809, 'spark.kryoserializer.buffer.max': 110.59017331880966, 'spark.locality.wait': 5.960609936242033, 'spark.maxRemoteBlockSizeFetchToMem': 1820950993.7819133, 'spark.memory.fraction': 0.5819487127885881, 'spark.memory.offHeap.enabled': 0.317771678406504, 'spark.memory.offHeap.size': 790.1742286421016, 'spark.memory.storageFraction': 0.6437878498964057, 'spark.rdd.compress': 0.17819513586510882, 'spark.reducer.maxBlocksInFlightPerAddress': 1656231736.749563, 'spark.reducer.maxReqsInFlight': 1205971683.5372324, 'spark.reducer.maxSizeInFlight': 28.47079050600625, 'spark.scheduler.mode': 0.7425075043188091, 'spark.scheduler.revive.interval': 595.686388771102, 'spark.shuffle.compress': 0.8949256002829524, 'spark.shuffle.file.buffer': 47.341508346482385, 'spark.shuffle.io.numConnectionsPerPeer': 3.024977378829365, 'spark.shuffle.sort.bypassMergeThreshold': 251.39466693013043, 'spark.storage.memoryMapThreshold': 1.3905136894752719} target = -187.8
x_probe target = -187.8
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.06253478e+01 6.40000000e+01]
 [7.30821852e-03 8.08077505e-01]
 [8.28554605e-03 8.09157709e-01]
 [3.11001801e+02 5.00000000e+02]
 [1.40765499e+00 3.82597357e+00]
 [5.89353166e+00 8.00000000e+00]
 [4.41941304e+00 7.00000000e+00]
 [5.84472817e+02 8.77000000e+02]
 [3.20000000e+01 7.60189303e+01]
 [7.32396820e+01 1.28000000e+02]
 [4.00000000e+00 8.33864043e+00]
 [1.39660259e+09 2.14748314e+09]
 [5.00000000e-01 7.38046148e-01]
 [0.00000000e+00 7.13660262e-01]
 [5.83922940e+02 9.96425517e+02]
 [5.00000000e-01 8.02977242e-01]
 [0.00000000e+00 5.67104893e-01]
 [1.22364725e+09 2.08881623e+09]
 [1.07374182e+09 1.62060507e+09]
 [2.40000000e+01 4.69343300e+01]
 [3.49632880e-01 1.00000000e+00]
 [5.00000000e+02 7.90470708e+02]
 [5.09671880e-01 1.00000000e+00]
 [3.51485838e+01 4.80000000e+01]
 [1.40622625e+00 4.64372851e+00]
 [1.70464400e+02 3.32324934e+02]
 [1.00000000e+00 2.55003937e+00]]
before probe, param.shape = (7, 27)
before probe, target = (7,)
================= config3 =================
2022年 01月 27日 星期四 09:21:59 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (109149) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (281909) - 没有那个进程
================= config4 =================
2022年 01月 27日 星期四 09:25:43 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (111636) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (283933) - 没有那个进程
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 61.515613792834266, 'spark.broadcast.checksum': 0.3039026115374115, 'spark.broadcast.compress': 0.13624287559621026, 'spark.default.parallelism': 381.41735578656693, 'spark.executor.cores': 3.0717748837076826, 'spark.executor.instances': 7.171887025408797, 'spark.executor.memory': 5.097254797908548, 'spark.executor.memoryOverhead': 837.4455939082903, 'spark.kryoserializer.buffer': 70.93016480413263, 'spark.kryoserializer.buffer.max': 114.9323745883601, 'spark.locality.wait': 5.623898949714029, 'spark.maxRemoteBlockSizeFetchToMem': 1875612689.0678499, 'spark.memory.fraction': 0.5344316390792307, 'spark.memory.offHeap.enabled': 0.26422115487147524, 'spark.memory.offHeap.size': 909.4523238545023, 'spark.memory.storageFraction': 0.7721704971664577, 'spark.rdd.compress': 0.3848076119164044, 'spark.reducer.maxBlocksInFlightPerAddress': 1384070643.117711, 'spark.reducer.maxReqsInFlight': 1356311946.0103416, 'spark.reducer.maxSizeInFlight': 41.812701712902566, 'spark.scheduler.mode': 0.35857505390082356, 'spark.scheduler.revive.interval': 669.2264670079796, 'spark.shuffle.compress': 0.5150653365311734, 'spark.shuffle.file.buffer': 42.59963418293377, 'spark.shuffle.io.numConnectionsPerPeer': 1.6649495082473196, 'spark.shuffle.sort.bypassMergeThreshold': 282.1851661431467, 'spark.storage.memoryMapThreshold': 1.2162839479986547} target = -223.921
x_probe target = -223.921
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.18947509e+01 6.40000000e+01]
 [4.73466828e-02 7.68039040e-01]
 [4.83291542e-02 7.69114101e-01]
 [3.22758935e+02 5.00000000e+02]
 [1.52857092e+00 3.70505764e+00]
 [6.04832437e+00 8.00000000e+00]
 [4.57646345e+00 7.00000000e+00]
 [6.03707708e+02 8.77000000e+02]
 [3.20000000e+01 7.23350307e+01]
 [7.69747311e+01 1.28000000e+02]
 [4.00000000e+00 8.10083738e+00]
 [1.43903743e+09 2.14748314e+09]
 [5.00000000e-01 7.22436405e-01]
 [0.00000000e+00 6.74071404e-01]
 [6.04548069e+02 9.75800388e+02]
 [5.00517397e-01 7.87058303e-01]
 [0.00000000e+00 5.28213917e-01]
 [1.26690570e+09 2.04555778e+09]
 [1.07374182e+09 1.57914173e+09]
 [2.40000000e+01 4.50879761e+01]
 [3.88920342e-01 1.00000000e+00]
 [5.00000000e+02 7.70992276e+02]
 [5.48197252e-01 1.00000000e+00]
 [3.63678762e+01 4.80000000e+01]
 [1.56810136e+00 4.48185340e+00]
 [1.78557427e+02 3.24231907e+02]
 [1.00000000e+00 2.43408681e+00]]
before probe, param.shape = (8, 27)
before probe, target = (8,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 57.61396380485064, 'spark.broadcast.checksum': 0.29082938369644473, 'spark.broadcast.compress': 0.12241751225140513, 'spark.default.parallelism': 367.2538574023235, 'spark.executor.cores': 1.9329005982803362, 'spark.executor.instances': 7.734304282509626, 'spark.executor.memory': 5.272835567642939, 'spark.executor.memoryOverhead': 704.1802988144574, 'spark.kryoserializer.buffer': 58.93944876920084, 'spark.kryoserializer.buffer.max': 125.15901685848868, 'spark.locality.wait': 4.423048953750633, 'spark.maxRemoteBlockSizeFetchToMem': 1456625618.8056734, 'spark.memory.fraction': 0.7215693767928355, 'spark.memory.offHeap.enabled': 0.3223018545160885, 'spark.memory.offHeap.size': 713.7131689830981, 'spark.memory.storageFraction': 0.6155046241361075, 'spark.rdd.compress': 0.24407608184821702, 'spark.reducer.maxBlocksInFlightPerAddress': 1375633975.9049194, 'spark.reducer.maxReqsInFlight': 1233523155.24056, 'spark.reducer.maxSizeInFlight': 33.918990732723984, 'spark.scheduler.mode': 0.7522399814459051, 'spark.scheduler.revive.interval': 673.9904990354001, 'spark.shuffle.compress': 0.8278460046771255, 'spark.shuffle.file.buffer': 47.80160007325969, 'spark.shuffle.io.numConnectionsPerPeer': 2.23466078459008, 'spark.shuffle.sort.bypassMergeThreshold': 184.69895274600518, 'spark.storage.memoryMapThreshold': 1.83960652146077} target = -215.047
x_probe target = -215.047
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.30372137e+01 6.36015440e+01]
 [8.33813007e-02 7.32004422e-01]
 [8.43684015e-02 7.33074853e-01]
 [3.33340356e+02 5.00000000e+02]
 [1.63739526e+00 3.59623330e+00]
 [6.18763781e+00 8.00000000e+00]
 [4.71780883e+00 7.00000000e+00]
 [6.21019111e+02 8.77000000e+02]
 [3.20000000e+01 6.90195210e+01]
 [8.03362753e+01 1.28000000e+02]
 [4.03440523e+00 7.88681464e+00]
 [1.47722879e+09 2.14748314e+09]
 [5.00000000e-01 7.08387636e-01]
 [0.00000000e+00 6.38441431e-01]
 [6.23110685e+02 9.57237772e+02]
 [5.14844442e-01 7.72731258e-01]
 [0.00000000e+00 4.93212039e-01]
 [1.30583830e+09 2.00662517e+09]
 [1.07374182e+09 1.54182473e+09]
 [2.40000000e+01 4.34262575e+01]
 [4.24279058e-01 1.00000000e+00]
 [5.00000000e+02 7.53461688e+02]
 [5.82870087e-01 1.00000000e+00]
 [3.74652394e+01 4.80000000e+01]
 [1.71378896e+00 4.33616579e+00]
 [1.85841151e+02 3.16948183e+02]
 [1.00000000e+00 2.32972949e+00]]
before probe, param.shape = (9, 27)
before probe, target = (9,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 58.890026146379526, 'spark.broadcast.checksum': 0.11989393055807196, 'spark.broadcast.compress': 0.42086247052525855, 'spark.default.parallelism': 403.0028018090622, 'spark.executor.cores': 2.640692860377554, 'spark.executor.instances': 7.697418904662343, 'spark.executor.memory': 6.418947923060111, 'spark.executor.memoryOverhead': 856.688668709839, 'spark.kryoserializer.buffer': 44.06508715231504, 'spark.kryoserializer.buffer.max': 127.31857352230625, 'spark.locality.wait': 4.494648609012358, 'spark.maxRemoteBlockSizeFetchToMem': 1831143037.4732404, 'spark.memory.fraction': 0.6727491969601058, 'spark.memory.offHeap.enabled': 0.459500868412861, 'spark.memory.offHeap.size': 940.2564953894531, 'spark.memory.storageFraction': 0.5793067575977697, 'spark.rdd.compress': 0.22924984179979505, 'spark.reducer.maxBlocksInFlightPerAddress': 1509467486.0387988, 'spark.reducer.maxReqsInFlight': 1517476410.6979687, 'spark.reducer.maxSizeInFlight': 33.613277048467054, 'spark.scheduler.mode': 0.49426560566238475, 'spark.scheduler.revive.interval': 596.336050619857, 'spark.shuffle.compress': 0.6912740284880666, 'spark.shuffle.file.buffer': 44.72435237903652, 'spark.shuffle.io.numConnectionsPerPeer': 3.991711508409791, 'spark.shuffle.sort.bypassMergeThreshold': 304.48956871978737, 'spark.storage.memoryMapThreshold': 2.300885510313967} target = -189.414
x_probe target = -189.414
================= config5 =================
2022年 01月 27日 星期四 09:29:01 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (113846) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (286207) - 没有那个进程
================= config6 =================
2022年 01月 27日 星期四 09:33:11 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (116790) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (288742) - 没有那个进程
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.40654302e+01 6.25733275e+01]
 [1.15812457e-01 6.99573266e-01]
 [1.16803724e-01 7.00639531e-01]
 [3.42863635e+02 5.00000000e+02]
 [1.73533716e+00 3.49829140e+00]
 [6.31301990e+00 8.00000000e+00]
 [4.84501966e+00 7.00000000e+00]
 [6.36599373e+02 8.77000000e+02]
 [3.20000000e+01 6.60355622e+01]
 [8.33616651e+01 1.28000000e+02]
 [4.22702570e+00 7.69419417e+00]
 [1.51160101e+09 2.13030098e+09]
 [5.00000000e-01 6.95743743e-01]
 [2.91689007e-02 6.06374456e-01]
 [6.39817039e+02 9.40531418e+02]
 [5.27738783e-01 7.59836917e-01]
 [0.00000000e+00 4.61710349e-01]
 [1.34087764e+09 1.97158583e+09]
 [1.07374182e+09 1.50823942e+09]
 [2.40000000e+01 4.19307108e+01]
 [4.56101903e-01 1.00000000e+00]
 [5.00000000e+02 7.37684158e+02]
 [6.14075638e-01 1.00000000e+00]
 [3.84528663e+01 4.80000000e+01]
 [1.84490780e+00 4.20504695e+00]
 [1.92396503e+02 3.10392831e+02]
 [1.00000000e+00 2.23580791e+00]]
before probe, param.shape = (10, 27)
before probe, target = (10,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 56.52249459616027, 'spark.broadcast.checksum': 0.23303375802234638, 'spark.broadcast.compress': 0.6931080815062295, 'spark.default.parallelism': 381.003272203661, 'spark.executor.cores': 2.0154605194102784, 'spark.executor.instances': 6.466413688783799, 'spark.executor.memory': 6.830293224641447, 'spark.executor.memoryOverhead': 790.9516097698719, 'spark.kryoserializer.buffer': 43.65569587010156, 'spark.kryoserializer.buffer.max': 96.68816612400309, 'spark.locality.wait': 4.631035567385525, 'spark.maxRemoteBlockSizeFetchToMem': 1888464959.1887133, 'spark.memory.fraction': 0.5681550785913121, 'spark.memory.offHeap.enabled': 0.6036298907028563, 'spark.memory.offHeap.size': 715.836508497976, 'spark.memory.storageFraction': 0.5289775237862179, 'spark.rdd.compress': 0.10993005230438697, 'spark.reducer.maxBlocksInFlightPerAddress': 1868385180.1568632, 'spark.reducer.maxReqsInFlight': 1130065900.8972347, 'spark.reducer.maxSizeInFlight': 37.39047532417417, 'spark.scheduler.mode': 0.8232856679156142, 'spark.scheduler.revive.interval': 567.687690383744, 'spark.shuffle.compress': 0.9461376216544952, 'spark.shuffle.file.buffer': 43.79975424034034, 'spark.shuffle.io.numConnectionsPerPeer': 3.988950566461908, 'spark.shuffle.sort.bypassMergeThreshold': 202.45960565938267, 'spark.storage.memoryMapThreshold': 2.037661858748204} target = -241.791
x_probe target = -241.791
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.49908251e+01 6.16479326e+01]
 [1.45000497e-01 6.70385226e-01]
 [1.45995514e-01 6.71447740e-01]
 [3.51434586e+02 5.00000000e+02]
 [1.82348487e+00 3.41014369e+00]
 [6.42586378e+00 8.00000000e+00]
 [4.95950941e+00 7.00000000e+00]
 [6.50621608e+02 8.77000000e+02]
 [3.20000000e+01 6.33499994e+01]
 [8.60845160e+01 1.28000000e+02]
 [4.40038413e+00 7.52083575e+00]
 [1.54253601e+09 2.09936598e+09]
 [5.00000000e-01 6.84364240e-01]
 [5.80291785e-02 5.77514178e-01]
 [6.54852758e+02 9.25495699e+02]
 [5.39343689e-01 7.48232010e-01]
 [0.00000000e+00 4.33358827e-01]
 [1.37241305e+09 1.94005042e+09]
 [1.07374182e+09 1.47801265e+09]
 [2.40000000e+01 4.05847188e+01]
 [4.84742463e-01 1.00000000e+00]
 [5.00000000e+02 7.23484381e+02]
 [6.42160635e-01 1.00000000e+00]
 [3.93417305e+01 4.80000000e+01]
 [1.96291476e+00 4.08704000e+00]
 [1.98296319e+02 3.04493015e+02]
 [1.00000000e+00 2.15127849e+00]]
before probe, param.shape = (11, 27)
before probe, target = (11,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 45.75904264934967, 'spark.broadcast.checksum': 0.657168942555354, 'spark.broadcast.compress': 0.5514761912782031, 'spark.default.parallelism': 366.4853106049752, 'spark.executor.cores': 2.0489267703372125, 'spark.executor.instances': 7.362265923065521, 'spark.executor.memory': 5.563207598981056, 'spark.executor.memoryOverhead': 744.1265871627018, 'spark.kryoserializer.buffer': 62.69054393290211, 'spark.kryoserializer.buffer.max': 87.64015695243347, 'spark.locality.wait': 4.458354323240465, 'spark.maxRemoteBlockSizeFetchToMem': 1701159088.767702, 'spark.memory.fraction': 0.5657699501522709, 'spark.memory.offHeap.enabled': 0.11061845955188626, 'spark.memory.offHeap.size': 659.4968542175358, 'spark.memory.storageFraction': 0.5748830831595458, 'spark.rdd.compress': 0.3676355877659694, 'spark.reducer.maxBlocksInFlightPerAddress': 1644857347.2715359, 'spark.reducer.maxReqsInFlight': 1091568751.3419008, 'spark.reducer.maxSizeInFlight': 32.13389783771403, 'spark.scheduler.mode': 0.4949528940728063, 'spark.scheduler.revive.interval': 625.3758487667316, 'spark.shuffle.compress': 0.8386037951244858, 'spark.shuffle.file.buffer': 40.41134836219747, 'spark.shuffle.io.numConnectionsPerPeer': 3.333703872871043, 'spark.shuffle.sort.bypassMergeThreshold': 232.13563864470112, 'spark.storage.memoryMapThreshold': 1.624402224592227} target = -249.352
x_probe target = -249.352
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
================= config7 =================
2022年 01月 27日 星期四 09:37:30 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (120007) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (291741) - 没有那个进程
================= config8 =================
2022年 01月 27日 星期四 09:41:14 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (122179) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (293985) - 没有那个进程
================= config9 =================
2022年 01月 27日 星期四 09:45:27 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (124672) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (296325) - 没有那个进程
bounds = 
[[4.58236804e+01 6.08150772e+01]
 [1.71269734e-01 6.44115989e-01]
 [1.72268126e-01 6.45175129e-01]
 [3.59148441e+02 4.97997846e+02]
 [1.90281781e+00 3.33081075e+00]
 [6.52742327e+00 8.00000000e+00]
 [5.06255019e+00 6.91728417e+00]
 [6.63241621e+02 8.77000000e+02]
 [3.20000000e+01 6.09329928e+01]
 [8.85350817e+01 1.28000000e+02]
 [4.55640671e+00 7.36481316e+00]
 [1.57037751e+09 2.07152448e+09]
 [5.00000000e-01 6.74122688e-01]
 [8.40034285e-02 5.51539928e-01]
 [6.68384905e+02 9.11963552e+02]
 [5.49788106e-01 7.37787594e-01]
 [0.00000000e+00 4.07842458e-01]
 [1.40079492e+09 1.91166855e+09]
 [1.07374182e+09 1.45080855e+09]
 [2.40000000e+01 3.93733260e+01]
 [5.10518967e-01 9.74496042e-01]
 [5.00000000e+02 7.10704582e+02]
 [6.67437131e-01 1.00000000e+00]
 [4.01417083e+01 4.80000000e+01]
 [2.06912102e+00 3.98083373e+00]
 [2.03606154e+02 2.99183180e+02]
 [1.00000000e+00 2.07520201e+00]]
before probe, param.shape = (12, 27)
before probe, target = (12,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 53.267063996770666, 'spark.broadcast.checksum': 0.20741262682521194, 'spark.broadcast.compress': 0.2658097597688643, 'spark.default.parallelism': 410.698749640201, 'spark.executor.cores': 2.1853025352886166, 'spark.executor.instances': 7.800069867485106, 'spark.executor.memory': 5.848301972012559, 'spark.executor.memoryOverhead': 695.1311688852533, 'spark.kryoserializer.buffer': 45.21501730141675, 'spark.kryoserializer.buffer.max': 116.88293669116862, 'spark.locality.wait': 4.672886095825459, 'spark.maxRemoteBlockSizeFetchToMem': 1767722518.9659185, 'spark.memory.fraction': 0.5601533187285016, 'spark.memory.offHeap.enabled': 0.20696108952657005, 'spark.memory.offHeap.size': 698.622361219281, 'spark.memory.storageFraction': 0.6926122966570825, 'spark.rdd.compress': 0.35434624769872397, 'spark.reducer.maxBlocksInFlightPerAddress': 1512424472.4541793, 'spark.reducer.maxReqsInFlight': 1074161325.9579961, 'spark.reducer.maxSizeInFlight': 38.11311605582247, 'spark.scheduler.mode': 0.56077596708329, 'spark.scheduler.revive.interval': 595.7339037023547, 'spark.shuffle.compress': 0.7269213340656582, 'spark.shuffle.file.buffer': 41.78570103549666, 'spark.shuffle.io.numConnectionsPerPeer': 3.1709600196163894, 'spark.shuffle.sort.bypassMergeThreshold': 227.0989372308391, 'spark.storage.memoryMapThreshold': 1.9920839947562874} target = -216.064
x_probe target = -216.064
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.65732503e+01 6.00655074e+01]
 [1.94912046e-01 6.20473677e-01]
 [1.95913476e-01 6.21529779e-01]
 [3.66090912e+02 4.91055376e+02]
 [1.97421746e+00 3.25941110e+00]
 [6.61882682e+00 8.00000000e+00]
 [5.15528689e+00 6.82454747e+00]
 [6.74599632e+02 8.77000000e+02]
 [3.20000000e+01 5.87576869e+01]
 [9.07405909e+01 1.28000000e+02]
 [4.69682703e+00 7.22439284e+00]
 [1.59543485e+09 2.04646713e+09]
 [5.00000000e-01 6.64905290e-01]
 [1.07380253e-01 5.28163103e-01]
 [6.80563838e+02 8.99784620e+02]
 [5.59188080e-01 7.28387620e-01]
 [0.00000000e+00 3.84877726e-01]
 [1.42633860e+09 1.88612487e+09]
 [1.07374182e+09 1.42632486e+09]
 [2.40000000e+01 3.82830724e+01]
 [5.33717821e-01 9.51297188e-01]
 [5.00000000e+02 6.99202762e+02]
 [6.90185978e-01 1.00000000e+00]
 [4.08616883e+01 4.80000000e+01]
 [2.16470666e+00 3.88524810e+00]
 [2.08385005e+02 2.94404329e+02]
 [1.00000000e+00 2.00673318e+00]]
before probe, param.shape = (13, 27)
before probe, target = (13,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 59.67330798252289, 'spark.broadcast.checksum': 0.24120862371891202, 'spark.broadcast.compress': 0.2976096091403365, 'spark.default.parallelism': 421.930780241468, 'spark.executor.cores': 2.7312387590876335, 'spark.executor.instances': 6.639834824016331, 'spark.executor.memory': 5.207953659760312, 'spark.executor.memoryOverhead': 706.9031789762035, 'spark.kryoserializer.buffer': 53.13966907851416, 'spark.kryoserializer.buffer.max': 90.99272035154085, 'spark.locality.wait': 4.727511565362013, 'spark.maxRemoteBlockSizeFetchToMem': 2020515359.4839048, 'spark.memory.fraction': 0.5232557049499611, 'spark.memory.offHeap.enabled': 0.21899055166440684, 'spark.memory.offHeap.size': 873.4542948737021, 'spark.memory.storageFraction': 0.7117720767533923, 'spark.rdd.compress': 0.34082486606591655, 'spark.reducer.maxBlocksInFlightPerAddress': 1585209944.3199522, 'spark.reducer.maxReqsInFlight': 1137125515.0990753, 'spark.reducer.maxSizeInFlight': 29.869551176949834, 'spark.scheduler.mode': 0.7305345418487409, 'spark.scheduler.revive.interval': 548.7591855731207, 'spark.shuffle.compress': 0.9797868018215976, 'spark.shuffle.file.buffer': 47.4990611385777, 'spark.shuffle.io.numConnectionsPerPeer': 3.654517651980328, 'spark.shuffle.sort.bypassMergeThreshold': 214.36436536070522, 'spark.storage.memoryMapThreshold': 1.104148409790317} target = -244.476
x_probe target = -244.476
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.72478631e+01 5.93908945e+01]
 [2.16190128e-01 5.99195595e-01]
 [2.17194291e-01 6.00248964e-01]
 [3.72339135e+02 4.84807152e+02]
 [2.03847714e+00 3.19515142e+00]
 [6.70109001e+00 8.00000000e+00]
 [5.23874992e+00 6.74108444e+00]
 [6.84821842e+02 8.68821620e+02]
 [3.20000000e+01 5.67999116e+01]
 [9.27255491e+01 1.28000000e+02]
 [4.82320532e+00 7.09801455e+00]
 [1.61798647e+09 2.02391552e+09]
 [5.07287793e-01 6.56609632e-01]
 [1.28419396e-01 5.07123961e-01]
 [6.91524877e+02 8.88823581e+02]
 [5.67648057e-01 7.19927643e-01]
 [0.00000000e+00 3.64209467e-01]
 [1.44932792e+09 1.86313556e+09]
 [1.07374182e+09 1.40428955e+09]
 [2.40000000e+01 3.73018442e+01]
 [5.54596789e-01 9.30418219e-01]
 [5.02521653e+02 6.88851125e+02]
 [7.10659940e-01 1.00000000e+00]
 [4.15096703e+01 4.80000000e+01]
 [2.25073373e+00 3.79922103e+00]
 [2.12685971e+02 2.90103363e+02]
 [1.00000000e+00 1.94511123e+00]]
before probe, param.shape = (14, 27)
before probe, target = (14,)
================= config10 =================
2022年 01月 27日 星期四 09:49:31 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (127166) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (298645) - 没有那个进程
================= config11 =================
2022年 01月 27日 星期四 09:53:28 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (129223) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (300527) - 没有那个进程
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 48.95417450551514, 'spark.broadcast.checksum': 0.4861664349677697, 'spark.broadcast.compress': 0.42283416298324894, 'spark.default.parallelism': 402.9339231717924, 'spark.executor.cores': 2.9001938484430205, 'spark.executor.instances': 6.883451431447155, 'spark.executor.memory': 5.4014375088564535, 'spark.executor.memoryOverhead': 845.652483974891, 'spark.kryoserializer.buffer': 44.69710796722347, 'spark.kryoserializer.buffer.max': 92.88812425734712, 'spark.locality.wait': 6.998563489705572, 'spark.maxRemoteBlockSizeFetchToMem': 1692182915.289091, 'spark.memory.fraction': 0.5931994710811526, 'spark.memory.offHeap.enabled': 0.1932229342335086, 'spark.memory.offHeap.size': 783.6627620157632, 'spark.memory.storageFraction': 0.6872247420317377, 'spark.rdd.compress': 0.25053955573496306, 'spark.reducer.maxBlocksInFlightPerAddress': 1664269327.0794692, 'spark.reducer.maxReqsInFlight': 1227629788.5783548, 'spark.reducer.maxSizeInFlight': 26.744115538242333, 'spark.scheduler.mode': 0.9063033943802967, 'spark.scheduler.revive.interval': 682.8719146074599, 'spark.shuffle.compress': 0.9413685896586423, 'spark.shuffle.file.buffer': 47.89020800801941, 'spark.shuffle.io.numConnectionsPerPeer': 2.6117565098977904, 'spark.shuffle.sort.bypassMergeThreshold': 278.60411575548983, 'spark.storage.memoryMapThreshold': 1.284588352201106} target = -234.959
x_probe target = -234.959
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.78550147e+01 5.87837430e+01]
 [2.35340401e-01 5.80045322e-01]
 [2.36347025e-01 5.81096230e-01]
 [3.77962536e+02 4.79183751e+02]
 [2.09631086e+00 3.13731770e+00]
 [6.77512688e+00 8.00000000e+00]
 [5.31386664e+00 6.66596772e+00]
 [6.94021830e+02 8.59621631e+02]
 [3.20000000e+01 5.50379138e+01]
 [9.45120115e+01 1.26668335e+02]
 [4.93694578e+00 6.98427409e+00]
 [1.63828292e+09 2.00361907e+09]
 [5.14753885e-01 6.49143540e-01]
 [1.47354624e-01 4.88188733e-01]
 [7.01389812e+02 8.78958645e+02]
 [5.75262036e-01 7.12313664e-01]
 [1.07822379e-02 3.45608034e-01]
 [1.47001830e+09 1.84244517e+09]
 [1.07374182e+09 1.38445776e+09]
 [2.40000000e+01 3.64187389e+01]
 [5.73387861e-01 9.11627148e-01]
 [5.11838126e+02 6.79534651e+02]
 [7.29086506e-01 1.00000000e+00]
 [4.20928541e+01 4.80000000e+01]
 [2.32815810e+00 3.72179666e+00]
 [2.16556841e+02 2.86232493e+02]
 [1.00000000e+00 1.88965148e+00]]
before probe, param.shape = (15, 27)
before probe, target = (15,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 56.14498077117163, 'spark.broadcast.checksum': 0.3549587930298046, 'spark.broadcast.compress': 0.3566794974889127, 'spark.default.parallelism': 414.95444943368705, 'spark.executor.cores': 2.318684932712271, 'spark.executor.instances': 7.895751996061933, 'spark.executor.memory': 6.504113246089398, 'spark.executor.memoryOverhead': 846.0747474418505, 'spark.kryoserializer.buffer': 36.586045682154435, 'spark.kryoserializer.buffer.max': 102.30355553169068, 'spark.locality.wait': 5.402858520437184, 'spark.maxRemoteBlockSizeFetchToMem': 1881786051.7111113, 'spark.memory.fraction': 0.5669777699965804, 'spark.memory.offHeap.enabled': 0.37245723516077855, 'spark.memory.offHeap.size': 746.912579664667, 'spark.memory.storageFraction': 0.6239935457742042, 'spark.rdd.compress': 0.0474214147416263, 'spark.reducer.maxBlocksInFlightPerAddress': 1565897171.3218832, 'spark.reducer.maxReqsInFlight': 1292453491.1172342, 'spark.reducer.maxSizeInFlight': 26.395903881661916, 'spark.scheduler.mode': 0.7952666686842409, 'spark.scheduler.revive.interval': 627.5698679141116, 'spark.shuffle.compress': 0.9499124687645575, 'spark.shuffle.file.buffer': 46.977225383958206, 'spark.shuffle.io.numConnectionsPerPeer': 2.900654355062814, 'spark.shuffle.sort.bypassMergeThreshold': 264.8106249271076, 'spark.storage.memoryMapThreshold': 1.6483447007541212} target = -228.43
x_probe target = -228.43
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[4.84014511e+01 5.82373066e+01]
 [2.52575647e-01 5.62810076e-01]
 [2.53584485e-01 5.63858770e-01]
 [3.83023596e+02 4.74122691e+02]
 [2.14836120e+00 3.08526736e+00]
 [6.84176007e+00 8.00000000e+00]
 [5.38147170e+00 6.59836266e+00]
 [7.02301820e+02 8.51341641e+02]
 [3.20000000e+01 5.34521158e+01]
 [9.61198277e+01 1.25060519e+02]
 [5.03931220e+00 6.88190767e+00]
 [1.65654973e+09 1.98535226e+09]
 [5.21473368e-01 6.42424058e-01]
 [1.64396330e-01 4.71147027e-01]
 [7.10268254e+02 8.70080204e+02]
 [5.82114618e-01 7.05461082e-01]
 [2.75235277e-02 3.28866744e-01]
 [1.48863964e+09 1.82382383e+09]
 [1.07374182e+09 1.36660915e+09]
 [2.40000000e+01 3.56239440e+01]
 [5.90299825e-01 8.94715184e-01]
 [5.20222952e+02 6.71149825e+02]
 [7.45670416e-01 1.00000000e+00]
 [4.26177195e+01 4.80000000e+01]
 [2.39784002e+00 3.65211473e+00]
 [2.20040623e+02 2.82748710e+02]
 [1.00000000e+00 1.83973770e+00]]
before probe, param.shape = (16, 27)
before probe, target = (16,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 57.875214765624726, 'spark.broadcast.checksum': 0.2831101793535184, 'spark.broadcast.compress': 0.4144644976031575, 'spark.default.parallelism': 419.95696549475747, 'spark.executor.cores': 2.6971198423668996, 'spark.executor.instances': 7.625324654272882, 'spark.executor.memory': 5.80131795477393, 'spark.executor.memoryOverhead': 718.2973719691075, 'spark.kryoserializer.buffer': 47.742076502991516, 'spark.kryoserializer.buffer.max': 105.6511262244029, 'spark.locality.wait': 5.870289995171963, 'spark.maxRemoteBlockSizeFetchToMem': 1902558343.3252213, 'spark.memory.fraction': 0.549175195460168, 'spark.memory.offHeap.enabled': 0.33844735322360764, 'spark.memory.offHeap.size': 803.45060802936, 'spark.memory.storageFraction': 0.6092631363824719, 'spark.rdd.compress': 0.2602821194790077, 'spark.reducer.maxBlocksInFlightPerAddress': 1548496918.931568, 'spark.reducer.maxReqsInFlight': 1354785829.4796634, 'spark.reducer.maxSizeInFlight': 30.762254590041415, 'spark.scheduler.mode': 0.8443269812662293, 'spark.scheduler.revive.interval': 606.0616049105884, 'spark.shuffle.compress': 0.9517355885072011, 'spark.shuffle.file.buffer': 46.60140718206189, 'spark.shuffle.io.numConnectionsPerPeer': 2.4614696487991528, 'spark.shuffle.sort.bypassMergeThreshold': 264.4956470132933, 'spark.storage.memoryMapThreshold': 1.4016735567888656} target = -185.947
x_probe target = -185.947
================= config12 =================
2022年 01月 27日 星期四 09:56:43 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (131805) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (302929) - 没有那个进程
================= config13 =================
2022年 01月 27日 星期四 10:00:49 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (134006) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (305233) - 没有那个进程
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.36768716e+01 6.20735579e+01]
 [1.49733821e-01 4.16486538e-01]
 [2.75128213e-01 5.53800782e-01]
 [3.79393182e+02 4.60520749e+02]
 [2.27952735e+00 3.11471234e+00]
 [7.09478916e+00 8.00000000e+00]
 [5.26314698e+00 6.33948893e+00]
 [6.54155671e+02 7.82439073e+02]
 [3.53252197e+01 6.01589333e+01]
 [9.28747675e+01 1.18427485e+02]
 [5.04563803e+00 6.69494196e+00]
 [1.75867757e+09 2.04643912e+09]
 [5.00000000e-01 6.01964330e-01]
 [2.01443323e-01 4.75451383e-01]
 [7.32199049e+02 8.74702167e+02]
 [5.55483463e-01 6.63042810e-01]
 [1.28782021e-01 3.91782218e-01]
 [1.40305078e+09 1.69394306e+09]
 [1.21765281e+09 1.49191884e+09]
 [2.44389896e+01 3.70855195e+01]
 [7.12431044e-01 9.76222919e-01]
 [5.38663273e+02 6.73459937e+02]
 [8.20246422e-01 1.00000000e+00]
 [4.23870023e+01 4.80000000e+01]
 [1.92522142e+00 2.99771788e+00]
 [2.36932057e+02 2.92059237e+02]
 [1.00000000e+00 1.80541717e+00]]
before probe, param.shape = (17, 27)
before probe, target = (17,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.98816257506113, 'spark.broadcast.checksum': 0.34154979287825504, 'spark.broadcast.compress': 0.3084341156590892, 'spark.default.parallelism': 425.3006172626483, 'spark.executor.cores': 2.3884761791166143, 'spark.executor.instances': 7.142333266060449, 'spark.executor.memory': 5.5109891761949665, 'spark.executor.memoryOverhead': 671.9482142202335, 'spark.kryoserializer.buffer': 38.41907692892745, 'spark.kryoserializer.buffer.max': 108.65553249031224, 'spark.locality.wait': 5.587817338845827, 'spark.maxRemoteBlockSizeFetchToMem': 1925669747.5335288, 'spark.memory.fraction': 0.5319517683991961, 'spark.memory.offHeap.enabled': 0.44189474728363587, 'spark.memory.offHeap.size': 844.029351933942, 'spark.memory.storageFraction': 0.6257385230120944, 'spark.rdd.compress': 0.17628439896825976, 'spark.reducer.maxBlocksInFlightPerAddress': 1608442883.3933525, 'spark.reducer.maxReqsInFlight': 1444687863.461722, 'spark.reducer.maxSizeInFlight': 24.998120488885647, 'spark.scheduler.mode': 0.7939086135036264, 'spark.scheduler.revive.interval': 664.1826707694426, 'spark.shuffle.compress': 0.9518758133779192, 'spark.shuffle.file.buffer': 44.0255996237245, 'spark.shuffle.io.numConnectionsPerPeer': 2.412976595747999, 'spark.shuffle.sort.bypassMergeThreshold': 238.90733049184584, 'spark.storage.memoryMapThreshold': 1.7286529588237731} target = -237.354
x_probe target = -237.354
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.40967059e+01 6.16537236e+01]
 [1.63071457e-01 4.03148902e-01]
 [2.89061841e-01 5.39867154e-01]
 [3.83449560e+02 4.56464371e+02]
 [2.32128660e+00 3.07295309e+00]
 [7.14784271e+00 8.00000000e+00]
 [5.31696408e+00 6.28567183e+00]
 [6.60569841e+02 7.76024903e+02]
 [3.65669054e+01 5.89172476e+01]
 [9.41524034e+01 1.17149849e+02]
 [5.12810322e+00 6.61247677e+00]
 [1.77306565e+09 2.03205104e+09]
 [5.01664974e-01 5.96685417e-01]
 [2.15143726e-01 4.61750980e-01]
 [7.39324205e+02 8.67577011e+02]
 [5.60861430e-01 6.57664842e-01]
 [1.41932031e-01 3.78632208e-01]
 [1.41759539e+09 1.67939845e+09]
 [1.23136612e+09 1.47820554e+09]
 [2.50713161e+01 3.64531931e+01]
 [7.25620638e-01 9.63033325e-01]
 [5.45403106e+02 6.66720104e+02]
 [8.33395338e-01 1.00000000e+00]
 [4.28084428e+01 4.80000000e+01]
 [1.97884624e+00 2.94409306e+00]
 [2.39688416e+02 2.89302878e+02]
 [1.03830430e+00 1.76504281e+00]]
before probe, param.shape = (18, 27)
before probe, target = (18,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 54.678983541916956, 'spark.broadcast.checksum': 0.1683659647359286, 'spark.broadcast.compress': 0.5256513751875058, 'spark.default.parallelism': 427.2372331515963, 'spark.executor.cores': 2.543049403695463, 'spark.executor.instances': 7.265846003398852, 'spark.executor.memory': 6.122211350650664, 'spark.executor.memoryOverhead': 764.6782034853559, 'spark.kryoserializer.buffer': 43.499109598816474, 'spark.kryoserializer.buffer.max': 106.05963557045814, 'spark.locality.wait': 6.336941153427936, 'spark.maxRemoteBlockSizeFetchToMem': 1891473665.3955033, 'spark.memory.fraction': 0.5345475562546408, 'spark.memory.offHeap.enabled': 0.4009973653105636, 'spark.memory.offHeap.size': 797.994768530942, 'spark.memory.storageFraction': 0.576572140245791, 'spark.rdd.compress': 0.18400525050792063, 'spark.reducer.maxBlocksInFlightPerAddress': 1667392274.6586573, 'spark.reducer.maxReqsInFlight': 1232023225.360977, 'spark.reducer.maxSizeInFlight': 26.60432093005576, 'spark.scheduler.mode': 0.7711320520546363, 'spark.scheduler.revive.interval': 629.2462625752948, 'spark.shuffle.compress': 0.9441283879449944, 'spark.shuffle.file.buffer': 44.61343378343466, 'spark.shuffle.io.numConnectionsPerPeer': 2.323527914411851, 'spark.shuffle.sort.bypassMergeThreshold': 262.82778895760504, 'spark.storage.memoryMapThreshold': 1.5188476700638125} target = -247.177
x_probe target = -247.177
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
================= config14 =================
2022年 01月 27日 星期四 10:05:05 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (136080) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (307109) - 没有那个进程
================= config15 =================
2022年 01月 27日 星期四 10:08:13 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (138490) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (309350) - 没有那个进程
================= config16 =================
2022年 01月 27日 星期四 10:12:05 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (140840) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (934) - 没有那个进程
bounds = 
[[5.44745568e+01 6.12758727e+01]
 [1.75075329e-01 3.91145030e-01]
 [3.01602107e-01 5.27326888e-01]
 [3.87100301e+02 4.52813630e+02]
 [2.35886992e+00 3.03536976e+00]
 [7.19559090e+00 8.00000000e+00]
 [5.36539947e+00 6.23723644e+00]
 [6.66342594e+02 7.70252150e+02]
 [3.76844225e+01 5.77997305e+01]
 [9.53022757e+01 1.15999977e+02]
 [5.20232190e+00 6.53825809e+00]
 [1.78601492e+09 2.01910177e+09]
 [5.06415997e-01 5.91934394e-01]
 [2.27474089e-01 4.49420618e-01]
 [7.45736846e+02 8.61164370e+02]
 [5.65701601e-01 6.52824672e-01]
 [1.53767040e-01 3.66797199e-01]
 [1.43068554e+09 1.66630830e+09]
 [1.24370809e+09 1.46586357e+09]
 [2.56404100e+01 3.58840992e+01]
 [7.37491272e-01 9.51162691e-01]
 [5.51468956e+02 6.60654254e+02]
 [8.45229363e-01 1.00000000e+00]
 [4.31877392e+01 4.80000000e+01]
 [2.02710858e+00 2.89583072e+00]
 [2.42169139e+02 2.86822155e+02]
 [1.07464123e+00 1.72870588e+00]]
before probe, param.shape = (19, 27)
before probe, target = (19,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.405846266412894, 'spark.broadcast.checksum': 0.19498210514636186, 'spark.broadcast.compress': 0.5172024532528521, 'spark.default.parallelism': 393.783295054708, 'spark.executor.cores': 2.552401609548871, 'spark.executor.instances': 7.431031891265464, 'spark.executor.memory': 6.170730348126142, 'spark.executor.memoryOverhead': 733.3257260041246, 'spark.kryoserializer.buffer': 39.6061519302221, 'spark.kryoserializer.buffer.max': 114.65172699047474, 'spark.locality.wait': 6.075005517516571, 'spark.maxRemoteBlockSizeFetchToMem': 1981097129.114383, 'spark.memory.fraction': 0.5507939198952226, 'spark.memory.offHeap.enabled': 0.33407027761184627, 'spark.memory.offHeap.size': 826.7662403036701, 'spark.memory.storageFraction': 0.6138105428246985, 'spark.rdd.compress': 0.3254435658392343, 'spark.reducer.maxBlocksInFlightPerAddress': 1546584562.3992367, 'spark.reducer.maxReqsInFlight': 1324411459.3533633, 'spark.reducer.maxSizeInFlight': 27.704000433245, 'spark.scheduler.mode': 0.7573497703913982, 'spark.scheduler.revive.interval': 587.898601803597, 'spark.shuffle.compress': 0.896087694585596, 'spark.shuffle.file.buffer': 43.35286117652733, 'spark.shuffle.io.numConnectionsPerPeer': 2.1238983470998116, 'spark.shuffle.sort.bypassMergeThreshold': 282.9942952318955, 'spark.storage.memoryMapThreshold': 1.1257625758057714} target = -179.578
x_probe target = -179.578
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.74717857e+01 6.33399068e+01]
 [1.02157143e-01 2.87807067e-01]
 [4.20763199e-01 6.13641707e-01]
 [3.65520980e+02 4.22045610e+02]
 [2.25521259e+00 2.84959063e+00]
 [7.05398615e+00 7.80807763e+00]
 [5.79687433e+00 6.54458637e+00]
 [6.87317844e+02 7.79333608e+02]
 [3.20000000e+01 4.82512443e+01]
 [1.05787792e+02 1.23515662e+02]
 [5.48407001e+00 6.66594103e+00]
 [1.88013499e+09 2.08205927e+09]
 [5.12391577e-01 5.89196263e-01]
 [2.34413193e-01 4.33727362e-01]
 [7.75989636e+02 8.77542845e+02]
 [5.74832531e-01 6.52788554e-01]
 [2.32838067e-01 4.18049065e-01]
 [1.44064994e+09 1.65251918e+09]
 [1.22596021e+09 1.42286271e+09]
 [2.40000000e+01 3.21607479e+01]
 [6.65546493e-01 8.49153048e-01]
 [5.39673368e+02 6.36123836e+02]
 [8.03014487e-01 9.89160902e-01]
 [4.04429873e+01 4.62627350e+01]
 [1.74985195e+00 2.49794474e+00]
 [2.63825370e+02 3.02163220e+02]
 [1.00000000e+00 1.40629612e+00]]
before probe, param.shape = (20, 27)
before probe, target = (20,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 63.13808524656162, 'spark.broadcast.checksum': 0.2127904267647519, 'spark.broadcast.compress': 0.46861725965031, 'spark.default.parallelism': 416.5895109878129, 'spark.executor.cores': 2.3668657098461705, 'spark.executor.instances': 7.61094858395716, 'spark.executor.memory': 6.140393841535369, 'spark.executor.memoryOverhead': 717.7881326931772, 'spark.kryoserializer.buffer': 42.76687068268677, 'spark.kryoserializer.buffer.max': 113.24112017071906, 'spark.locality.wait': 6.24522533267918, 'spark.maxRemoteBlockSizeFetchToMem': 1954212234.050764, 'spark.memory.fraction': 0.5830385159410527, 'spark.memory.offHeap.enabled': 0.4257892844697353, 'spark.memory.offHeap.size': 855.3741889171355, 'spark.memory.storageFraction': 0.6431618568006292, 'spark.rdd.compress': 0.2716148397841202, 'spark.reducer.maxBlocksInFlightPerAddress': 1577205268.942987, 'spark.reducer.maxReqsInFlight': 1277760510.365191, 'spark.reducer.maxSizeInFlight': 31.79536717473264, 'spark.scheduler.mode': 0.8285909901886743, 'spark.scheduler.revive.interval': 618.1348706405346, 'spark.shuffle.compress': 0.9678628739236724, 'spark.shuffle.file.buffer': 45.429170541340184, 'spark.shuffle.io.numConnectionsPerPeer': 2.061974086239972, 'spark.shuffle.sort.bypassMergeThreshold': 282.75107496507997, 'spark.storage.memoryMapThreshold': 1.0010809137941932} target = -222.398
x_probe target = -222.398
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.77651917e+01 6.30465008e+01]
 [1.11439640e-01 2.78524571e-01]
 [4.30407125e-01 6.03997782e-01]
 [3.68347212e+02 4.19219378e+02]
 [2.28493149e+00 2.81987173e+00]
 [7.09169073e+00 7.77037306e+00]
 [5.83425993e+00 6.50720077e+00]
 [6.91918632e+02 7.74732820e+02]
 [3.20000000e+01 4.73867350e+01]
 [1.06674185e+02 1.22629269e+02]
 [5.54316356e+00 6.60684747e+00]
 [1.89023120e+09 2.07196306e+09]
 [5.16231811e-01 5.85356028e-01]
 [2.44378902e-01 4.23761653e-01]
 [7.81067296e+02 8.72465184e+02]
 [5.78730332e-01 6.48890753e-01]
 [2.42098617e-01 4.08788515e-01]
 [1.45124340e+09 1.64192572e+09]
 [1.23580533e+09 1.41301758e+09]
 [2.40000000e+01 3.17150731e+01]
 [6.74726820e-01 8.39972720e-01]
 [5.44495891e+02 6.31301312e+02]
 [8.12321808e-01 9.79853582e-01]
 [4.07339747e+01 4.59717476e+01]
 [1.78725659e+00 2.46054010e+00]
 [2.65742263e+02 3.00246328e+02]
 [1.00000000e+00 1.37824277e+00]]
before probe, param.shape = (21, 27)
before probe, target = (21,)
================= config17 =================
2022年 01月 27日 星期四 10:16:35 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (143178) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (3469) - 没有那个进程
================= config18 =================
2022年 01月 27日 星期四 10:20:23 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (145579) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (6056) - 没有那个进程
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 62.29900586280915, 'spark.broadcast.checksum': 0.26892103243090504, 'spark.broadcast.compress': 0.5299507565183664, 'spark.default.parallelism': 395.9748477376693, 'spark.executor.cores': 2.4008204883400417, 'spark.executor.instances': 7.129055689652164, 'spark.executor.memory': 6.466878622353864, 'spark.executor.memoryOverhead': 754.1846743432538, 'spark.kryoserializer.buffer': 45.49183302433277, 'spark.kryoserializer.buffer.max': 108.61973346866903, 'spark.locality.wait': 6.290546957012037, 'spark.maxRemoteBlockSizeFetchToMem': 2062834996.4442317, 'spark.memory.fraction': 0.5345846252079938, 'spark.memory.offHeap.enabled': 0.31506129509542485, 'spark.memory.offHeap.size': 816.6808381457988, 'spark.memory.storageFraction': 0.6040847847382788, 'spark.rdd.compress': 0.3455531721244264, 'spark.reducer.maxBlocksInFlightPerAddress': 1584714574.8921046, 'spark.reducer.maxReqsInFlight': 1350173840.4339647, 'spark.reducer.maxSizeInFlight': 29.790457037979614, 'spark.scheduler.mode': 0.7632814146339437, 'spark.scheduler.revive.interval': 605.5981515248408, 'spark.shuffle.compress': 0.9252836597865224, 'spark.shuffle.file.buffer': 44.62256085029577, 'spark.shuffle.io.numConnectionsPerPeer': 1.8238652119244452, 'spark.shuffle.sort.bypassMergeThreshold': 267.4200452391775, 'spark.storage.memoryMapThreshold': 1.3188863434259586} target = -261.945
x_probe target = -261.945
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.80292572e+01 6.27824353e+01]
 [1.19793886e-01 2.70170324e-01]
 [4.39086658e-01 5.95318249e-01]
 [3.70890820e+02 4.16675770e+02]
 [2.31167851e+00 2.79312471e+00]
 [7.12562484e+00 7.73643894e+00]
 [5.86790697e+00 6.47355372e+00]
 [6.96059341e+02 7.70592111e+02]
 [3.26036271e+01 4.66086767e+01]
 [1.07471939e+02 1.21831515e+02]
 [5.59634776e+00 6.55366328e+00]
 [1.89931779e+09 2.06287647e+09]
 [5.19688022e-01 5.81899818e-01]
 [2.53348039e-01 4.14792516e-01]
 [7.85637191e+02 8.67895290e+02]
 [5.82238353e-01 6.45382732e-01]
 [2.50433111e-01 4.00454020e-01]
 [1.46077752e+09 1.63239161e+09]
 [1.24466595e+09 1.40415697e+09]
 [2.40940350e+01 3.13139659e+01]
 [6.82989115e-01 8.31710425e-01]
 [5.48836162e+02 6.26961041e+02]
 [8.20698396e-01 9.71476993e-01]
 [4.09958634e+01 4.57098590e+01]
 [1.82092077e+00 2.42687593e+00]
 [2.67467466e+02 2.98521124e+02]
 [1.00000000e+00 1.35299475e+00]]
before probe, param.shape = (22, 27)
before probe, target = (22,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 61.512769595655136, 'spark.broadcast.checksum': 0.16769193405800897, 'spark.broadcast.compress': 0.46033901458882287, 'spark.default.parallelism': 390.6006590036622, 'spark.executor.cores': 2.357758459047164, 'spark.executor.instances': 7.375329476438639, 'spark.executor.memory': 6.011200156612315, 'spark.executor.memoryOverhead': 741.1813561968087, 'spark.kryoserializer.buffer': 45.13667167799882, 'spark.kryoserializer.buffer.max': 110.14676804059212, 'spark.locality.wait': 6.373946792712745, 'spark.maxRemoteBlockSizeFetchToMem': 1936685023.4576905, 'spark.memory.fraction': 0.5315514133465632, 'spark.memory.offHeap.enabled': 0.38718210414664744, 'spark.memory.offHeap.size': 805.5812550978604, 'spark.memory.storageFraction': 0.6391318793085072, 'spark.rdd.compress': 0.39309733872092095, 'spark.reducer.maxBlocksInFlightPerAddress': 1507553952.4719574, 'spark.reducer.maxReqsInFlight': 1325733025.8772879, 'spark.reducer.maxSizeInFlight': 27.939257050197305, 'spark.scheduler.mode': 0.6975019529066476, 'spark.scheduler.revive.interval': 602.7173603369412, 'spark.shuffle.compress': 0.8585183503489785, 'spark.shuffle.file.buffer': 45.310505861609364, 'spark.shuffle.io.numConnectionsPerPeer': 2.426322189037314, 'spark.shuffle.sort.bypassMergeThreshold': 277.4298785793654, 'spark.storage.memoryMapThreshold': 1.198932935872779} target = -218.978
x_probe target = -218.978
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.82669161e+01 6.25447764e+01]
 [1.27312708e-01 2.62651502e-01]
 [4.46898237e-01 5.87506669e-01]
 [3.73180068e+02 4.14386522e+02]
 [2.33575082e+00 2.76905240e+00]
 [7.15616555e+00 7.70589824e+00]
 [5.89818931e+00 6.44327139e+00]
 [6.99785980e+02 7.66865472e+02]
 [3.33038796e+01 4.59084243e+01]
 [1.08189918e+02 1.21113536e+02]
 [5.64421353e+00 6.50579750e+00]
 [1.90749573e+09 2.05469853e+09]
 [5.22798612e-01 5.78789228e-01]
 [2.61420263e-01 4.06720292e-01]
 [7.89750096e+02 8.63782385e+02]
 [5.85395572e-01 6.42225513e-01]
 [2.57934157e-01 3.92952975e-01]
 [1.46935822e+09 1.62381090e+09]
 [1.25264050e+09 1.39618242e+09]
 [2.44550315e+01 3.09529693e+01]
 [6.90425181e-01 8.24274360e-01]
 [5.52742406e+02 6.23054797e+02]
 [8.28237326e-01 9.63938063e-01]
 [4.12315631e+01 4.54741592e+01]
 [1.85121852e+00 2.39657817e+00]
 [2.69020149e+02 2.96968441e+02]
 [1.00000000e+00 1.33027153e+00]]
before probe, param.shape = (23, 27)
before probe, target = (23,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 62.02461957360334, 'spark.broadcast.checksum': 0.13870785320447965, 'spark.broadcast.compress': 0.5480468394743098, 'spark.default.parallelism': 402.4111858964692, 'spark.executor.cores': 2.4526333730786747, 'spark.executor.instances': 7.638006757239582, 'spark.executor.memory': 6.233682472636122, 'spark.executor.memoryOverhead': 713.2797558696737, 'spark.kryoserializer.buffer': 39.83731336669512, 'spark.kryoserializer.buffer.max': 114.9550821790581, 'spark.locality.wait': 5.823535504628312, 'spark.maxRemoteBlockSizeFetchToMem': 1992668808.1815686, 'spark.memory.fraction': 0.5601096139242374, 'spark.memory.offHeap.enabled': 0.3854337388941364, 'spark.memory.offHeap.size': 862.4371192067545, 'spark.memory.storageFraction': 0.6173992640239172, 'spark.rdd.compress': 0.311196530090289, 'spark.reducer.maxBlocksInFlightPerAddress': 1530190087.189172, 'spark.reducer.maxReqsInFlight': 1337072288.0461726, 'spark.reducer.maxSizeInFlight': 25.19948115769695, 'spark.scheduler.mode': 0.7305488868218271, 'spark.scheduler.revive.interval': 605.271101645638, 'spark.shuffle.compress': 0.8925907674622434, 'spark.shuffle.file.buffer': 41.952717627400624, 'spark.shuffle.io.numConnectionsPerPeer': 2.037918919470645, 'spark.shuffle.sort.bypassMergeThreshold': 283.5601120504194, 'spark.storage.memoryMapThreshold': 1.007581757109847} target = -222.578
x_probe target = -222.578
================= config19 =================
2022年 01月 27日 星期四 10:24:14 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (147347) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (7900) - 没有那个进程
================= config20 =================
2022年 01月 27日 星期四 10:27:34 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (150043) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (10439) - 没有那个进程
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.84808091e+01 6.23308834e+01]
 [1.34079648e-01 2.55884563e-01]
 [4.53928659e-01 5.80476248e-01]
 [3.75240390e+02 4.12326200e+02]
 [2.35741590e+00 2.74738732e+00]
 [7.18365218e+00 7.67841160e+00]
 [5.92544341e+00 6.41601728e+00]
 [7.03139954e+02 7.63511498e+02]
 [3.39341068e+01 4.52781970e+01]
 [1.08836099e+02 1.20467355e+02]
 [5.68729273e+00 6.46271830e+00]
 [1.91485587e+09 2.04733839e+09]
 [5.25598143e-01 5.75989697e-01]
 [2.68685265e-01 3.99455291e-01]
 [7.93451710e+02 8.60080771e+02]
 [5.88237069e-01 6.39384016e-01]
 [2.64685098e-01 3.86202034e-01]
 [1.47708086e+09 1.61608827e+09]
 [1.25981759e+09 1.38900532e+09]
 [2.47799284e+01 3.06280724e+01]
 [6.97117640e-01 8.17581901e-01]
 [5.56258026e+02 6.19539178e+02]
 [8.35022363e-01 9.57153026e-01]
 [4.14436929e+01 4.52620294e+01]
 [1.87848651e+00 2.36931019e+00]
 [2.70417564e+02 2.95571027e+02]
 [1.00000000e+00 1.30982063e+00]]
before probe, param.shape = (24, 27)
before probe, target = (24,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.05355167428334, 'spark.broadcast.checksum': 0.18834052070010127, 'spark.broadcast.compress': 0.5481351633826944, 'spark.default.parallelism': 380.93251329347316, 'spark.executor.cores': 2.6455949078581207, 'spark.executor.instances': 7.257920885527408, 'spark.executor.memory': 6.021272097367328, 'spark.executor.memoryOverhead': 731.4953416490366, 'spark.kryoserializer.buffer': 43.78313547274096, 'spark.kryoserializer.buffer.max': 113.12637901194664, 'spark.locality.wait': 6.34324265418727, 'spark.maxRemoteBlockSizeFetchToMem': 2031478085.1875582, 'spark.memory.fraction': 0.5291435913286482, 'spark.memory.offHeap.enabled': 0.2973840972205764, 'spark.memory.offHeap.size': 838.3204373616013, 'spark.memory.storageFraction': 0.5934533439730493, 'spark.rdd.compress': 0.30516612875569943, 'spark.reducer.maxBlocksInFlightPerAddress': 1548850368.5810137, 'spark.reducer.maxReqsInFlight': 1332873818.257057, 'spark.reducer.maxSizeInFlight': 27.558900885930683, 'spark.scheduler.mode': 0.7138304418672875, 'spark.scheduler.revive.interval': 614.99426727651, 'spark.shuffle.compress': 0.9467433545790768, 'spark.shuffle.file.buffer': 42.58712804273531, 'spark.shuffle.io.numConnectionsPerPeer': 1.912715972958427, 'spark.shuffle.sort.bypassMergeThreshold': 291.9266690476128, 'spark.storage.memoryMapThreshold': 1.2026415807766715} target = -190.821
x_probe target = -190.821
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.86733128e+01 6.21383797e+01]
 [1.40169893e-01 2.49794317e-01]
 [4.60256038e-01 5.74148868e-01]
 [3.77094681e+02 4.10471909e+02]
 [2.37691447e+00 2.72788875e+00]
 [7.20839015e+00 7.65367363e+00]
 [5.94997211e+00 6.39148859e+00]
 [7.06158531e+02 7.60492921e+02]
 [3.45013114e+01 4.47109925e+01]
 [1.09417662e+02 1.19885792e+02]
 [5.72606401e+00 6.42394703e+00]
 [1.92147999e+09 2.04071427e+09]
 [5.28117720e-01 5.73470119e-01]
 [2.75223766e-01 3.92916789e-01]
 [7.96783163e+02 8.56749318e+02]
 [5.90794417e-01 6.36826669e-01]
 [2.70760945e-01 3.80126187e-01]
 [1.48403123e+09 1.60913790e+09]
 [1.26627698e+09 1.38254594e+09]
 [2.50723356e+01 3.03356652e+01]
 [7.03140853e-01 8.11558688e-01]
 [5.59422084e+02 6.16375120e+02]
 [8.41128896e-01 9.51046493e-01]
 [4.16346098e+01 4.50711126e+01]
 [1.90302769e+00 2.34476900e+00]
 [2.71675237e+02 2.94313354e+02]
 [1.00000000e+00 1.29141483e+00]]
before probe, param.shape = (25, 27)
before probe, target = (25,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.37400482500437, 'spark.broadcast.checksum': 0.20669236085287168, 'spark.broadcast.compress': 0.4868944764822436, 'spark.default.parallelism': 390.35410896652405, 'spark.executor.cores': 2.4218197470390246, 'spark.executor.instances': 7.3035489111338325, 'spark.executor.memory': 6.343460609699667, 'spark.executor.memoryOverhead': 706.555227027658, 'spark.kryoserializer.buffer': 40.62818845588252, 'spark.kryoserializer.buffer.max': 115.89868651943794, 'spark.locality.wait': 6.279614130064429, 'spark.maxRemoteBlockSizeFetchToMem': 2036254346.5102472, 'spark.memory.fraction': 0.5559827024911917, 'spark.memory.offHeap.enabled': 0.35082228758478107, 'spark.memory.offHeap.size': 822.7946827545795, 'spark.memory.storageFraction': 0.6126611805092518, 'spark.rdd.compress': 0.354842200553778, 'spark.reducer.maxBlocksInFlightPerAddress': 1509048220.8086276, 'spark.reducer.maxReqsInFlight': 1284615544.3309307, 'spark.reducer.maxSizeInFlight': 29.446225616944, 'spark.scheduler.mode': 0.7756458477441284, 'spark.scheduler.revive.interval': 574.4070551493014, 'spark.shuffle.compress': 0.8965630919897996, 'spark.shuffle.file.buffer': 44.90871823009042, 'spark.shuffle.io.numConnectionsPerPeer': 2.0317944477126675, 'spark.shuffle.sort.bypassMergeThreshold': 290.0443956638234, 'spark.storage.memoryMapThreshold': 1.0575675568494642} target = -257.736
x_probe target = -257.736
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
================= config21 =================
2022年 01月 27日 星期四 10:32:00 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (152461) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (12677) - 没有那个进程
================= config22 =================
2022年 01月 27日 星期四 10:35:49 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (155048) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (15103) - 没有那个进程
================= config23 =================
2022年 01月 27日 星期四 10:40:05 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (157108) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (17217) - 没有那个进程
bounds = 
[[5.88465662e+01 6.19651264e+01]
 [1.45651115e-01 2.44313096e-01]
 [4.65950680e-01 5.68454227e-01]
 [3.78763542e+02 4.08803048e+02]
 [2.39446318e+00 2.71034004e+00]
 [7.23065433e+00 7.63140946e+00]
 [5.97204793e+00 6.36941276e+00]
 [7.08875251e+02 7.57776201e+02]
 [3.50117954e+01 4.42005085e+01]
 [1.09941068e+02 1.19362386e+02]
 [5.76095816e+00 6.38905287e+00]
 [1.92744171e+09 2.03475255e+09]
 [5.30385340e-01 5.71202499e-01]
 [2.81108417e-01 3.87032138e-01]
 [7.99781471e+02 8.53751010e+02]
 [5.93096029e-01 6.34525056e-01]
 [2.76229207e-01 3.74657925e-01]
 [1.49028656e+09 1.60288256e+09]
 [1.27209043e+09 1.37673249e+09]
 [2.53355021e+01 3.00724988e+01]
 [7.08561745e-01 8.06137796e-01]
 [5.62269735e+02 6.13527468e+02]
 [8.46624776e-01 9.45550613e-01]
 [4.18064349e+01 4.48992874e+01]
 [1.92511476e+00 2.32268194e+00]
 [2.72807143e+02 2.93181448e+02]
 [1.00000000e+00 1.27484960e+00]]
before probe, param.shape = (26, 27)
before probe, target = (26,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 59.62658010513752, 'spark.broadcast.checksum': 0.20280488819395098, 'spark.broadcast.compress': 0.5540187596093776, 'spark.default.parallelism': 386.2459711139289, 'spark.executor.cores': 2.4774683900088905, 'spark.executor.instances': 7.451357162872794, 'spark.executor.memory': 6.334196489469276, 'spark.executor.memoryOverhead': 727.623941694705, 'spark.kryoserializer.buffer': 35.242217604990564, 'spark.kryoserializer.buffer.max': 116.38826650213278, 'spark.locality.wait': 6.337844716545888, 'spark.maxRemoteBlockSizeFetchToMem': 1932880068.4807427, 'spark.memory.fraction': 0.5393024932064591, 'spark.memory.offHeap.enabled': 0.2811117167121986, 'spark.memory.offHeap.size': 839.7678533246382, 'spark.memory.storageFraction': 0.6294013440266805, 'spark.rdd.compress': 0.33528768846711693, 'spark.reducer.maxBlocksInFlightPerAddress': 1494679085.2964005, 'spark.reducer.maxReqsInFlight': 1303631199.717379, 'spark.reducer.maxSizeInFlight': 28.568287626125027, 'spark.scheduler.mode': 0.7447520650025395, 'spark.scheduler.revive.interval': 590.144004355755, 'spark.shuffle.compress': 0.8831636574920017, 'spark.shuffle.file.buffer': 43.17270502230025, 'spark.shuffle.io.numConnectionsPerPeer': 2.189509110795915, 'spark.shuffle.sort.bypassMergeThreshold': 285.1255241328051, 'spark.storage.memoryMapThreshold': 1.1169646598036875} target = -219.928
x_probe target = -219.928
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.90024942e+01 6.18091983e+01]
 [1.50584214e-01 2.39379997e-01]
 [4.71075857e-01 5.63329049e-01]
 [3.80265518e+02 4.07301073e+02]
 [2.41025702e+00 2.69454620e+00]
 [7.25069208e+00 7.61137170e+00]
 [5.99191617e+00 6.34954452e+00]
 [7.11320298e+02 7.55331154e+02]
 [3.54712311e+01 4.37410728e+01]
 [1.10412134e+02 1.18891320e+02]
 [5.79236290e+00 6.35764814e+00]
 [1.93280725e+09 2.02938701e+09]
 [5.32426198e-01 5.69161641e-01]
 [2.86404603e-01 3.81735952e-01]
 [8.02479948e+02 8.51052533e+02]
 [5.95167481e-01 6.32453605e-01]
 [2.81150643e-01 3.69736489e-01]
 [1.49591636e+09 1.59725276e+09]
 [1.27732253e+09 1.37150039e+09]
 [2.55723519e+01 2.98356489e+01]
 [7.13440547e-01 8.01258994e-01]
 [5.64832622e+02 6.10964582e+02]
 [8.51571068e-01 9.40604321e-01]
 [4.19610775e+01 4.47446448e+01]
 [1.94499312e+00 2.30280358e+00]
 [2.73825858e+02 2.92162733e+02]
 [1.00000000e+00 1.25994090e+00]]
before probe, param.shape = (27, 27)
before probe, target = (27,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.302555965803606, 'spark.broadcast.checksum': 0.1819549297899743, 'spark.broadcast.compress': 0.5457739263795246, 'spark.default.parallelism': 397.4027319347279, 'spark.executor.cores': 2.4500180130703053, 'spark.executor.instances': 7.259200517562683, 'spark.executor.memory': 6.120939901443566, 'spark.executor.memoryOverhead': 731.7787669964505, 'spark.kryoserializer.buffer': 36.15137376817155, 'spark.kryoserializer.buffer.max': 117.87382215755889, 'spark.locality.wait': 5.989311933233621, 'spark.maxRemoteBlockSizeFetchToMem': 1965071569.9795837, 'spark.memory.fraction': 0.5439753826328297, 'spark.memory.offHeap.enabled': 0.37565231823385126, 'spark.memory.offHeap.size': 829.4963270373858, 'spark.memory.storageFraction': 0.6162523515713079, 'spark.rdd.compress': 0.3663836184236556, 'spark.reducer.maxBlocksInFlightPerAddress': 1536135617.5388427, 'spark.reducer.maxReqsInFlight': 1322097520.2684543, 'spark.reducer.maxSizeInFlight': 28.146548258299333, 'spark.scheduler.mode': 0.7636472814434542, 'spark.scheduler.revive.interval': 603.100115665219, 'spark.shuffle.compress': 0.9064737307374242, 'spark.shuffle.file.buffer': 42.13212465338877, 'spark.shuffle.io.numConnectionsPerPeer': 2.1051429605622265, 'spark.shuffle.sort.bypassMergeThreshold': 274.599995875615, 'spark.storage.memoryMapThreshold': 1.2381343643633251} target = -247.427
x_probe target = -247.427
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.91428294e+01 6.16688631e+01]
 [1.55024003e-01 2.34940207e-01]
 [4.75688517e-01 5.58716390e-01]
 [3.81617295e+02 4.05949295e+02]
 [2.42447148e+00 2.68033174e+00]
 [7.26872606e+00 7.59333772e+00]
 [6.00979759e+00 6.33166311e+00]
 [7.13520841e+02 7.53130611e+02]
 [3.58847231e+01 4.33275807e+01]
 [1.10836093e+02 1.18467361e+02]
 [5.82062716e+00 6.32938388e+00]
 [1.93763624e+09 2.02455802e+09]
 [5.34262971e-01 5.67324869e-01]
 [2.91171171e-01 3.76969385e-01]
 [8.04908577e+02 8.48623904e+02]
 [5.97031787e-01 6.30589299e-01]
 [2.85579935e-01 3.65307197e-01]
 [1.50098318e+09 1.59218594e+09]
 [1.28203142e+09 1.36679149e+09]
 [2.57855168e+01 2.96224841e+01]
 [7.17831469e-01 7.96868071e-01]
 [5.67139220e+02 6.08657984e+02]
 [8.56022730e-01 9.36152659e-01]
 [4.21002559e+01 4.46054665e+01]
 [1.96288364e+00 2.28491306e+00]
 [2.74742702e+02 2.91245889e+02]
 [1.00500208e+00 1.24652307e+00]]
before probe, param.shape = (28, 27)
before probe, target = (28,)
================= config24 =================
2022年 01月 27日 星期四 10:44:01 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (159468) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (19231) - 没有那个进程
================= config25 =================
2022年 01月 27日 星期四 10:47:22 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (161524) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (21345) - 没有那个进程
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.31395807517678, 'spark.broadcast.checksum': 0.1910104293758853, 'spark.broadcast.compress': 0.5415785172603483, 'spark.default.parallelism': 391.22855694323505, 'spark.executor.cores': 2.463313023691475, 'spark.executor.instances': 7.399737656911363, 'spark.executor.memory': 6.049327076517307, 'spark.executor.memoryOverhead': 733.0334455254441, 'spark.kryoserializer.buffer': 42.296130811799685, 'spark.kryoserializer.buffer.max': 116.59245343615665, 'spark.locality.wait': 6.061248733807595, 'spark.maxRemoteBlockSizeFetchToMem': 1948493980.1000984, 'spark.memory.fraction': 0.5526050024033476, 'spark.memory.offHeap.enabled': 0.3732788195309126, 'spark.memory.offHeap.size': 834.3666043606343, 'spark.memory.storageFraction': 0.6172702470363983, 'spark.rdd.compress': 0.34988565851270564, 'spark.reducer.maxBlocksInFlightPerAddress': 1532613683.163599, 'spark.reducer.maxReqsInFlight': 1343003211.8764403, 'spark.reducer.maxSizeInFlight': 26.839890868231013, 'spark.scheduler.mode': 0.7604460389004424, 'spark.scheduler.revive.interval': 582.0366716408578, 'spark.shuffle.compress': 0.8864373607227588, 'spark.shuffle.file.buffer': 42.109101737592106, 'spark.shuffle.io.numConnectionsPerPeer': 2.0263841163667564, 'spark.shuffle.sort.bypassMergeThreshold': 285.886992614252, 'spark.storage.memoryMapThreshold': 1.0660886783388253} target = -227.573
x_probe target = -227.573
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.92691311e+01 6.15425615e+01]
 [1.59019813e-01 2.30944397e-01]
 [4.79839910e-01 5.54564996e-01]
 [3.82833895e+02 4.04732695e+02]
 [2.43726450e+00 2.66753872e+00]
 [7.28495665e+00 7.57710714e+00]
 [6.02589087e+00 6.31556983e+00]
 [7.15501330e+02 7.51150122e+02]
 [3.62568660e+01 4.29554378e+01]
 [1.11217657e+02 1.18085797e+02]
 [5.84606499e+00 6.30394604e+00]
 [1.94198233e+09 2.02021193e+09]
 [5.35916065e-01 5.65671774e-01]
 [2.95461081e-01 3.72679474e-01]
 [8.07094343e+02 8.46438137e+02]
 [5.98709663e-01 6.28911423e-01]
 [2.89566298e-01 3.61320834e-01]
 [1.50554332e+09 1.58762581e+09]
 [1.28626943e+09 1.36255349e+09]
 [2.59773652e+01 2.94306357e+01]
 [7.21783300e-01 7.92916241e-01]
 [5.69215158e+02 6.06582045e+02]
 [8.60029227e-01 9.32146162e-01]
 [4.22255164e+01 4.44802059e+01]
 [1.97898511e+00 2.26881158e+00]
 [2.75567861e+02 2.90420729e+02]
 [1.01707813e+00 1.23444702e+00]]
before probe, param.shape = (29, 27)
before probe, target = (29,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 59.81192603149219, 'spark.broadcast.checksum': 0.20686315920398113, 'spark.broadcast.compress': 0.5235897951994143, 'spark.default.parallelism': 383.54819190657844, 'spark.executor.cores': 2.613025113225668, 'spark.executor.instances': 7.518414021447581, 'spark.executor.memory': 6.142330614500049, 'spark.executor.memoryOverhead': 745.8549517847036, 'spark.kryoserializer.buffer': 36.7109284784138, 'spark.kryoserializer.buffer.max': 116.33092226531328, 'spark.locality.wait': 6.109374523696308, 'spark.maxRemoteBlockSizeFetchToMem': 2008857804.3571148, 'spark.memory.fraction': 0.545462309775948, 'spark.memory.offHeap.enabled': 0.3383499670080304, 'spark.memory.offHeap.size': 810.2601287171537, 'spark.memory.storageFraction': 0.6095840751564283, 'spark.rdd.compress': 0.34105586128396165, 'spark.reducer.maxBlocksInFlightPerAddress': 1555310617.754913, 'spark.reducer.maxReqsInFlight': 1297979999.2113042, 'spark.reducer.maxSizeInFlight': 26.98514428734974, 'spark.scheduler.mode': 0.7315532133569455, 'spark.scheduler.revive.interval': 572.567874022553, 'spark.shuffle.compress': 0.9252270620004989, 'spark.shuffle.file.buffer': 43.79393816344273, 'spark.shuffle.io.numConnectionsPerPeer': 2.161220365087435, 'spark.shuffle.sort.bypassMergeThreshold': 281.00063953165335, 'spark.storage.memoryMapThreshold': 1.1966485398420417} target = -192.148
x_probe target = -192.148
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.93828026e+01 6.14288899e+01]
 [1.62616042e-01 2.27348168e-01]
 [4.83576165e-01 5.50828742e-01]
 [3.83928835e+02 4.03637755e+02]
 [2.44877821e+00 2.65602501e+00]
 [7.29956417e+00 7.56249961e+00]
 [6.04037481e+00 6.30108588e+00]
 [7.17283769e+02 7.49367683e+02]
 [3.65917946e+01 4.26205092e+01]
 [1.11561064e+02 1.17742390e+02]
 [5.86895905e+00 6.28105199e+00]
 [1.94589381e+09 2.01630045e+09]
 [5.37403851e-01 5.64183989e-01]
 [2.99322001e-01 3.68818554e-01]
 [8.09061533e+02 8.44470948e+02]
 [6.00219751e-01 6.27401335e-01]
 [2.93154025e-01 3.57733107e-01]
 [1.50964744e+09 1.58352168e+09]
 [1.29008363e+09 1.35873929e+09]
 [2.61500287e+01 2.92579722e+01]
 [7.25339947e-01 7.89359594e-01]
 [5.71083503e+02 6.04713701e+02]
 [8.63635074e-01 9.28540316e-01]
 [4.23382509e+01 4.43674715e+01]
 [1.99347643e+00 2.25432026e+00]
 [2.76310504e+02 2.89678086e+02]
 [1.02794658e+00 1.22357857e+00]]
before probe, param.shape = (30, 27)
before probe, target = (30,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 61.39372107010247, 'spark.broadcast.checksum': 0.19361989658142875, 'spark.broadcast.compress': 0.5007378704711467, 'spark.default.parallelism': 384.5724535975289, 'spark.executor.cores': 2.4617681344978197, 'spark.executor.instances': 7.445949833563311, 'spark.executor.memory': 6.06672949840108, 'spark.executor.memoryOverhead': 745.8928639806035, 'spark.kryoserializer.buffer': 41.35335213624708, 'spark.kryoserializer.buffer.max': 116.32715572053235, 'spark.locality.wait': 6.0967885598571225, 'spark.maxRemoteBlockSizeFetchToMem': 1987692720.9154546, 'spark.memory.fraction': 0.5435496849690246, 'spark.memory.offHeap.enabled': 0.3462666818284814, 'spark.memory.offHeap.size': 844.420078559289, 'spark.memory.storageFraction': 0.6057168808652927, 'spark.rdd.compress': 0.34440350635704803, 'spark.reducer.maxBlocksInFlightPerAddress': 1560245392.3491864, 'spark.reducer.maxReqsInFlight': 1354222170.452711, 'spark.reducer.maxSizeInFlight': 27.95090599799148, 'spark.scheduler.mode': 0.7322156320494699, 'spark.scheduler.revive.interval': 584.8850289938176, 'spark.shuffle.compress': 0.8761435571901175, 'spark.shuffle.file.buffer': 42.72301765430749, 'spark.shuffle.io.numConnectionsPerPeer': 2.026666551366758, 'spark.shuffle.sort.bypassMergeThreshold': 284.58296865904146, 'spark.storage.memoryMapThreshold': 1.1621006972494434} target = -226.034
x_probe target = -226.034
================= config26 =================
2022年 01月 27日 星期四 10:51:18 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (164573) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (23998) - 没有那个进程
================= config27 =================
2022年 01月 27日 星期四 10:55:46 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (167288) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (26261) - 没有那个进程
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.94851070e+01 6.13265856e+01]
 [1.65852649e-01 2.24111562e-01]
 [4.86938794e-01 5.47466113e-01]
 [3.84914281e+02 4.02652309e+02]
 [2.45914055e+00 2.64566267e+00]
 [7.31271094e+00 7.54935284e+00]
 [6.05341037e+00 6.28805033e+00]
 [7.18887965e+02 7.47763487e+02]
 [3.68932303e+01 4.23190735e+01]
 [1.11870130e+02 1.17433324e+02]
 [5.88956369e+00 6.26044734e+00]
 [1.94941414e+09 2.01278012e+09]
 [5.38742858e-01 5.62844982e-01]
 [3.02796829e-01 3.65343727e-01]
 [8.10832004e+02 8.42700477e+02]
 [6.01578830e-01 6.26042256e-01]
 [2.96382979e-01 3.54504153e-01]
 [1.51334116e+09 1.57982797e+09]
 [1.29351641e+09 1.35530650e+09]
 [2.63054259e+01 2.91025750e+01]
 [7.28540929e-01 7.86158612e-01]
 [5.72765012e+02 6.03032191e+02]
 [8.66880336e-01 9.25295053e-01]
 [4.24397119e+01 4.42660104e+01]
 [2.00651862e+00 2.24127807e+00]
 [2.76978884e+02 2.89009707e+02]
 [1.03772818e+00 1.21379697e+00]]
before probe, param.shape = (31, 27)
before probe, target = (31,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.48588742313982, 'spark.broadcast.checksum': 0.1888801957746455, 'spark.broadcast.compress': 0.5094816759266785, 'spark.default.parallelism': 391.429249173586, 'spark.executor.cores': 2.4929366435483953, 'spark.executor.instances': 7.462232525940491, 'spark.executor.memory': 6.234306947187594, 'spark.executor.memoryOverhead': 746.3198185017972, 'spark.kryoserializer.buffer': 38.11145353218128, 'spark.kryoserializer.buffer.max': 116.93331509613458, 'spark.locality.wait': 6.1355883715629105, 'spark.maxRemoteBlockSizeFetchToMem': 1966987999.9715664, 'spark.memory.fraction': 0.5597084144426206, 'spark.memory.offHeap.enabled': 0.3564147289482951, 'spark.memory.offHeap.size': 827.712276060934, 'spark.memory.storageFraction': 0.6042981348679485, 'spark.rdd.compress': 0.3250289899218078, 'spark.reducer.maxBlocksInFlightPerAddress': 1566828701.3132982, 'spark.reducer.maxReqsInFlight': 1300875881.2062314, 'spark.reducer.maxSizeInFlight': 28.46865897591819, 'spark.scheduler.mode': 0.7635051890754871, 'spark.scheduler.revive.interval': 595.2183622245497, 'spark.shuffle.compress': 0.8936320943707162, 'spark.shuffle.file.buffer': 43.393222109619174, 'spark.shuffle.io.numConnectionsPerPeer': 2.2245311615099874, 'spark.shuffle.sort.bypassMergeThreshold': 286.0697513612452, 'spark.storage.memoryMapThreshold': 1.1099392343798922} target = -258.976
x_probe target = -258.976
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.95771809e+01 6.12345116e+01]
 [1.68765594e-01 2.21198616e-01]
 [4.89965160e-01 5.44439747e-01]
 [3.85801183e+02 4.01765407e+02]
 [2.46846665e+00 2.63633657e+00]
 [7.32454304e+00 7.53752074e+00]
 [6.06514237e+00 6.27631833e+00]
 [7.20331741e+02 7.46319711e+02]
 [3.71645225e+01 4.20477814e+01]
 [1.12148290e+02 1.17155164e+02]
 [5.90810788e+00 6.24190316e+00]
 [1.95258244e+09 2.00961182e+09]
 [5.39947964e-01 5.61639876e-01]
 [3.05924174e-01 3.62216382e-01]
 [8.12425427e+02 8.41107053e+02]
 [6.02802001e-01 6.24819085e-01]
 [2.99289038e-01 3.51598094e-01]
 [1.51666550e+09 1.57650363e+09]
 [1.29660592e+09 1.35221700e+09]
 [2.64452833e+01 2.89627175e+01]
 [7.31421813e-01 7.83277728e-01]
 [5.74278371e+02 6.01518832e+02]
 [8.69801072e-01 9.22374318e-01]
 [4.25310269e+01 4.41746955e+01]
 [2.01825660e+00 2.22954010e+00]
 [2.77580425e+02 2.88408166e+02]
 [1.04653162e+00 1.20499353e+00]]
before probe, param.shape = (32, 27)
before probe, target = (32,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 59.703190052142276, 'spark.broadcast.checksum': 0.20553244078166588, 'spark.broadcast.compress': 0.4940633079208878, 'spark.default.parallelism': 392.48612107636524, 'spark.executor.cores': 2.5336279291797745, 'spark.executor.instances': 7.495793470580085, 'spark.executor.memory': 6.202975138546453, 'spark.executor.memoryOverhead': 724.57438147316, 'spark.kryoserializer.buffer': 41.05747256186357, 'spark.kryoserializer.buffer.max': 116.16241255791087, 'spark.locality.wait': 5.9630717466034655, 'spark.maxRemoteBlockSizeFetchToMem': 1996113214.886207, 'spark.memory.fraction': 0.5461023879314045, 'spark.memory.offHeap.enabled': 0.34366165717461356, 'spark.memory.offHeap.size': 839.9894444837831, 'spark.memory.storageFraction': 0.6221094086504828, 'spark.rdd.compress': 0.30827504000887324, 'spark.reducer.maxBlocksInFlightPerAddress': 1538596000.9755278, 'spark.reducer.maxReqsInFlight': 1334526547.3208861, 'spark.reducer.maxSizeInFlight': 28.897846870469078, 'spark.scheduler.mode': 0.7598485854469417, 'spark.scheduler.revive.interval': 576.143203651252, 'spark.shuffle.compress': 0.9172114578273136, 'spark.shuffle.file.buffer': 44.08681998606072, 'spark.shuffle.io.numConnectionsPerPeer': 2.0449373304327967, 'spark.shuffle.sort.bypassMergeThreshold': 282.54148595175735, 'spark.storage.memoryMapThreshold': 1.1197167325505168} target = -219.173
x_probe target = -219.173
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
================= config28 =================
2022年 01月 27日 星期四 10:59:34 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (169028) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (28090) - 没有那个进程
================= config29 =================
2022年 01月 27日 星期四 11:02:52 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (171714) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (30351) - 没有那个进程
================= config30 =================
2022年 01月 27日 星期四 11:06:38 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (173762) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (32475) - 没有那个进程
bounds = 
[[5.96600474e+01 6.11516451e+01]
 [1.71387245e-01 2.18576965e-01]
 [4.92688889e-01 5.41716018e-01]
 [3.86599394e+02 4.00967196e+02]
 [2.47686015e+00 2.62794307e+00]
 [7.33519192e+00 7.52687186e+00]
 [6.07570116e+00 6.26575953e+00]
 [7.21631140e+02 7.45020312e+02]
 [3.74086854e+01 4.18036184e+01]
 [1.12398634e+02 1.16904820e+02]
 [5.92479764e+00 6.22521339e+00]
 [1.95543391e+09 2.00676035e+09]
 [5.41032560e-01 5.60555280e-01]
 [3.08738784e-01 3.59401771e-01]
 [8.13859509e+02 8.39672972e+02]
 [6.03902855e-01 6.23718230e-01]
 [3.01904490e-01 3.48982641e-01]
 [1.51965740e+09 1.57351172e+09]
 [1.29938647e+09 1.34943645e+09]
 [2.65711550e+01 2.88368458e+01]
 [7.34014609e-01 7.80684932e-01]
 [5.75640394e+02 6.00156809e+02]
 [8.72429734e-01 9.19745655e-01]
 [4.26132103e+01 4.40925121e+01]
 [2.02882077e+00 2.21897592e+00]
 [2.78121812e+02 2.87866779e+02]
 [1.05445471e+00 1.19707044e+00]]
before probe, param.shape = (33, 27)
before probe, target = (33,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.5212167912552, 'spark.broadcast.checksum': 0.19329087283885493, 'spark.broadcast.compress': 0.49775498739872776, 'spark.default.parallelism': 394.83559319808927, 'spark.executor.cores': 2.5649664825477325, 'spark.executor.instances': 7.366089097650683, 'spark.executor.memory': 6.09683372788111, 'spark.executor.memoryOverhead': 722.9369743846131, 'spark.kryoserializer.buffer': 41.023541020110315, 'spark.kryoserializer.buffer.max': 115.78643032018326, 'spark.locality.wait': 6.00797018498777, 'spark.maxRemoteBlockSizeFetchToMem': 1988392510.0881066, 'spark.memory.fraction': 0.555344229249316, 'spark.memory.offHeap.enabled': 0.34686126662690525, 'spark.memory.offHeap.size': 818.0402036848973, 'spark.memory.storageFraction': 0.6107658903385198, 'spark.rdd.compress': 0.3019638725609557, 'spark.reducer.maxBlocksInFlightPerAddress': 1555171533.1813211, 'spark.reducer.maxReqsInFlight': 1322129560.617793, 'spark.reducer.maxSizeInFlight': 28.570279268324228, 'spark.scheduler.mode': 0.7805301633447117, 'spark.scheduler.revive.interval': 591.1284096273292, 'spark.shuffle.compress': 0.8874617739596827, 'spark.shuffle.file.buffer': 43.1706260311813, 'spark.shuffle.io.numConnectionsPerPeer': 2.1630390269598267, 'spark.shuffle.sort.bypassMergeThreshold': 281.3660416366416, 'spark.storage.memoryMapThreshold': 1.1398257726746954} target = -189.194
x_probe target = -189.194
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.97346273e+01 6.10770652e+01]
 [1.73746731e-01 2.16217479e-01]
 [4.95140245e-01 5.39264661e-01]
 [3.87317784e+02 4.00248806e+02]
 [2.48441429e+00 2.62038892e+00]
 [7.34477592e+00 7.51728786e+00]
 [6.08520408e+00 6.25625661e+00]
 [7.22800598e+02 7.43850854e+02]
 [3.76284321e+01 4.15838718e+01]
 [1.12623943e+02 1.16679511e+02]
 [5.93981843e+00 6.21019261e+00]
 [1.95800023e+09 2.00419403e+09]
 [5.42008696e-01 5.59579144e-01]
 [3.11271933e-01 3.56868622e-01]
 [8.15150182e+02 8.38382299e+02]
 [6.04893624e-01 6.22727462e-01]
 [3.04258398e-01 3.46628734e-01]
 [1.52235012e+09 1.57081901e+09]
 [1.30188897e+09 1.34693395e+09]
 [2.66844396e+01 2.87235613e+01]
 [7.36348125e-01 7.78351416e-01]
 [5.76866215e+02 5.98930988e+02]
 [8.74795530e-01 9.17379859e-01]
 [4.26871754e+01 4.40185470e+01]
 [2.03832853e+00 2.20946816e+00]
 [2.78609060e+02 2.87379530e+02]
 [1.06158550e+00 1.18993965e+00]]
before probe, param.shape = (34, 27)
before probe, target = (34,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.38318955548768, 'spark.broadcast.checksum': 0.1870397831009814, 'spark.broadcast.compress': 0.4955867354819097, 'spark.default.parallelism': 388.4435141089154, 'spark.executor.cores': 2.6100529919435247, 'spark.executor.instances': 7.484655956838662, 'spark.executor.memory': 6.126478495271525, 'spark.executor.memoryOverhead': 727.6711698594048, 'spark.kryoserializer.buffer': 38.954599348179826, 'spark.kryoserializer.buffer.max': 116.6207585877997, 'spark.locality.wait': 6.187659901503636, 'spark.maxRemoteBlockSizeFetchToMem': 1979724144.3282006, 'spark.memory.fraction': 0.5563197411079315, 'spark.memory.offHeap.enabled': 0.312492101263336, 'spark.memory.offHeap.size': 832.9292384585581, 'spark.memory.storageFraction': 0.6211900220111337, 'spark.rdd.compress': 0.3092212268280758, 'spark.reducer.maxBlocksInFlightPerAddress': 1544925530.2124157, 'spark.reducer.maxReqsInFlight': 1311313297.291423, 'spark.reducer.maxSizeInFlight': 27.468201426699817, 'spark.scheduler.mode': 0.772613519731529, 'spark.scheduler.revive.interval': 588.7394058570026, 'spark.shuffle.compress': 0.9117672475568998, 'spark.shuffle.file.buffer': 43.61943560077491, 'spark.shuffle.io.numConnectionsPerPeer': 2.0554098738820406, 'spark.shuffle.sort.bypassMergeThreshold': 287.0862231288875, 'spark.storage.memoryMapThreshold': 1.132167016469009} target = -217.742
x_probe target = -217.742
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.98017492e+01 6.10099433e+01]
 [1.75870269e-01 2.14093942e-01]
 [4.97346466e-01 5.37058440e-01]
 [3.87964335e+02 3.99602255e+02]
 [2.49121303e+00 2.61359019e+00]
 [7.35340152e+00 7.50866227e+00]
 [6.09375671e+00 6.24770399e+00]
 [7.23853111e+02 7.42798341e+02]
 [3.78262041e+01 4.13860998e+01]
 [1.12826721e+02 1.16476733e+02]
 [5.95333714e+00 6.19667390e+00]
 [1.96030992e+09 2.00188434e+09]
 [5.42887218e-01 5.58700622e-01]
 [3.13551768e-01 3.54588787e-01]
 [8.16311788e+02 8.37220693e+02]
 [6.05785316e-01 6.21835770e-01]
 [3.06376915e-01 3.44510217e-01]
 [1.52477356e+09 1.56839556e+09]
 [1.30414122e+09 1.34468170e+09]
 [2.67863957e+01 2.86216052e+01]
 [7.38448290e-01 7.76251251e-01]
 [5.77969454e+02 5.97827750e+02]
 [8.76924746e-01 9.15250643e-01]
 [4.27537440e+01 4.39519784e+01]
 [2.04688551e+00 2.20091118e+00]
 [2.79047584e+02 2.86941007e+02]
 [1.06800321e+00 1.18352195e+00]]
before probe, param.shape = (35, 27)
before probe, target = (35,)
================= config31 =================
2022年 01月 27日 星期四 11:10:33 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (176197) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (34505) - 没有那个进程
================= config32 =================
2022年 01月 27日 星期四 11:13:47 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (178988) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (36853) - 没有那个进程
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.99260699315098, 'spark.broadcast.checksum': 0.20903057138703357, 'spark.broadcast.compress': 0.5300466976398225, 'spark.default.parallelism': 391.19987220964873, 'spark.executor.cores': 2.5682823258753342, 'spark.executor.instances': 7.425373812810935, 'spark.executor.memory': 6.2082806300653335, 'spark.executor.memoryOverhead': 726.1618610627235, 'spark.kryoserializer.buffer': 41.00157654218363, 'spark.kryoserializer.buffer.max': 114.14434205482675, 'spark.locality.wait': 5.973795356378464, 'spark.maxRemoteBlockSizeFetchToMem': 1962598963.4770572, 'spark.memory.fraction': 0.5497560993589945, 'spark.memory.offHeap.enabled': 0.3169072260603564, 'spark.memory.offHeap.size': 823.9849794116237, 'spark.memory.storageFraction': 0.6080454766816371, 'spark.rdd.compress': 0.31855609061764495, 'spark.reducer.maxBlocksInFlightPerAddress': 1556881280.5908792, 'spark.reducer.maxReqsInFlight': 1334457747.3757324, 'spark.reducer.maxSizeInFlight': 28.54993820927774, 'spark.scheduler.mode': 0.750611782828449, 'spark.scheduler.revive.interval': 592.2348573685437, 'spark.shuffle.compress': 0.8818383350852796, 'spark.shuffle.file.buffer': 43.71261505282031, 'spark.shuffle.io.numConnectionsPerPeer': 2.050659472585677, 'spark.shuffle.sort.bypassMergeThreshold': 280.2776348778021, 'spark.storage.memoryMapThreshold': 1.183342157562729} target = -226.009
x_probe target = -226.009
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.98621589e+01 6.09495336e+01]
 [1.77781452e-01 2.12182758e-01]
 [4.99332065e-01 5.35072842e-01]
 [3.88546231e+02 3.99020359e+02]
 [2.49733188e+00 2.60747133e+00]
 [7.36116455e+00 7.50089923e+00]
 [6.10145407e+00 6.24000662e+00]
 [7.24800373e+02 7.41851080e+02]
 [3.80041989e+01 4.12081050e+01]
 [1.13009222e+02 1.16294232e+02]
 [5.96550397e+00 6.18450706e+00]
 [1.96238864e+09 1.99980562e+09]
 [5.43677888e-01 5.57909952e-01]
 [3.15603619e-01 3.52536937e-01]
 [8.17357233e+02 8.36175248e+02]
 [6.06587839e-01 6.21033247e-01]
 [3.08283580e-01 3.42603552e-01]
 [1.52695466e+09 1.56621446e+09]
 [1.30616824e+09 1.34265467e+09]
 [2.68781561e+01 2.85298447e+01]
 [7.40338438e-01 7.74361103e-01]
 [5.78962369e+02 5.96834835e+02]
 [8.78841041e-01 9.13334348e-01]
 [4.28136557e+01 4.38920667e+01]
 [2.05458679e+00 2.19320990e+00]
 [2.79442255e+02 2.86546336e+02]
 [1.07377914e+00 1.17774601e+00]]
before probe, param.shape = (36, 27)
before probe, target = (36,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.0596535525802, 'spark.broadcast.checksum': 0.1844438948822082, 'spark.broadcast.compress': 0.5285752328130491, 'spark.default.parallelism': 394.69111027887686, 'spark.executor.cores': 2.542171866321441, 'spark.executor.instances': 7.3649809350683695, 'spark.executor.memory': 6.202712829662689, 'spark.executor.memoryOverhead': 733.9578623272462, 'spark.kryoserializer.buffer': 39.72149634061593, 'spark.kryoserializer.buffer.max': 115.72897697123265, 'spark.locality.wait': 6.130208597583817, 'spark.maxRemoteBlockSizeFetchToMem': 1966860902.488517, 'spark.memory.fraction': 0.5466624510434689, 'spark.memory.offHeap.enabled': 0.328727143470245, 'spark.memory.offHeap.size': 820.924011315251, 'spark.memory.storageFraction': 0.6141971594606733, 'spark.rdd.compress': 0.32889052142022485, 'spark.reducer.maxBlocksInFlightPerAddress': 1553509762.2163675, 'spark.reducer.maxReqsInFlight': 1320114692.6832979, 'spark.reducer.maxSizeInFlight': 28.038245217498844, 'spark.scheduler.mode': 0.7648090691536086, 'spark.scheduler.revive.interval': 587.5074608198573, 'spark.shuffle.compress': 0.9036089137745085, 'spark.shuffle.file.buffer': 43.189803966473136, 'spark.shuffle.io.numConnectionsPerPeer': 2.124140597221014, 'spark.shuffle.sort.bypassMergeThreshold': 283.51730507531, 'spark.storage.memoryMapThreshold': 1.1254236226237357} target = -185.313
x_probe target = -185.313
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.99165277e+01 6.08951649e+01]
 [1.79501518e-01 2.10462693e-01]
 [5.01119104e-01 5.33285803e-01]
 [3.89069937e+02 3.98496653e+02]
 [2.50283886e+00 2.60196436e+00]
 [7.36815129e+00 7.49391249e+00]
 [6.10838170e+00 6.23307900e+00]
 [7.25652908e+02 7.40998544e+02]
 [3.81643942e+01 4.10479097e+01]
 [1.13173472e+02 1.16129982e+02]
 [5.97645413e+00 6.17355691e+00]
 [1.96425949e+09 1.99793477e+09]
 [5.44389491e-01 5.57198348e-01]
 [3.17450285e-01 3.50690271e-01]
 [8.18298134e+02 8.35234347e+02]
 [6.07310109e-01 6.20310977e-01]
 [3.09999578e-01 3.40887553e-01]
 [1.52891765e+09 1.56425147e+09]
 [1.30799257e+09 1.34083035e+09]
 [2.69607406e+01 2.84472603e+01]
 [7.42039571e-01 7.72659970e-01]
 [5.79855992e+02 5.95941212e+02]
 [8.80565707e-01 9.11609683e-01]
 [4.28675762e+01 4.38381461e+01]
 [2.06151795e+00 2.18627874e+00]
 [2.79797459e+02 2.86191132e+02]
 [1.07897749e+00 1.17254766e+00]]
before probe, param.shape = (37, 27)
before probe, target = (37,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.44807448052767, 'spark.broadcast.checksum': 0.18094732089121113, 'spark.broadcast.compress': 0.5107627717026338, 'spark.default.parallelism': 392.37894153448474, 'spark.executor.cores': 2.5167366977829344, 'spark.executor.instances': 7.434158329109602, 'spark.executor.memory': 6.12326809374643, 'spark.executor.memoryOverhead': 736.0573827863291, 'spark.kryoserializer.buffer': 40.614594724873825, 'spark.kryoserializer.buffer.max': 114.53629918001393, 'spark.locality.wait': 6.054135889894403, 'spark.maxRemoteBlockSizeFetchToMem': 1973124553.693095, 'spark.memory.fraction': 0.54656841061733, 'spark.memory.offHeap.enabled': 0.3392978866402524, 'spark.memory.offHeap.size': 830.4412370525857, 'spark.memory.storageFraction': 0.6196841807559645, 'spark.rdd.compress': 0.3340568399488055, 'spark.reducer.maxBlocksInFlightPerAddress': 1559333073.4974124, 'spark.reducer.maxReqsInFlight': 1330919056.8478987, 'spark.reducer.maxSizeInFlight': 27.39796748481521, 'spark.scheduler.mode': 0.7428609966714256, 'spark.scheduler.revive.interval': 590.4815368147881, 'spark.shuffle.compress': 0.8924351314303802, 'spark.shuffle.file.buffer': 43.46169885731588, 'spark.shuffle.io.numConnectionsPerPeer': 2.0958846746126882, 'spark.shuffle.sort.bypassMergeThreshold': 281.8629441713431, 'spark.storage.memoryMapThreshold': 1.0832681166215634} target = -226.83
x_probe target = -226.83
================= config33 =================
2022年 01月 27日 星期四 11:17:43 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (180681) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (38590) - 没有那个进程
================= config34 =================
2022年 01月 27日 星期四 11:21:00 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (183195) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (40704) - 没有那个进程
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[5.99654595e+01 6.08462330e+01]
 [1.81049576e-01 2.08914634e-01]
 [5.02727439e-01 5.31677468e-01]
 [3.89541273e+02 3.98025317e+02]
 [2.50779513e+00 2.59700809e+00]
 [7.37443935e+00 7.48762443e+00]
 [6.11461657e+00 6.22684413e+00]
 [7.26420190e+02 7.40231262e+02]
 [3.83085699e+01 4.09037339e+01]
 [1.13321298e+02 1.15982156e+02]
 [5.98630927e+00 6.16370177e+00]
 [1.96594325e+09 1.99625101e+09]
 [5.45029934e-01 5.56557906e-01]
 [3.19112284e-01 3.49028271e-01]
 [8.19144944e+02 8.34387536e+02]
 [6.07960152e-01 6.19660933e-01]
 [3.11543977e-01 3.39343154e-01]
 [1.53068434e+09 1.56248478e+09]
 [1.30963445e+09 1.33918846e+09]
 [2.70350666e+01 2.83729343e+01]
 [7.43570591e-01 7.71128950e-01]
 [5.80660253e+02 5.95136951e+02]
 [8.82117905e-01 9.10057484e-01]
 [4.29161047e+01 4.37896176e+01]
 [2.06775599e+00 2.18004070e+00]
 [2.80117142e+02 2.85871448e+02]
 [1.08365600e+00 1.16786916e+00]]
before probe, param.shape = (38, 27)
before probe, target = (38,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.046701010249414, 'spark.broadcast.checksum': 0.19445895816931902, 'spark.broadcast.compress': 0.5253938411401171, 'spark.default.parallelism': 390.67105200900136, 'spark.executor.cores': 2.5517639707409407, 'spark.executor.instances': 7.407602983556377, 'spark.executor.memory': 6.180529839670274, 'spark.executor.memoryOverhead': 727.9963562565428, 'spark.kryoserializer.buffer': 38.97810712326056, 'spark.kryoserializer.buffer.max': 114.83394559225209, 'spark.locality.wait': 6.043113546879788, 'spark.maxRemoteBlockSizeFetchToMem': 1992692964.584127, 'spark.memory.fraction': 0.5542106296670503, 'spark.memory.offHeap.enabled': 0.33955738186147655, 'spark.memory.offHeap.size': 830.5562943805747, 'spark.memory.storageFraction': 0.6129954998868927, 'spark.rdd.compress': 0.3199377518444687, 'spark.reducer.maxBlocksInFlightPerAddress': 1546668521.546186, 'spark.reducer.maxReqsInFlight': 1321326652.2659557, 'spark.reducer.maxSizeInFlight': 28.3468124890011, 'spark.scheduler.mode': 0.757860315225642, 'spark.scheduler.revive.interval': 593.6612604500797, 'spark.shuffle.compress': 0.9084365276413547, 'spark.shuffle.file.buffer': 43.304619157167274, 'spark.shuffle.io.numConnectionsPerPeer': 2.094945766893092, 'spark.shuffle.sort.bypassMergeThreshold': 280.45310376633614, 'spark.storage.memoryMapThreshold': 1.1245180677022748} target = -188.5
x_probe target = -188.5
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.00094982e+01 6.08021943e+01]
 [1.82442829e-01 2.07521381e-01]
 [5.04174940e-01 5.30229966e-01]
 [3.89965475e+02 3.97601115e+02]
 [2.51225578e+00 2.59254744e+00]
 [7.38009860e+00 7.48196518e+00]
 [6.12022794e+00 6.22123275e+00]
 [7.27110743e+02 7.39540709e+02]
 [3.84383281e+01 4.07739757e+01]
 [1.13454341e+02 1.15849113e+02]
 [5.99517889e+00 6.15483214e+00]
 [1.96745864e+09 1.99473562e+09]
 [5.45606333e-01 5.55981507e-01]
 [3.20608083e-01 3.47532472e-01]
 [8.19907074e+02 8.33625407e+02]
 [6.08545191e-01 6.19075894e-01]
 [3.12933936e-01 3.37953196e-01]
 [1.53227437e+09 1.56089476e+09]
 [1.31111216e+09 1.33771076e+09]
 [2.71019599e+01 2.83060409e+01]
 [7.44948509e-01 7.69751032e-01]
 [5.81384088e+02 5.94413116e+02]
 [8.83514884e-01 9.08660505e-01]
 [4.29597804e+01 4.37459420e+01]
 [2.07337023e+00 2.17442647e+00]
 [2.80404858e+02 2.85583733e+02]
 [1.08786665e+00 1.16365850e+00]]
before probe, param.shape = (39, 27)
before probe, target = (39,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.7377674876258, 'spark.broadcast.checksum': 0.1968983932262539, 'spark.broadcast.compress': 0.5253123366569243, 'spark.default.parallelism': 393.1512168669965, 'spark.executor.cores': 2.5397611961778903, 'spark.executor.instances': 7.418374498659391, 'spark.executor.memory': 6.126167398570815, 'spark.executor.memoryOverhead': 727.5019288471713, 'spark.kryoserializer.buffer': 38.87774326369168, 'spark.kryoserializer.buffer.max': 115.2485116543817, 'spark.locality.wait': 6.011951674234075, 'spark.maxRemoteBlockSizeFetchToMem': 1977797762.2977762, 'spark.memory.fraction': 0.5536318685733006, 'spark.memory.offHeap.enabled': 0.3247907903959891, 'spark.memory.offHeap.size': 825.831621953052, 'spark.memory.storageFraction': 0.6089085918732109, 'spark.rdd.compress': 0.32219996124699646, 'spark.reducer.maxBlocksInFlightPerAddress': 1535525066.9966214, 'spark.reducer.maxReqsInFlight': 1316940801.635976, 'spark.reducer.maxSizeInFlight': 27.600880474496055, 'spark.scheduler.mode': 0.7663579479868948, 'spark.scheduler.revive.interval': 592.4156676603019, 'spark.shuffle.compress': 0.8899591501393121, 'spark.shuffle.file.buffer': 43.45929101034218, 'spark.shuffle.io.numConnectionsPerPeer': 2.1219368304250392, 'spark.shuffle.sort.bypassMergeThreshold': 284.1050386272745, 'spark.storage.memoryMapThreshold': 1.1275298310869846} target = -194.807
x_probe target = -194.807
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
================= config35 =================
2022年 01月 27日 星期四 11:24:24 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (185248) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (42795) - 没有那个进程
================= config36 =================
2022年 01月 27日 星期四 11:28:18 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (187715) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (44834) - 没有那个进程
================= config37 =================
2022年 01月 27日 星期四 11:31:36 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (189332) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (46516) - 没有那个进程
bounds = 
[[6.00491330e+01 6.07625595e+01]
 [1.83696757e-01 2.06267453e-01]
 [5.05477691e-01 5.28927215e-01]
 [3.90347257e+02 3.97219333e+02]
 [2.51627036e+00 2.58853286e+00]
 [7.38519193e+00 7.47687185e+00]
 [6.12527818e+00 6.21618251e+00]
 [7.27732242e+02 7.38919210e+02]
 [3.85551105e+01 4.06571933e+01]
 [1.13574079e+02 1.15729375e+02]
 [6.00316156e+00 6.14684948e+00]
 [1.96882249e+09 1.99337177e+09]
 [5.46125092e-01 5.55462748e-01]
 [3.21954303e-01 3.46186253e-01]
 [8.20592991e+02 8.32939490e+02]
 [6.09071727e-01 6.18549359e-01]
 [3.14184899e-01 3.36702233e-01]
 [1.53370539e+09 1.55946374e+09]
 [1.31244209e+09 1.33638083e+09]
 [2.71621640e+01 2.82458369e+01]
 [7.46188635e-01 7.68510906e-01]
 [5.82035539e+02 5.93761664e+02]
 [8.84772165e-01 9.07403224e-01]
 [4.29990884e+01 4.37066339e+01]
 [2.07842304e+00 2.16937366e+00]
 [2.80663801e+02 2.85324789e+02]
 [1.09165625e+00 1.15986891e+00]]
before probe, param.shape = (40, 27)
before probe, target = (40,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.456185270120216, 'spark.broadcast.checksum': 0.19331226309473767, 'spark.broadcast.compress': 0.5111118525212194, 'spark.default.parallelism': 394.35840709926964, 'spark.executor.cores': 2.5772147387804245, 'spark.executor.instances': 7.456527536535197, 'spark.executor.memory': 6.18592594444142, 'spark.executor.memoryOverhead': 731.8332328680382, 'spark.kryoserializer.buffer': 38.80242705542448, 'spark.kryoserializer.buffer.max': 115.60799317208537, 'spark.locality.wait': 6.085951814044743, 'spark.maxRemoteBlockSizeFetchToMem': 1985444205.1858337, 'spark.memory.fraction': 0.5546219218431637, 'spark.memory.offHeap.enabled': 0.3353077002190846, 'spark.memory.offHeap.size': 831.709339759778, 'spark.memory.storageFraction': 0.6184190868822939, 'spark.rdd.compress': 0.3208554470316871, 'spark.reducer.maxBlocksInFlightPerAddress': 1538464760.4923298, 'spark.reducer.maxReqsInFlight': 1320573581.3685243, 'spark.reducer.maxSizeInFlight': 27.83682436208424, 'spark.scheduler.mode': 0.7648149619828861, 'spark.scheduler.revive.interval': 583.4794195785402, 'spark.shuffle.compress': 0.8979113087277135, 'spark.shuffle.file.buffer': 43.649236480603484, 'spark.shuffle.io.numConnectionsPerPeer': 2.145274820495198, 'spark.shuffle.sort.bypassMergeThreshold': 283.8231942143412, 'spark.storage.memoryMapThreshold': 1.0984462761322278} target = -226.023
x_probe target = -226.023
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.00848043e+01 6.07268882e+01]
 [1.84825292e-01 2.05138919e-01]
 [5.06650168e-01 5.27754739e-01]
 [3.90690861e+02 3.96875729e+02]
 [2.51988349e+00 2.58491973e+00]
 [7.38977593e+00 7.47228785e+00]
 [6.12982340e+00 6.21163730e+00]
 [7.28291590e+02 7.38359862e+02]
 [3.86602147e+01 4.05520892e+01]
 [1.13681844e+02 1.15621610e+02]
 [6.01034595e+00 6.13966508e+00]
 [1.97004995e+09 1.99214430e+09]
 [5.46591974e-01 5.54995865e-01]
 [3.23165900e-01 3.44974655e-01]
 [8.21210316e+02 8.32322165e+02]
 [6.09545608e-01 6.18075477e-01]
 [3.15310766e-01 3.35576366e-01]
 [1.53499330e+09 1.55817582e+09]
 [1.31363902e+09 1.33518390e+09]
 [2.72163476e+01 2.81916532e+01]
 [7.47304749e-01 7.67394792e-01]
 [5.82621845e+02 5.93175358e+02]
 [8.85903718e-01 9.06271671e-01]
 [4.30344657e+01 4.36712566e+01]
 [2.08297057e+00 2.16482613e+00]
 [2.80896851e+02 2.85091740e+02]
 [1.09506688e+00 1.15645827e+00]]
before probe, param.shape = (41, 27)
before probe, target = (41,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.540459889495345, 'spark.broadcast.checksum': 0.19661172908190355, 'spark.broadcast.compress': 0.5236543586855875, 'spark.default.parallelism': 393.73214948246397, 'spark.executor.cores': 2.571664837461705, 'spark.executor.instances': 7.45541633790515, 'spark.executor.memory': 6.196083871213451, 'spark.executor.memoryOverhead': 732.919071572291, 'spark.kryoserializer.buffer': 39.76100935807629, 'spark.kryoserializer.buffer.max': 114.53144677233203, 'spark.locality.wait': 6.0660853256770055, 'spark.maxRemoteBlockSizeFetchToMem': 1972550847.7665656, 'spark.memory.fraction': 0.5482359144074028, 'spark.memory.offHeap.enabled': 0.33290003115546346, 'spark.memory.offHeap.size': 828.2770803602842, 'spark.memory.storageFraction': 0.6110157987753244, 'spark.rdd.compress': 0.31937459988896344, 'spark.reducer.maxBlocksInFlightPerAddress': 1537967293.3927293, 'spark.reducer.maxReqsInFlight': 1323308153.3781185, 'spark.reducer.maxSizeInFlight': 27.298922140539318, 'spark.scheduler.mode': 0.7501357526276252, 'spark.scheduler.revive.interval': 587.4172525279297, 'spark.shuffle.compress': 0.8951199794924591, 'spark.shuffle.file.buffer': 43.536511824332344, 'spark.shuffle.io.numConnectionsPerPeer': 2.1633992002948346, 'spark.shuffle.sort.bypassMergeThreshold': 282.72365772663994, 'spark.storage.memoryMapThreshold': 1.1153456786764968} target = -188.326
x_probe target = -188.326
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.01169085e+01 6.06947840e+01]
 [1.85840973e-01 2.04123237e-01]
 [5.07705396e-01 5.26699510e-01]
 [3.91000105e+02 3.96566486e+02]
 [2.52313530e+00 2.58166792e+00]
 [7.39390152e+00 7.46816226e+00]
 [6.13391410e+00 6.20754660e+00]
 [7.28795004e+02 7.37856448e+02]
 [3.87548084e+01 4.04574955e+01]
 [1.13778832e+02 1.15524622e+02]
 [6.01681191e+00 6.13319913e+00]
 [1.97115467e+09 1.99103959e+09]
 [5.47012169e-01 5.54575671e-01]
 [3.24256338e-01 3.43884217e-01]
 [8.21765908e+02 8.31766573e+02]
 [6.09972102e-01 6.17648984e-01]
 [3.16324046e-01 3.34563086e-01]
 [1.53615243e+09 1.55701670e+09]
 [1.31471627e+09 1.33410665e+09]
 [2.72651129e+01 2.81428880e+01]
 [7.48309251e-01 7.66390290e-01]
 [5.83149521e+02 5.92647682e+02]
 [8.86922116e-01 9.05253273e-01]
 [4.30663053e+01 4.36394171e+01]
 [2.08706335e+00 2.16073335e+00]
 [2.81106595e+02 2.84881995e+02]
 [1.09813645e+00 1.15338870e+00]]
before probe, param.shape = (42, 27)
before probe, target = (42,)
================= config38 =================
2022年 01月 27日 星期四 11:34:43 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (192114) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (48988) - 没有那个进程
================= config39 =================
2022年 01月 27日 星期四 11:38:38 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (194286) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (51232) - 没有那个进程
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.41830199138451, 'spark.broadcast.checksum': 0.19671011311639267, 'spark.broadcast.compress': 0.5127890522658555, 'spark.default.parallelism': 391.0959927409954, 'spark.executor.cores': 2.550739238375923, 'spark.executor.instances': 7.3953127129345315, 'spark.executor.memory': 6.142122731768923, 'spark.executor.memoryOverhead': 733.3238840327767, 'spark.kryoserializer.buffer': 40.12203020279821, 'spark.kryoserializer.buffer.max': 115.2902468895105, 'spark.locality.wait': 6.047523292880369, 'spark.maxRemoteBlockSizeFetchToMem': 1978078284.263371, 'spark.memory.fraction': 0.5482621516058879, 'spark.memory.offHeap.enabled': 0.34323510391201467, 'spark.memory.offHeap.size': 824.327484033826, 'spark.memory.storageFraction': 0.6157668934818158, 'spark.rdd.compress': 0.32636802634322226, 'spark.reducer.maxBlocksInFlightPerAddress': 1537307004.9290528, 'spark.reducer.maxReqsInFlight': 1331684474.8281758, 'spark.reducer.maxSizeInFlight': 27.774858795782507, 'spark.scheduler.mode': 0.7580272225198543, 'spark.scheduler.revive.interval': 589.3815661962897, 'spark.shuffle.compress': 0.903484020459158, 'spark.shuffle.file.buffer': 43.06944434271434, 'spark.shuffle.io.numConnectionsPerPeer': 2.1038329236464244, 'spark.shuffle.sort.bypassMergeThreshold': 281.66664414642895, 'spark.storage.memoryMapThreshold': 1.1404553623449893} target = -178.648
x_probe target = -178.648
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.01588808e+01 6.06777232e+01]
 [1.88569495e-01 2.04850732e-01]
 [5.04462371e-01 5.21115734e-01]
 [3.88725486e+02 3.93466499e+02]
 [2.52448268e+00 2.57699580e+00]
 [7.36368134e+00 7.42694408e+00]
 [6.11041848e+00 6.17382698e+00]
 [7.29246326e+02 7.37401442e+02]
 [3.93816149e+01 4.08624455e+01]
 [1.14536568e+02 1.16043926e+02]
 [5.99652316e+00 6.09852343e+00]
 [1.96928101e+09 1.98687555e+09]
 [5.44985164e-01 5.51539139e-01]
 [3.34860800e-01 3.51609408e-01]
 [8.19949123e+02 8.28705845e+02]
 [6.12410114e-01 6.19123673e-01]
 [3.18206681e-01 3.34529371e-01]
 [1.52838196e+09 1.54623205e+09]
 [1.32332245e+09 1.34004650e+09]
 [2.73834029e+01 2.81663146e+01]
 [7.49924627e-01 7.66129818e-01]
 [5.85181542e+02 5.93581591e+02]
 [8.95604816e-01 9.11363225e-01]
 [4.28257149e+01 4.33131738e+01]
 [2.07168469e+00 2.13598115e+00]
 [2.80034097e+02 2.83299192e+02]
 [1.11632649e+00 1.16458424e+00]]
before probe, param.shape = (43, 27)
before probe, target = (43,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.66482729730264, 'spark.broadcast.checksum': 0.19014699986869876, 'spark.broadcast.compress': 0.5045490790243867, 'spark.default.parallelism': 393.0432063505622, 'spark.executor.cores': 2.5732557812507952, 'spark.executor.instances': 7.397520826425465, 'spark.executor.memory': 6.143308755568949, 'spark.executor.memoryOverhead': 734.3159387687119, 'spark.kryoserializer.buffer': 39.646054388323925, 'spark.kryoserializer.buffer.max': 115.63192534070497, 'spark.locality.wait': 6.00956842851678, 'spark.maxRemoteBlockSizeFetchToMem': 1974891395.4106617, 'spark.memory.fraction': 0.5496401300781312, 'spark.memory.offHeap.enabled': 0.3387984467071963, 'spark.memory.offHeap.size': 826.822721672344, 'spark.memory.storageFraction': 0.6134480345259332, 'spark.rdd.compress': 0.32716171663571497, 'spark.reducer.maxBlocksInFlightPerAddress': 1530089787.3190312, 'spark.reducer.maxReqsInFlight': 1334138209.7022414, 'spark.reducer.maxSizeInFlight': 28.11601361803378, 'spark.scheduler.mode': 0.763575243470768, 'spark.scheduler.revive.interval': 585.6144496391838, 'spark.shuffle.compress': 0.9031391256551009, 'spark.shuffle.file.buffer': 42.83885956141986, 'spark.shuffle.io.numConnectionsPerPeer': 2.13136199450144, 'spark.shuffle.sort.bypassMergeThreshold': 283.2357561681819, 'spark.storage.memoryMapThreshold': 1.136777924589868} target = -226.393
x_probe target = -226.393
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.01848229e+01 6.06517811e+01]
 [1.89383556e-01 2.04036670e-01]
 [5.05295039e-01 5.20283065e-01]
 [3.88962537e+02 3.93229448e+02]
 [2.52710833e+00 2.57437014e+00]
 [7.36684448e+00 7.42378095e+00]
 [6.11358891e+00 6.17065655e+00]
 [7.29654082e+02 7.36993686e+02]
 [3.94556565e+01 4.07884039e+01]
 [1.14611936e+02 1.15968558e+02]
 [6.00162317e+00 6.09342342e+00]
 [1.97016074e+09 1.98599583e+09]
 [5.45312863e-01 5.51211440e-01]
 [3.35698230e-01 3.50771978e-01]
 [8.20386959e+02 8.28268009e+02]
 [6.12745792e-01 6.18787995e-01]
 [3.19022816e-01 3.33713237e-01]
 [1.52927447e+09 1.54533954e+09]
 [1.32415865e+09 1.33921030e+09]
 [2.74225485e+01 2.81271691e+01]
 [7.50734887e-01 7.65319558e-01]
 [5.85601544e+02 5.93161588e+02]
 [8.96392736e-01 9.10575305e-01]
 [4.28500878e+01 4.32888009e+01]
 [2.07489952e+00 2.13276633e+00]
 [2.80197351e+02 2.83135937e+02]
 [1.11873937e+00 1.16217135e+00]]
before probe, param.shape = (44, 27)
before probe, target = (44,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.58747696048058, 'spark.broadcast.checksum': 0.2035179938017566, 'spark.broadcast.compress': 0.5174128445085492, 'spark.default.parallelism': 391.17516173566594, 'spark.executor.cores': 2.558567999561304, 'spark.executor.instances': 7.371605759569672, 'spark.executor.memory': 6.159540948303618, 'spark.executor.memoryOverhead': 736.5295819832694, 'spark.kryoserializer.buffer': 39.81166320795034, 'spark.kryoserializer.buffer.max': 115.15211280407264, 'spark.locality.wait': 6.032402996834271, 'spark.maxRemoteBlockSizeFetchToMem': 1980704937.7405417, 'spark.memory.fraction': 0.5501472579175575, 'spark.memory.offHeap.enabled': 0.34822447694844166, 'spark.memory.offHeap.size': 827.7466893402767, 'spark.memory.storageFraction': 0.6163878861049575, 'spark.rdd.compress': 0.3309509008299314, 'spark.reducer.maxBlocksInFlightPerAddress': 1530521944.965597, 'spark.reducer.maxReqsInFlight': 1326069284.5958834, 'spark.reducer.maxSizeInFlight': 27.79746882001238, 'spark.scheduler.mode': 0.755364498571419, 'spark.scheduler.revive.interval': 589.9159413533038, 'spark.shuffle.compress': 0.9042026237768844, 'spark.shuffle.file.buffer': 43.183317744194234, 'spark.shuffle.io.numConnectionsPerPeer': 2.0871546116028603, 'spark.shuffle.sort.bypassMergeThreshold': 280.89749762332184, 'spark.storage.memoryMapThreshold': 1.128275873902494} target = -242.768
x_probe target = -242.768
================= config40 =================
2022年 01月 27日 星期四 11:42:50 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (197016) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (53547) - 没有那个进程
================= config41 =================
2022年 01月 27日 星期四 11:46:41 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (199149) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (55760) - 没有那个进程
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.02081708e+01 6.06284332e+01]
 [1.90116212e-01 2.03304014e-01]
 [5.06044440e-01 5.19533664e-01]
 [3.89175883e+02 3.93016103e+02]
 [2.52947142e+00 2.57200705e+00]
 [7.36969130e+00 7.42093412e+00]
 [6.11644229e+00 6.16780317e+00]
 [7.30021062e+02 7.36626706e+02]
 [3.95222938e+01 4.07217666e+01]
 [1.14679767e+02 1.15900727e+02]
 [6.00621318e+00 6.08883340e+00]
 [1.97095250e+09 1.98520407e+09]
 [5.45607792e-01 5.50916511e-01]
 [3.36451917e-01 3.50018290e-01]
 [8.20781011e+02 8.27873957e+02]
 [6.13047902e-01 6.18485885e-01]
 [3.19757337e-01 3.32978716e-01]
 [1.53007772e+09 1.54453629e+09]
 [1.32491124e+09 1.33845771e+09]
 [2.74577796e+01 2.80919380e+01]
 [7.51464120e-01 7.64590325e-01]
 [5.85979546e+02 5.92783586e+02]
 [8.97101865e-01 9.09866176e-01]
 [4.28720235e+01 4.32668652e+01]
 [2.07779286e+00 2.12987299e+00]
 [2.80344281e+02 2.82989008e+02]
 [1.12091097e+00 1.15999975e+00]]
before probe, param.shape = (45, 27)
before probe, target = (45,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.2084990102201, 'spark.broadcast.checksum': 0.19635631705836426, 'spark.broadcast.compress': 0.5160105306456035, 'spark.default.parallelism': 391.38104396199117, 'spark.executor.cores': 2.5374172001298483, 'spark.executor.instances': 7.376072190135586, 'spark.executor.memory': 6.134364313396591, 'spark.executor.memoryOverhead': 734.2103364097618, 'spark.kryoserializer.buffer': 40.390106854639555, 'spark.kryoserializer.buffer.max': 115.57771074095304, 'spark.locality.wait': 6.043809698815957, 'spark.maxRemoteBlockSizeFetchToMem': 1981501377.0001976, 'spark.memory.fraction': 0.550824153608815, 'spark.memory.offHeap.enabled': 0.34557862168851755, 'spark.memory.offHeap.size': 824.5685246417271, 'spark.memory.storageFraction': 0.6156315615521265, 'spark.rdd.compress': 0.3317579258558261, 'spark.reducer.maxBlocksInFlightPerAddress': 1537148918.942981, 'spark.reducer.maxReqsInFlight': 1328627833.0290618, 'spark.reducer.maxSizeInFlight': 27.615116463438575, 'spark.scheduler.mode': 0.752097494181838, 'spark.scheduler.revive.interval': 586.0019108660755, 'spark.shuffle.compress': 0.9011454773292773, 'spark.shuffle.file.buffer': 43.09669723353059, 'spark.shuffle.io.numConnectionsPerPeer': 2.1077740862998047, 'spark.shuffle.sort.bypassMergeThreshold': 280.9573917077643, 'spark.storage.memoryMapThreshold': 1.1294996811192648} target = -222.069
x_probe target = -222.069
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.02291839e+01 6.06074200e+01]
 [1.90775602e-01 2.02644624e-01]
 [5.06718902e-01 5.18859203e-01]
 [3.89367894e+02 3.92824092e+02]
 [2.53159821e+00 2.56988027e+00]
 [7.37225344e+00 7.41837198e+00]
 [6.11901034e+00 6.16523513e+00]
 [7.30351344e+02 7.36296424e+02]
 [3.95822675e+01 4.06617929e+01]
 [1.14740815e+02 1.15839679e+02]
 [6.01034419e+00 6.08470239e+00]
 [1.97166507e+09 1.98449149e+09]
 [5.45873228e-01 5.50651075e-01]
 [3.37130236e-01 3.49339972e-01]
 [8.21135659e+02 8.27519309e+02]
 [6.13319801e-01 6.18213986e-01]
 [3.20418406e-01 3.32317647e-01]
 [1.53080065e+09 1.54381336e+09]
 [1.32558856e+09 1.33778039e+09]
 [2.74894875e+01 2.80602301e+01]
 [7.52120431e-01 7.63934014e-01]
 [5.86319748e+02 5.92443384e+02]
 [8.97740080e-01 9.09227961e-01]
 [4.28917656e+01 4.32471231e+01]
 [2.08039686e+00 2.12726898e+00]
 [2.80476517e+02 2.82856771e+02]
 [1.12286541e+00 1.15804531e+00]]
before probe, param.shape = (46, 27)
before probe, target = (46,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.52804778566076, 'spark.broadcast.checksum': 0.19079853868655045, 'spark.broadcast.compress': 0.5088289304260672, 'spark.default.parallelism': 389.7284824619599, 'spark.executor.cores': 2.5371265321235983, 'spark.executor.instances': 7.376080876226385, 'spark.executor.memory': 6.129068947819786, 'spark.executor.memoryOverhead': 735.612726610384, 'spark.kryoserializer.buffer': 39.84186102880156, 'spark.kryoserializer.buffer.max': 115.35028514164658, 'spark.locality.wait': 6.043455481923783, 'spark.maxRemoteBlockSizeFetchToMem': 1982458434.0916414, 'spark.memory.fraction': 0.5494637221074229, 'spark.memory.offHeap.enabled': 0.3431133129872797, 'spark.memory.offHeap.size': 823.028259627244, 'spark.memory.storageFraction': 0.6142838783818793, 'spark.rdd.compress': 0.330406305659812, 'spark.reducer.maxBlocksInFlightPerAddress': 1536881294.1446173, 'spark.reducer.maxReqsInFlight': 1326233282.7624, 'spark.reducer.maxSizeInFlight': 27.57253325123789, 'spark.scheduler.mode': 0.7617528429815148, 'spark.scheduler.revive.interval': 589.3504844720769, 'spark.shuffle.compress': 0.9035133997346347, 'spark.shuffle.file.buffer': 43.10348561461011, 'spark.shuffle.io.numConnectionsPerPeer': 2.1163237083433426, 'spark.shuffle.sort.bypassMergeThreshold': 280.50295703061227, 'spark.storage.memoryMapThreshold': 1.124988434824818} target = -235.345
x_probe target = -235.345
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
================= config42 =================
2022年 01月 27日 星期四 11:50:45 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (201857) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (58028) - 没有那个进程
================= config43 =================
2022年 01月 27日 星期四 11:54:30 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (203902) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (60153) - 没有那个进程
================= config44 =================
2022年 01月 27日 星期四 11:58:25 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (206274) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (62102) - 没有那个进程
bounds = 
[[6.02480958e+01 6.05885082e+01]
 [1.91369053e-01 2.02051173e-01]
 [5.07325917e-01 5.18252188e-01]
 [3.89540704e+02 3.92651282e+02]
 [2.53351231e+00 2.56796617e+00]
 [7.37455937e+00 7.41606606e+00]
 [6.12132158e+00 6.16292389e+00]
 [7.30648598e+02 7.35999170e+02]
 [3.96362437e+01 4.06078167e+01]
 [1.14795758e+02 1.15784736e+02]
 [6.01406210e+00 6.08098448e+00]
 [1.97230640e+09 1.98385017e+09]
 [5.46112120e-01 5.50412183e-01]
 [3.37740723e-01 3.48729485e-01]
 [8.21454841e+02 8.27200127e+02]
 [6.13564510e-01 6.17969277e-01]
 [3.21013368e-01 3.31722685e-01]
 [1.53145128e+09 1.54316273e+09]
 [1.32619815e+09 1.33717080e+09]
 [2.75180246e+01 2.80316930e+01]
 [7.52711110e-01 7.63343335e-01]
 [5.86625930e+02 5.92137202e+02]
 [8.98314474e-01 9.08653567e-01]
 [4.29095334e+01 4.32293553e+01]
 [2.08274047e+00 2.12492538e+00]
 [2.80595530e+02 2.82737758e+02]
 [1.12462441e+00 1.15628632e+00]]
before probe, param.shape = (47, 27)
before probe, target = (47,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.30875963136979, 'spark.broadcast.checksum': 0.19553994783997045, 'spark.broadcast.compress': 0.5124284978861826, 'spark.default.parallelism': 390.23168787570296, 'spark.executor.cores': 2.544496616828919, 'spark.executor.instances': 7.388154686074761, 'spark.executor.memory': 6.135212072921705, 'spark.executor.memoryOverhead': 731.0671614908883, 'spark.kryoserializer.buffer': 40.46902009987553, 'spark.kryoserializer.buffer.max': 115.14775298973186, 'spark.locality.wait': 6.054489782244516, 'spark.maxRemoteBlockSizeFetchToMem': 1977857315.640157, 'spark.memory.fraction': 0.550110815480138, 'spark.memory.offHeap.enabled': 0.3388027591279225, 'spark.memory.offHeap.size': 821.8781401167739, 'spark.memory.storageFraction': 0.6151778535335618, 'spark.rdd.compress': 0.3284926692901479, 'spark.reducer.maxBlocksInFlightPerAddress': 1533449727.3140154, 'spark.reducer.maxReqsInFlight': 1334140637.5502543, 'spark.reducer.maxSizeInFlight': 27.60762970756559, 'spark.scheduler.mode': 0.7628834527820674, 'spark.scheduler.revive.interval': 586.9786334114315, 'spark.shuffle.compress': 0.9035391090442978, 'spark.shuffle.file.buffer': 43.03355630406598, 'spark.shuffle.io.numConnectionsPerPeer': 2.1054191387697863, 'spark.shuffle.sort.bypassMergeThreshold': 282.6459403598023, 'spark.storage.memoryMapThreshold': 1.1416291835348464} target = -216.145
x_probe target = -216.145
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.02651164e+01 6.05714876e+01]
 [1.91903159e-01 2.01517067e-01]
 [5.07872230e-01 5.17705874e-01]
 [3.89696232e+02 3.92495753e+02]
 [2.53523500e+00 2.56624347e+00]
 [7.37663470e+00 7.41399072e+00]
 [6.12340169e+00 6.16084377e+00]
 [7.30916127e+02 7.35731641e+02]
 [3.96848224e+01 4.05592380e+01]
 [1.14845207e+02 1.15735287e+02]
 [6.01740822e+00 6.07763836e+00]
 [1.97288358e+09 1.98327298e+09]
 [5.46327123e-01 5.50197180e-01]
 [3.38290161e-01 3.48180047e-01]
 [8.21742106e+02 8.26912863e+02]
 [6.13784749e-01 6.17749038e-01]
 [3.21548834e-01 3.31187219e-01]
 [1.53203686e+09 1.54257715e+09]
 [1.32674678e+09 1.33662217e+09]
 [2.75437080e+01 2.80060096e+01]
 [7.53242721e-01 7.62811724e-01]
 [5.86901494e+02 5.91861639e+02]
 [8.98831429e-01 9.08136612e-01]
 [4.29255245e+01 4.32133642e+01]
 [2.08484972e+00 2.12281613e+00]
 [2.80702641e+02 2.82630647e+02]
 [1.12620750e+00 1.15470322e+00]]
before probe, param.shape = (48, 27)
before probe, target = (48,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.438371337923066, 'spark.broadcast.checksum': 0.20064121176203267, 'spark.broadcast.compress': 0.5165895169153022, 'spark.default.parallelism': 391.043191478762, 'spark.executor.cores': 2.541896319419684, 'spark.executor.instances': 7.409456566170292, 'spark.executor.memory': 6.145106900056434, 'spark.executor.memoryOverhead': 734.0282918740559, 'spark.kryoserializer.buffer': 40.21420766139941, 'spark.kryoserializer.buffer.max': 115.4579806530582, 'spark.locality.wait': 6.072076830742838, 'spark.maxRemoteBlockSizeFetchToMem': 1975683281.51895, 'spark.memory.fraction': 0.5483597806177551, 'spark.memory.offHeap.enabled': 0.3383054228536347, 'spark.memory.offHeap.size': 823.0566673028558, 'spark.memory.storageFraction': 0.6160353801702161, 'spark.rdd.compress': 0.32377862427958215, 'spark.reducer.maxBlocksInFlightPerAddress': 1540641864.708912, 'spark.reducer.maxReqsInFlight': 1335390758.9318545, 'spark.reducer.maxSizeInFlight': 27.853797520040025, 'spark.scheduler.mode': 0.7568023546966582, 'spark.scheduler.revive.interval': 591.2802382732004, 'spark.shuffle.compress': 0.9078620542407447, 'spark.shuffle.file.buffer': 43.126856390312085, 'spark.shuffle.io.numConnectionsPerPeer': 2.1029067813309705, 'spark.shuffle.sort.bypassMergeThreshold': 282.2291624061251, 'spark.storage.memoryMapThreshold': 1.141535638484396} target = -226.236
x_probe target = -226.236
key = 
['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
bounds = 
[[6.02804349e+01 6.05561690e+01]
 [1.92383855e-01 2.01036372e-01]
 [5.08363912e-01 5.17214192e-01]
 [3.89836208e+02 3.92355777e+02]
 [2.53678543e+00 2.56469305e+00]
 [7.37850251e+00 7.41212292e+00]
 [6.12527380e+00 6.15897167e+00]
 [7.31156903e+02 7.35490866e+02]
 [3.97285432e+01 4.05155172e+01]
 [1.14889711e+02 1.15690783e+02]
 [6.02041973e+00 6.07462686e+00]
 [1.97340305e+09 1.98275351e+09]
 [5.46520626e-01 5.50003677e-01]
 [3.38784655e-01 3.47685553e-01]
 [8.22000643e+02 8.26654325e+02]
 [6.13982963e-01 6.17550824e-01]
 [3.22030753e-01 3.30705300e-01]
 [1.53256387e+09 1.54205014e+09]
 [1.32724055e+09 1.33612840e+09]
 [2.75668231e+01 2.79828945e+01]
 [7.53721171e-01 7.62333274e-01]
 [5.87149501e+02 5.91613631e+02]
 [8.99296688e-01 9.07671353e-01]
 [4.29399165e+01 4.31989722e+01]
 [2.08674804e+00 2.12091781e+00]
 [2.80799042e+02 2.82534247e+02]
 [1.12763229e+00 1.15327844e+00]]
before probe, param.shape = (49, 27)
before probe, target = (49,)
probe说：没见过！帮你计算target！params = {'spark.broadcast.blockSize': 60.52408001789608, 'spark.broadcast.checksum': 0.19815730266352247, 'spark.broadcast.compress': 0.5160196277923565, 'spark.default.parallelism': 390.27861975643737, 'spark.executor.cores': 2.541319592079329, 'spark.executor.instances': 7.389211326136831, 'spark.executor.memory': 6.155949440650322, 'spark.executor.memoryOverhead': 735.2843398129976, 'spark.kryoserializer.buffer': 39.730276771719424, 'spark.kryoserializer.buffer.max': 115.02237476002199, 'spark.locality.wait': 6.052443581518355, 'spark.maxRemoteBlockSizeFetchToMem': 1973915976.521566, 'spark.memory.fraction': 0.5466501723217488, 'spark.memory.offHeap.enabled': 0.34147639695884174, 'spark.memory.offHeap.size': 826.1763995356723, 'spark.memory.storageFraction': 0.6160112067816602, 'spark.rdd.compress': 0.3228830895553732, 'spark.reducer.maxBlocksInFlightPerAddress': 1539317250.7634785, 'spark.reducer.maxReqsInFlight': 1334658405.1650822, 'spark.reducer.maxSizeInFlight': 27.967005696821523, 'spark.scheduler.mode': 0.7606306730279934, 'spark.scheduler.revive.interval': 591.0611733169654, 'spark.shuffle.compress': 0.903943945871302, 'spark.shuffle.file.buffer': 43.050963273372616, 'spark.shuffle.io.numConnectionsPerPeer': 2.113259096463017, 'spark.shuffle.sort.bypassMergeThreshold': 282.1257978385593, 'spark.storage.memoryMapThreshold': 1.1381727251760339} target = -177.999
x_probe target = -177.999
{'maxIndex': 50, 'target': -177.999, 'params': {'spark.broadcast.blockSize': 60.52408001789608, 'spark.broadcast.checksum': 0.19815730266352247, 'spark.broadcast.compress': 0.5160196277923565, 'spark.default.parallelism': 390.27861975643737, 'spark.executor.cores': 2.541319592079329, 'spark.executor.instances': 7.389211326136831, 'spark.executor.memory': 6.155949440650322, 'spark.executor.memoryOverhead': 735.2843398129976, 'spark.kryoserializer.buffer': 39.730276771719424, 'spark.kryoserializer.buffer.max': 115.02237476002199, 'spark.locality.wait': 6.052443581518355, 'spark.maxRemoteBlockSizeFetchToMem': 1973915976.521566, 'spark.memory.fraction': 0.5466501723217488, 'spark.memory.offHeap.enabled': 0.34147639695884174, 'spark.memory.offHeap.size': 826.1763995356723, 'spark.memory.storageFraction': 0.6160112067816602, 'spark.rdd.compress': 0.3228830895553732, 'spark.reducer.maxBlocksInFlightPerAddress': 1539317250.7634785, 'spark.reducer.maxReqsInFlight': 1334658405.1650822, 'spark.reducer.maxSizeInFlight': 27.967005696821523, 'spark.scheduler.mode': 0.7606306730279934, 'spark.scheduler.revive.interval': 591.0611733169654, 'spark.shuffle.compress': 0.903943945871302, 'spark.shuffle.file.buffer': 43.050963273372616, 'spark.shuffle.io.numConnectionsPerPeer': 2.113259096463017, 'spark.shuffle.sort.bypassMergeThreshold': 282.1257978385593, 'spark.storage.memoryMapThreshold': 1.1381727251760339}}
=============== finish wordcount-100G ===============
2022年 01月 27日 星期四 12:01:32 CST
./run-new.sh:行62: d/usr/local/home/yyq/bo/ganrs_bo/direct_ganrs_Bayesian.log: 没有那个文件或目录
