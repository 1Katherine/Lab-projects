nohup: 忽略输入
/usr/local/home/yyq/bo/ganrs_bo_new
=============== start wordcount-100G ===============
2022年 02月 22日 星期二 10:22:46 CST
=============== start wordcount-100G ===============
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/config/wordcount-100G" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/logs.json" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/generationConf.csv" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/target.png" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/dataset.csv" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/GAN*" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/general_data.csv" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/sgan_sample.csv" 的文件状态(stat): 没有那个文件或目录
sys.path = ['/usr/local/home/yyq/bo/ganrs_bo_new', '/usr/local/home/python3/python3/lib/python38.zip', '/usr/local/home/python3/python3/lib/python3.8', '/usr/local/home/python3/python3/lib/python3.8/lib-dynload', '/usr/local/home/python3/python3/lib/python3.8/site-packages', '/usr/local/home/yyq/bo/ganrs_bo_new', '/usr/local/home/yyq/bo/ganrs_bo_new/bayes_scode']
benchmark = wordcount-100G	 gan+rs生成的样本数：initpoints = 6	 bo迭代搜索次数：--niters = 6
重要参数列表（将贝叶斯的x_probe按照重要参数列表顺序转成配置文件实际运行:
                                 vital_params
0                   spark.broadcast.blockSize
1                    spark.broadcast.checksum
2                    spark.broadcast.compress
3                   spark.default.parallelism
4                        spark.executor.cores
5                    spark.executor.instances
6                       spark.executor.memory
7               spark.executor.memoryOverhead
8                 spark.kryoserializer.buffer
9             spark.kryoserializer.buffer.max
10                        spark.locality.wait
11         spark.maxRemoteBlockSizeFetchToMem
12                      spark.memory.fraction
13               spark.memory.offHeap.enabled
14                  spark.memory.offHeap.size
15               spark.memory.storageFraction
16                         spark.rdd.compress
17  spark.reducer.maxBlocksInFlightPerAddress
18              spark.reducer.maxReqsInFlight
19              spark.reducer.maxSizeInFlight
20                       spark.scheduler.mode
21            spark.scheduler.revive.interval
22                     spark.shuffle.compress
23                  spark.shuffle.file.buffer
24     spark.shuffle.io.numConnectionsPerPeer
25    spark.shuffle.sort.bypassMergeThreshold
26           spark.storage.memoryMapThreshold
按照key值排序前的d2 = {'spark.broadcast.blockSize': (32.0, 64.0), 'spark.broadcast.checksum': (0.0, 1.0), 'spark.broadcast.compress': (0.0, 1.0), 'spark.default.parallelism': (200.0, 500.0), 'spark.executor.cores': (1.0, 4.0), 'spark.executor.instances': (4.0, 8.0), 'spark.executor.memory': (3.0, 7.0), 'spark.executor.memoryOverhead': (384.0, 877.0), 'spark.kryoserializer.buffer': (32.0, 128.0), 'spark.kryoserializer.buffer.max': (32.0, 128.0), 'spark.locality.wait': (4.0, 10.0), 'spark.maxRemoteBlockSizeFetchToMem': (1073741567.0, 2147483135.0), 'spark.memory.fraction': (0.5, 0.9), 'spark.memory.offHeap.enabled': (0.0, 1.0), 'spark.memory.offHeap.size': (0.0, 1024.0), 'spark.memory.storageFraction': (0.5, 0.9), 'spark.rdd.compress': (0.0, 1.0), 'spark.reducer.maxBlocksInFlightPerAddress': (1073741823.0, 2147483647.0), 'spark.reducer.maxReqsInFlight': (1073741823.0, 2147483647.0), 'spark.reducer.maxSizeInFlight': (24.0, 72.0), 'spark.scheduler.mode': (0.0, 1.0), 'spark.scheduler.revive.interval': (500.0, 1000.0), 'spark.shuffle.compress': (0.0, 1.0), 'spark.shuffle.file.buffer': (16.0, 48.0), 'spark.shuffle.io.numConnectionsPerPeer': (1.0, 5.0), 'spark.shuffle.sort.bypassMergeThreshold': (150.0, 350.0), 'spark.storage.memoryMapThreshold': (1.0, 4.0)}
按照key值排序后的d2 = {'spark.broadcast.blockSize': (32.0, 64.0), 'spark.broadcast.checksum': (0.0, 1.0), 'spark.broadcast.compress': (0.0, 1.0), 'spark.default.parallelism': (200.0, 500.0), 'spark.executor.cores': (1.0, 4.0), 'spark.executor.instances': (4.0, 8.0), 'spark.executor.memory': (3.0, 7.0), 'spark.executor.memoryOverhead': (384.0, 877.0), 'spark.kryoserializer.buffer': (32.0, 128.0), 'spark.kryoserializer.buffer.max': (32.0, 128.0), 'spark.locality.wait': (4.0, 10.0), 'spark.maxRemoteBlockSizeFetchToMem': (1073741567.0, 2147483135.0), 'spark.memory.fraction': (0.5, 0.9), 'spark.memory.offHeap.enabled': (0.0, 1.0), 'spark.memory.offHeap.size': (0.0, 1024.0), 'spark.memory.storageFraction': (0.5, 0.9), 'spark.rdd.compress': (0.0, 1.0), 'spark.reducer.maxBlocksInFlightPerAddress': (1073741823.0, 2147483647.0), 'spark.reducer.maxReqsInFlight': (1073741823.0, 2147483647.0), 'spark.reducer.maxSizeInFlight': (24.0, 72.0), 'spark.scheduler.mode': (0.0, 1.0), 'spark.scheduler.revive.interval': (500.0, 1000.0), 'spark.shuffle.compress': (0.0, 1.0), 'spark.shuffle.file.buffer': (16.0, 48.0), 'spark.shuffle.io.numConnectionsPerPeer': (1.0, 5.0), 'spark.shuffle.sort.bypassMergeThreshold': (150.0, 350.0), 'spark.storage.memoryMapThreshold': (1.0, 4.0)}
vital_params_name = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold']
vital_params_list = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.default.parallelism', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.executor.memoryOverhead', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.storage.memoryMapThreshold', 'runtime']
gan_random方法获取的参数:	 n = 6	 type = random	 bo_res_df = Empty DataFrame
Columns: []
Index: []	 iterations = 0
随机生成的配置:[56, 1, 1, 309, 1, 5, 5, 744, 65, 120, 5, 1485524652, 0.51, 0, 200, 0.68, 0, 1177067994, 1187027607, 39, 0, 945, 0, 39, 4, 257, 3]
需要通过formatConf处理的数据 : conf = spark.broadcast.blockSize	 value = 56
需要通过formatConf处理的数据 : conf = spark.broadcast.checksum	 value = 1
需要通过formatConf处理的数据 : conf = spark.broadcast.compress	 value = 1
需要通过formatConf处理的数据 : conf = spark.default.parallelism	 value = 309
需要通过formatConf处理的数据 : conf = spark.executor.cores	 value = 1
需要通过formatConf处理的数据 : conf = spark.executor.instances	 value = 5
需要通过formatConf处理的数据 : conf = spark.executor.memory	 value = 5
需要通过formatConf处理的数据 : conf = spark.executor.memoryOverhead	 value = 744
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer	 value = 65
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer.max	 value = 120
需要通过formatConf处理的数据 : conf = spark.locality.wait	 value = 5
需要通过formatConf处理的数据 : conf = spark.maxRemoteBlockSizeFetchToMem	 value = 1485524652
需要通过formatConf处理的数据 : conf = spark.memory.fraction	 value = 0.51
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.enabled	 value = 0
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.size	 value = 200
需要通过formatConf处理的数据 : conf = spark.memory.storageFraction	 value = 0.68
需要通过formatConf处理的数据 : conf = spark.rdd.compress	 value = 0
需要通过formatConf处理的数据 : conf = spark.reducer.maxBlocksInFlightPerAddress	 value = 1177067994
需要通过formatConf处理的数据 : conf = spark.reducer.maxReqsInFlight	 value = 1187027607
需要通过formatConf处理的数据 : conf = spark.reducer.maxSizeInFlight	 value = 39
================= config1 =================
2022年 02月 22日 星期二 10:22:47 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (97965) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (156641) - 没有那个进程
================= config2 =================
2022年 02月 22日 星期二 10:36:47 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (111486) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (182133) - 没有那个进程
================= config3 =================
2022年 02月 22日 星期二 10:40:38 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (121328) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (192590) - 没有那个进程
需要通过formatConf处理的数据 : conf = spark.scheduler.mode	 value = 0
需要通过formatConf处理的数据 : conf = spark.scheduler.revive.interval	 value = 945
需要通过formatConf处理的数据 : conf = spark.shuffle.compress	 value = 0
需要通过formatConf处理的数据 : conf = spark.shuffle.file.buffer	 value = 39
需要通过formatConf处理的数据 : conf = spark.shuffle.io.numConnectionsPerPeer	 value = 4
需要通过formatConf处理的数据 : conf = spark.shuffle.sort.bypassMergeThreshold	 value = 257
需要通过formatConf处理的数据 : conf = spark.storage.memoryMapThreshold	 value = 3
configNum = 1	 run_cmd = /usr/local/home/zwr/wordcount-100G-ga.sh 1 /usr/local/home/yyq/bo/ganrs_bo_new/config/wordcount-100G
run_cmd命令执行成功
随机生成的配置和实际运行时间:[56, 1, 1, 309, 1, 5, 5, 744, 65, 120, 5, 1485524652, 0.51, 0, 200, 0.68, 0, 1177067994, 1187027607, 39, 0, 945, 0, 39, 4, 257, 3, 831.529]
   spark.broadcast.blockSize  ...  runtime
0                         56  ...  831.529

[1 rows x 28 columns]
  spark.broadcast.blockSize  ...  runtime
0                        56  ...  831.529

[1 rows x 28 columns]
随机生成的配置:[35, 0, 1, 282, 4, 8, 4, 437, 127, 111, 7, 1217787614, 0.84, 0, 877, 0.89, 1, 1490966255, 1820212257, 51, 0, 563, 1, 36, 1, 343, 1]
需要通过formatConf处理的数据 : conf = spark.broadcast.blockSize	 value = 35
需要通过formatConf处理的数据 : conf = spark.broadcast.checksum	 value = 0
需要通过formatConf处理的数据 : conf = spark.broadcast.compress	 value = 1
需要通过formatConf处理的数据 : conf = spark.default.parallelism	 value = 282
需要通过formatConf处理的数据 : conf = spark.executor.cores	 value = 4
需要通过formatConf处理的数据 : conf = spark.executor.instances	 value = 8
需要通过formatConf处理的数据 : conf = spark.executor.memory	 value = 4
需要通过formatConf处理的数据 : conf = spark.executor.memoryOverhead	 value = 437
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer	 value = 127
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer.max	 value = 111
需要通过formatConf处理的数据 : conf = spark.locality.wait	 value = 7
需要通过formatConf处理的数据 : conf = spark.maxRemoteBlockSizeFetchToMem	 value = 1217787614
需要通过formatConf处理的数据 : conf = spark.memory.fraction	 value = 0.84
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.enabled	 value = 0
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.size	 value = 877
需要通过formatConf处理的数据 : conf = spark.memory.storageFraction	 value = 0.89
需要通过formatConf处理的数据 : conf = spark.rdd.compress	 value = 1
需要通过formatConf处理的数据 : conf = spark.reducer.maxBlocksInFlightPerAddress	 value = 1490966255
需要通过formatConf处理的数据 : conf = spark.reducer.maxReqsInFlight	 value = 1820212257
需要通过formatConf处理的数据 : conf = spark.reducer.maxSizeInFlight	 value = 51
需要通过formatConf处理的数据 : conf = spark.scheduler.mode	 value = 0
需要通过formatConf处理的数据 : conf = spark.scheduler.revive.interval	 value = 563
需要通过formatConf处理的数据 : conf = spark.shuffle.compress	 value = 1
需要通过formatConf处理的数据 : conf = spark.shuffle.file.buffer	 value = 36
需要通过formatConf处理的数据 : conf = spark.shuffle.io.numConnectionsPerPeer	 value = 1
需要通过formatConf处理的数据 : conf = spark.shuffle.sort.bypassMergeThreshold	 value = 343
需要通过formatConf处理的数据 : conf = spark.storage.memoryMapThreshold	 value = 1
configNum = 2	 run_cmd = /usr/local/home/zwr/wordcount-100G-ga.sh 2 /usr/local/home/yyq/bo/ganrs_bo_new/config/wordcount-100G
run_cmd命令执行成功
随机生成的配置和实际运行时间:[35, 0, 1, 282, 4, 8, 4, 437, 127, 111, 7, 1217787614, 0.84, 0, 877, 0.89, 1, 1490966255, 1820212257, 51, 0, 563, 1, 36, 1, 343, 1, 221.911]
   spark.broadcast.blockSize  ...  runtime
0                         35  ...  221.911

[1 rows x 28 columns]
  spark.broadcast.blockSize  ...  runtime
0                        56  ...  831.529
1                        35  ...  221.911

[2 rows x 28 columns]
随机生成的配置:[40, 0, 0, 483, 1, 7, 5, 655, 40, 42, 7, 2002462452, 0.6, 1, 576, 0.53, 0, 1151325234, 1149548765, 64, 1, 571, 1, 41, 3, 240, 2]
需要通过formatConf处理的数据 : conf = spark.broadcast.blockSize	 value = 40
需要通过formatConf处理的数据 : conf = spark.broadcast.checksum	 value = 0
需要通过formatConf处理的数据 : conf = spark.broadcast.compress	 value = 0
需要通过formatConf处理的数据 : conf = spark.default.parallelism	 value = 483
需要通过formatConf处理的数据 : conf = spark.executor.cores	 value = 1
需要通过formatConf处理的数据 : conf = spark.executor.instances	 value = 7
需要通过formatConf处理的数据 : conf = spark.executor.memory	 value = 5
需要通过formatConf处理的数据 : conf = spark.executor.memoryOverhead	 value = 655
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer	 value = 40
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer.max	 value = 42
需要通过formatConf处理的数据 : conf = spark.locality.wait	 value = 7
需要通过formatConf处理的数据 : conf = spark.maxRemoteBlockSizeFetchToMem	 value = 2002462452
需要通过formatConf处理的数据 : conf = spark.memory.fraction	 value = 0.6
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.enabled	 value = 1
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.size	 value = 576
需要通过formatConf处理的数据 : conf = spark.memory.storageFraction	 value = 0.53
需要通过formatConf处理的数据 : conf = spark.rdd.compress	 value = 0
需要通过formatConf处理的数据 : conf = spark.reducer.maxBlocksInFlightPerAddress	 value = 1151325234
需要通过formatConf处理的数据 : conf = spark.reducer.maxReqsInFlight	 value = 1149548765
需要通过formatConf处理的数据 : conf = spark.reducer.maxSizeInFlight	 value = 64
需要通过formatConf处理的数据 : conf = spark.scheduler.mode	 value = 1
需要通过formatConf处理的数据 : conf = spark.scheduler.revive.interval	 value = 571
需要通过formatConf处理的数据 : conf = spark.shuffle.compress	 value = 1
需要通过formatConf处理的数据 : conf = spark.shuffle.file.buffer	 value = 41
需要通过formatConf处理的数据 : conf = spark.shuffle.io.numConnectionsPerPeer	 value = 3
需要通过formatConf处理的数据 : conf = spark.shuffle.sort.bypassMergeThreshold	 value = 240
需要通过formatConf处理的数据 : conf = spark.storage.memoryMapThreshold	 value = 2
configNum = 3	 run_cmd = /usr/local/home/zwr/wordcount-100G-ga.sh 3 /usr/local/home/yyq/bo/ganrs_bo_new/config/wordcount-100G
run_cmd命令执行成功
随机生成的配置和实际运行时间:[40, 0, 0, 483, 1, 7, 5, 655, 40, 42, 7, 2002462452, 0.6, 1, 576, 0.53, 0, 1151325234, 1149548765, 64, 1, 571, 1, 41, 3, 240, 2, 373.212]
   spark.broadcast.blockSize  ...  runtime
0                         40  ...  373.212

[1 rows x 28 columns]
  spark.broadcast.blockSize  ...  runtime
0                        56  ...  831.529
1                        35  ...  221.911
2                        40  ...  373.212

[3 rows x 28 columns]
  spark.broadcast.blockSize  ... spark.storage.memoryMapThreshold
0                        35  ...                                1

[1 rows x 27 columns]
  spark.broadcast.blockSize  ... spark.storage.memoryMapThreshold
0                   -0.8125  ...                             -1.0

[1 rows x 27 columns]
traindata.shape:(1, 27)
traindata.count_value:19.936675550194735
0
2000
Epoch: 0 Loss D.: 0.029305338859558105
Epoch: 0 Loss G.: 19.918335651757474
tensor(19.7674, dtype=torch.float64, grad_fn=<DivBackward0>)
1
2000
Epoch: 1 Loss D.: -0.009898364543914795
Epoch: 1 Loss G.: 19.84104200244164
tensor(19.7231, dtype=torch.float64, grad_fn=<DivBackward0>)
2
2000
Epoch: 2 Loss D.: -0.007126331329345703
Epoch: 2 Loss G.: 20.259630452620144
tensor(19.8058, dtype=torch.float64, grad_fn=<DivBackward0>)
3
2000
Epoch: 3 Loss D.: -0.000797569751739502
Epoch: 3 Loss G.: 19.051678987929876
tensor(19.2927, dtype=torch.float64, grad_fn=<DivBackward0>)
4
2000
Epoch: 4 Loss D.: 0.0004194974899291992
Epoch: 4 Loss G.: 20.38006628934508
tensor(18.9948, dtype=torch.float64, grad_fn=<DivBackward0>)
5
2000
Epoch: 5 Loss D.: -0.007261157035827637
Epoch: 5 Loss G.: 20.14839120745873
tensor(19.2754, dtype=torch.float64, grad_fn=<DivBackward0>)
6
2000
Epoch: 6 Loss D.: -0.0032320022583007812
Epoch: 6 Loss G.: 19.58349370468622
tensor(18.6830, dtype=torch.float64, grad_fn=<DivBackward0>)
7
2000
Epoch: 7 Loss D.: -0.003395557403564453
Epoch: 7 Loss G.: 18.770955645723546
tensor(18.6852, dtype=torch.float64, grad_fn=<DivBackward0>)
8
2000
Epoch: 8 Loss D.: -0.00033217668533325195
Epoch: 8 Loss G.: 18.716737298073525
tensor(18.8775, dtype=torch.float64, grad_fn=<DivBackward0>)
9
2000
Epoch: 9 Loss D.: 0.007373809814453125
Epoch: 9 Loss G.: 18.42035875261044
tensor(18.5680, dtype=torch.float64, grad_fn=<DivBackward0>)
10
2000
Epoch: 10 Loss D.: 0.0017429590225219727
Epoch: 10 Loss G.: 17.97726320732599
tensor(18.2769, dtype=torch.float64, grad_fn=<DivBackward0>)
11
2000
Epoch: 11 Loss D.: 0.002763986587524414
Epoch: 11 Loss G.: 20.088768947648163
tensor(18.0816, dtype=torch.float64, grad_fn=<DivBackward0>)
12
2000
Epoch: 12 Loss D.: -0.0008211731910705566
Epoch: 12 Loss G.: 18.096708980167563
tensor(18.1746, dtype=torch.float64, grad_fn=<DivBackward0>)
13
2000
Epoch: 13 Loss D.: -0.010960698127746582
Epoch: 13 Loss G.: 20.44923050120687
tensor(17.4577, dtype=torch.float64, grad_fn=<DivBackward0>)
14
2000
Epoch: 14 Loss D.: -0.00040435791015625
Epoch: 14 Loss G.: 18.0158591845653
tensor(17.7836, dtype=torch.float64, grad_fn=<DivBackward0>)
15
2000
Epoch: 15 Loss D.: -0.0009208321571350098
Epoch: 15 Loss G.: 18.757581306865717
tensor(17.2224, dtype=torch.float64, grad_fn=<DivBackward0>)
16
2000
Epoch: 16 Loss D.: 0.0014497041702270508
Epoch: 16 Loss G.: 16.87104140944993
tensor(17.2753, dtype=torch.float64, grad_fn=<DivBackward0>)
17
2000
Epoch: 17 Loss D.: -0.0024104714393615723
Epoch: 17 Loss G.: 15.75235095411753
tensor(17.0342, dtype=torch.float64, grad_fn=<DivBackward0>)
18
2000
Epoch: 18 Loss D.: 0.008095502853393555
Epoch: 18 Loss G.: 16.27802850470161
tensor(16.2670, dtype=torch.float64, grad_fn=<DivBackward0>)
19
2000
Epoch: 19 Loss D.: 0.011976003646850586
Epoch: 19 Loss G.: 14.972370952146404
tensor(15.8526, dtype=torch.float64, grad_fn=<DivBackward0>)
20
2000
Epoch: 20 Loss D.: -0.0011758804321289062
Epoch: 20 Loss G.: 15.117239013092869
tensor(16.3509, dtype=torch.float64, grad_fn=<DivBackward0>)
21
2000
Epoch: 21 Loss D.: 0.004396796226501465
Epoch: 21 Loss G.: 16.10941522360062
tensor(16.2737, dtype=torch.float64, grad_fn=<DivBackward0>)
22
2000
Epoch: 22 Loss D.: -0.0006780624389648438
Epoch: 22 Loss G.: 18.441373676771576
tensor(15.1818, dtype=torch.float64, grad_fn=<DivBackward0>)
23
2000
Epoch: 23 Loss D.: 0.003059685230255127
Epoch: 23 Loss G.: 17.719277434760983
tensor(15.2022, dtype=torch.float64, grad_fn=<DivBackward0>)
24
2000
Epoch: 24 Loss D.: 0.0038136839866638184
Epoch: 24 Loss G.: 15.755023997055643
tensor(15.4883, dtype=torch.float64, grad_fn=<DivBackward0>)
25
2000
Epoch: 25 Loss D.: -0.0029028654098510742
Epoch: 25 Loss G.: 13.948768993618243
tensor(15.4723, dtype=torch.float64, grad_fn=<DivBackward0>)
26
2000
Epoch: 26 Loss D.: -0.001484692096710205
Epoch: 26 Loss G.: 16.396781781466956
tensor(15.0125, dtype=torch.float64, grad_fn=<DivBackward0>)
27
2000
Epoch: 27 Loss D.: 0.009857416152954102
Epoch: 27 Loss G.: 18.96966328263497
tensor(15.0569, dtype=torch.float64, grad_fn=<DivBackward0>)
28
2000
Epoch: 28 Loss D.: 0.0005085468292236328
Epoch: 28 Loss G.: 13.875154921563619
tensor(15.5606, dtype=torch.float64, grad_fn=<DivBackward0>)
29
2000
Epoch: 29 Loss D.: 0.00894010066986084
Epoch: 29 Loss G.: 15.082806886883729
tensor(14.7602, dtype=torch.float64, grad_fn=<DivBackward0>)
30
2000
Epoch: 30 Loss D.: 0.004023909568786621
Epoch: 30 Loss G.: 13.333544955901457
tensor(13.7325, dtype=torch.float64, grad_fn=<DivBackward0>)
31
2000
Epoch: 31 Loss D.: 0.0023660659790039062
Epoch: 31 Loss G.: 14.482249908393525
tensor(13.6450, dtype=torch.float64, grad_fn=<DivBackward0>)
32
2000
Epoch: 32 Loss D.: 0.006467700004577637
Epoch: 32 Loss G.: 14.007419803889745
tensor(13.4969, dtype=torch.float64, grad_fn=<DivBackward0>)
33
2000
Epoch: 33 Loss D.: -0.003128528594970703
Epoch: 33 Loss G.: 14.107882087828868
tensor(12.4853, dtype=torch.float64, grad_fn=<DivBackward0>)
34
2000
Epoch: 34 Loss D.: -0.003075122833251953
Epoch: 34 Loss G.: 13.705397838505261
tensor(12.9789, dtype=torch.float64, grad_fn=<DivBackward0>)
35
2000
Epoch: 35 Loss D.: 0.008006691932678223
Epoch: 35 Loss G.: 9.311420598379923
tensor(12.7032, dtype=torch.float64, grad_fn=<DivBackward0>)
36
2000
Epoch: 36 Loss D.: 0.007365584373474121
Epoch: 36 Loss G.: 13.937875706481092
tensor(11.5963, dtype=torch.float64, grad_fn=<DivBackward0>)
37
2000
Epoch: 37 Loss D.: -0.009985387325286865
Epoch: 37 Loss G.: 13.732013167592042
tensor(10.9152, dtype=torch.float64, grad_fn=<DivBackward0>)
38
2000
Epoch: 38 Loss D.: -0.005562782287597656
Epoch: 38 Loss G.: 10.103139193109824
tensor(11.4278, dtype=torch.float64, grad_fn=<DivBackward0>)
39
2000
Epoch: 39 Loss D.: 0.004179775714874268
Epoch: 39 Loss G.: 9.190884658845418
tensor(10.3031, dtype=torch.float64, grad_fn=<DivBackward0>)
40
2000
Epoch: 40 Loss D.: 0.009876757860183716
Epoch: 40 Loss G.: 6.620324322335079
tensor(9.5988, dtype=torch.float64, grad_fn=<DivBackward0>)
41
2000
Epoch: 41 Loss D.: 0.009107887744903564
Epoch: 41 Loss G.: 9.13653399919684
tensor(9.6724, dtype=torch.float64, grad_fn=<DivBackward0>)
42
2000
Epoch: 42 Loss D.: -0.010157585144042969
Epoch: 42 Loss G.: 9.540872575493806
tensor(7.4980, dtype=torch.float64, grad_fn=<DivBackward0>)
43
2000
Epoch: 43 Loss D.: 0.0012205839157104492
Epoch: 43 Loss G.: 5.255234516910865
tensor(9.1306, dtype=torch.float64, grad_fn=<DivBackward0>)
44
2000
Epoch: 44 Loss D.: -0.0031069517135620117
Epoch: 44 Loss G.: 8.452851587287737
tensor(7.1404, dtype=torch.float64, grad_fn=<DivBackward0>)
45
2000
Epoch: 45 Loss D.: -0.003754138946533203
Epoch: 45 Loss G.: 7.222427369407965
tensor(6.5353, dtype=torch.float64, grad_fn=<DivBackward0>)
46
2000
Epoch: 46 Loss D.: 0.013252943754196167
Epoch: 46 Loss G.: 14.393568681331628
tensor(6.6144, dtype=torch.float64, grad_fn=<DivBackward0>)
47
2000
Epoch: 47 Loss D.: 0.0029060840606689453
Epoch: 47 Loss G.: 4.613318972043349
tensor(6.7265, dtype=torch.float64, grad_fn=<DivBackward0>)
48
2000
Epoch: 48 Loss D.: 0.005918562412261963
Epoch: 48 Loss G.: 4.9393524525087615
tensor(5.0396, dtype=torch.float64, grad_fn=<DivBackward0>)
49
2000
Epoch: 49 Loss D.: -0.008126676082611084
Epoch: 49 Loss G.: 8.05623257284577
tensor(5.5969, dtype=torch.float64, grad_fn=<DivBackward0>)
50
2000
Epoch: 50 Loss D.: -0.012415707111358643
Epoch: 50 Loss G.: 8.99183286105807
tensor(4.8175, dtype=torch.float64, grad_fn=<DivBackward0>)
51
2000
Epoch: 51 Loss D.: -0.0016698837280273438
Epoch: 51 Loss G.: 3.3835709078588545
tensor(5.0714, dtype=torch.float64, grad_fn=<DivBackward0>)
52
2000
Epoch: 52 Loss D.: 0.002045154571533203
Epoch: 52 Loss G.: 8.622739741800077
tensor(3.5328, dtype=torch.float64, grad_fn=<DivBackward0>)
53
2000
Epoch: 53 Loss D.: 0.004864871501922607
Epoch: 53 Loss G.: 2.1644469459773648
tensor(3.5202, dtype=torch.float64, grad_fn=<DivBackward0>)
54
2000
Epoch: 54 Loss D.: -0.0013805031776428223
Epoch: 54 Loss G.: 5.654061229678148
tensor(6.0835, dtype=torch.float64, grad_fn=<DivBackward0>)
55
2000
Epoch: 55 Loss D.: -0.0009788870811462402
Epoch: 55 Loss G.: 3.2091216658630435
tensor(4.8432, dtype=torch.float64, grad_fn=<DivBackward0>)
56
2000
Epoch: 56 Loss D.: 0.007673978805541992
Epoch: 56 Loss G.: 2.337329627105979
tensor(3.8524, dtype=torch.float64, grad_fn=<DivBackward0>)
57
2000
Epoch: 57 Loss D.: 0.007466733455657959
Epoch: 57 Loss G.: 7.865201806870944
tensor(3.4849, dtype=torch.float64, grad_fn=<DivBackward0>)
58
2000
Epoch: 58 Loss D.: 0.002658843994140625
Epoch: 58 Loss G.: 3.0174535202461943
tensor(6.4780, dtype=torch.float64, grad_fn=<DivBackward0>)
59
2000
Epoch: 59 Loss D.: 0.005692422389984131
Epoch: 59 Loss G.: 2.2710889306559032
tensor(3.6512, dtype=torch.float64, grad_fn=<DivBackward0>)
60
2000
Epoch: 60 Loss D.: 0.0035592317581176758
Epoch: 60 Loss G.: 2.7721921399837868
tensor(3.4372, dtype=torch.float64, grad_fn=<DivBackward0>)
61
2000
Epoch: 61 Loss D.: -0.0002473592758178711
Epoch: 61 Loss G.: 7.879167424293511
tensor(3.5543, dtype=torch.float64, grad_fn=<DivBackward0>)
62
2000
Epoch: 62 Loss D.: 0.0020386576652526855
Epoch: 62 Loss G.: 1.4451756643730858
tensor(3.1516, dtype=torch.float64, grad_fn=<DivBackward0>)
63
2000
Epoch: 63 Loss D.: 0.006909787654876709
Epoch: 63 Loss G.: 1.9894547297039091
tensor(2.8265, dtype=torch.float64, grad_fn=<DivBackward0>)
64
2000
Epoch: 64 Loss D.: -0.0014728903770446777
Epoch: 64 Loss G.: 3.0901143381701033
tensor(5.2061, dtype=torch.float64, grad_fn=<DivBackward0>)
65
2000
Epoch: 65 Loss D.: -0.00737917423248291
Epoch: 65 Loss G.: 2.754515981523043
tensor(3.4374, dtype=torch.float64, grad_fn=<DivBackward0>)
66
2000
Epoch: 66 Loss D.: -0.0032045841217041016
Epoch: 66 Loss G.: 4.976074403373248
tensor(2.5874, dtype=torch.float64, grad_fn=<DivBackward0>)
67
2000
Epoch: 67 Loss D.: -0.010806381702423096
Epoch: 67 Loss G.: 2.5334653610382345
tensor(3.3334, dtype=torch.float64, grad_fn=<DivBackward0>)
68
2000
Epoch: 68 Loss D.: 0.01055365800857544
Epoch: 68 Loss G.: 0.9824755292655575
tensor(3.9740, dtype=torch.float64, grad_fn=<DivBackward0>)
69
2000
Epoch: 69 Loss D.: -0.00159531831741333
Epoch: 69 Loss G.: 2.754625799753115
tensor(2.6842, dtype=torch.float64, grad_fn=<DivBackward0>)
70
2000
Epoch: 70 Loss D.: 0.007203996181488037
Epoch: 70 Loss G.: 2.534661120025164
tensor(3.0686, dtype=torch.float64, grad_fn=<DivBackward0>)
71
2000
Epoch: 71 Loss D.: 0.006360769271850586
Epoch: 71 Loss G.: 3.458327898534292
tensor(3.5925, dtype=torch.float64, grad_fn=<DivBackward0>)
72
2000
Epoch: 72 Loss D.: 0.0031695961952209473
Epoch: 72 Loss G.: 2.0025586204252184
tensor(4.5625, dtype=torch.float64, grad_fn=<DivBackward0>)
73
2000
Epoch: 73 Loss D.: 0.0003781914710998535
Epoch: 73 Loss G.: 1.910420287551636
tensor(4.1073, dtype=torch.float64, grad_fn=<DivBackward0>)
74
2000
Epoch: 74 Loss D.: 0.002698361873626709
Epoch: 74 Loss G.: 1.3345830651229749
tensor(2.8838, dtype=torch.float64, grad_fn=<DivBackward0>)
75
2000
Epoch: 75 Loss D.: 0.012036710977554321
Epoch: 75 Loss G.: 10.10933510378217
tensor(3.0350, dtype=torch.float64, grad_fn=<DivBackward0>)
76
2000
Epoch: 76 Loss D.: 0.003169536590576172
Epoch: 76 Loss G.: 2.718930111165456
tensor(3.3788, dtype=torch.float64, grad_fn=<DivBackward0>)
77
2000
Epoch: 77 Loss D.: 0.00941997766494751
Epoch: 77 Loss G.: 1.9815164373836458
tensor(2.3562, dtype=torch.float64, grad_fn=<DivBackward0>)
78
2000
Epoch: 78 Loss D.: 0.0064280033111572266
Epoch: 78 Loss G.: 1.6757370268597076
tensor(3.7260, dtype=torch.float64, grad_fn=<DivBackward0>)
79
2000
Epoch: 79 Loss D.: 0.002738654613494873
Epoch: 79 Loss G.: 1.8975333215317667
tensor(3.2423, dtype=torch.float64, grad_fn=<DivBackward0>)
80
2000
Epoch: 80 Loss D.: 0.004340827465057373
Epoch: 80 Loss G.: 1.318542665927993
tensor(2.6054, dtype=torch.float64, grad_fn=<DivBackward0>)
81
2000
Epoch: 81 Loss D.: 0.009596288204193115
Epoch: 81 Loss G.: 2.020943923582024
tensor(2.1288, dtype=torch.float64, grad_fn=<DivBackward0>)
82
2000
Epoch: 82 Loss D.: 0.010724276304244995
Epoch: 82 Loss G.: 2.073844165452168
tensor(2.3744, dtype=torch.float64, grad_fn=<DivBackward0>)
83
2000
Epoch: 83 Loss D.: 0.02349674701690674
Epoch: 83 Loss G.: 6.7215780259690225
tensor(3.6044, dtype=torch.float64, grad_fn=<DivBackward0>)
84
2000
Epoch: 84 Loss D.: 0.0036098361015319824
Epoch: 84 Loss G.: 0.9680124759752008
tensor(2.8934, dtype=torch.float64, grad_fn=<DivBackward0>)
85
2000
Epoch: 85 Loss D.: 0.007830142974853516
Epoch: 85 Loss G.: 5.411076037434585
tensor(2.4734, dtype=torch.float64, grad_fn=<DivBackward0>)
86
2000
Epoch: 86 Loss D.: -0.01609128713607788
Epoch: 86 Loss G.: 2.6601695029416144
tensor(2.8231, dtype=torch.float64, grad_fn=<DivBackward0>)
87
2000
Epoch: 87 Loss D.: 0.0020009279251098633
Epoch: 87 Loss G.: 2.8574741846841114
tensor(3.4043, dtype=torch.float64, grad_fn=<DivBackward0>)
88
2000
Epoch: 88 Loss D.: 0.0035643577575683594
Epoch: 88 Loss G.: 1.1979752808517345
tensor(2.4655, dtype=torch.float64, grad_fn=<DivBackward0>)
89
2000
Epoch: 89 Loss D.: 0.014302223920822144
Epoch: 89 Loss G.: 5.221933046134465
tensor(2.9496, dtype=torch.float64, grad_fn=<DivBackward0>)
90
2000
Epoch: 90 Loss D.: -0.0005453824996948242
Epoch: 90 Loss G.: 2.1196305393880737
tensor(3.3136, dtype=torch.float64, grad_fn=<DivBackward0>)
91
2000
Epoch: 91 Loss D.: -0.0069506168365478516
Epoch: 91 Loss G.: 1.2554934904271233
tensor(2.5416, dtype=torch.float64, grad_fn=<DivBackward0>)
92
2000
Epoch: 92 Loss D.: 0.0014049410820007324
Epoch: 92 Loss G.: 1.5759316541856658
tensor(2.7460, dtype=torch.float64, grad_fn=<DivBackward0>)
93
2000
Epoch: 93 Loss D.: -0.002243220806121826
Epoch: 93 Loss G.: 1.2817052201038732
tensor(3.4128, dtype=torch.float64, grad_fn=<DivBackward0>)
94
2000
Epoch: 94 Loss D.: 0.0014655590057373047
Epoch: 94 Loss G.: 1.108591827901358
tensor(2.4165, dtype=torch.float64, grad_fn=<DivBackward0>)
在第94轮收敛
----------------第0轮的原数据---------------------
         range  dissimilarity_value  ...  Euclidean_distance    distance
0     0.021189             0.028072  ...            0.972037   36.490156
1     0.022505             0.020713  ...            0.596204   80.752001
2     0.027793             0.019238  ...            0.603167   86.039994
3     0.029261             0.022675  ...            0.639134   68.833509
4     0.033224             0.014336  ...            0.546747  126.955337
..         ...                  ...  ...                 ...         ...
195  15.380619             0.028406  ...            1.014211   34.651612
196  15.550505             0.029834  ...            1.051261   31.856904
197  16.009261             0.027054  ...            1.143958   32.255424
198  16.143406             0.032103  ...            1.187538   26.206339
199  21.687074             0.035053  ...            1.307851   21.775128

[200 rows x 5 columns]
-----------------按照dissimilarity_value排序----------
     range  dissimilarity_value  cos_distance  Euclidean_distance  distance
0    6.222                0.003         0.999               0.215  1490.612
1    8.350                0.004         0.998               0.255   873.274
2    4.404                0.004         0.997               0.298   744.360
3    2.097                0.004         0.998               0.259   890.367
4    8.577                0.005         0.999               0.251   873.346
..     ...                  ...           ...                 ...       ...
195  0.778                0.047         0.994               1.096    19.121
196  0.812                0.047         0.992               0.978    21.459
197  0.779                0.048         0.987               1.393    14.667
198  0.817                0.052         0.990               1.147    16.549
199  0.783                0.055         0.996               1.080    16.921

[200 rows x 5 columns]
-----------------按照range排序----------
================= config4 =================
2022年 02月 22日 星期二 10:47:08 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (126886) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (202183) - 没有那个进程
      range  dissimilarity_value  cos_distance  Euclidean_distance  distance
0     0.021                0.028         0.996               0.972    36.490
1     0.023                0.021         0.997               0.596    80.752
2     0.028                0.019         0.998               0.603    86.040
3     0.029                0.023         0.998               0.639    68.834
4     0.033                0.014         0.995               0.547   126.955
..      ...                  ...           ...                 ...       ...
195  15.381                0.028         0.998               1.014    34.652
196  15.551                0.030         0.999               1.051    31.857
197  16.009                0.027         0.998               1.144    32.255
198  16.143                0.032         0.999               1.188    26.206
199  21.687                0.035         0.998               1.308    21.775

[200 rows x 5 columns]
-----------------按照cos_distance排序----------
      range  dissimilarity_value  cos_distance  Euclidean_distance  distance
0     0.779                0.048         0.987               1.393    14.667
1     0.817                0.052         0.990               1.147    16.549
2     0.803                0.041         0.992               0.981    24.403
3     0.812                0.047         0.992               0.978    21.459
4     0.778                0.047         0.994               1.096    19.121
..      ...                  ...           ...                 ...       ...
195  11.845                0.017         0.999               0.668    88.872
196   4.077                0.005         0.999               0.233   830.076
197  12.052                0.021         0.999               0.723    66.508
198   6.222                0.003         0.999               0.215  1490.612
199  11.753                0.020         0.999               0.674    73.900

[200 rows x 5 columns]
-----------------按照Euclidean_distance排序----------
      range  dissimilarity_value  cos_distance  Euclidean_distance  distance
0     6.180                0.005         0.999               0.183   996.403
1    10.479                0.006         0.999               0.213   848.896
2     6.222                0.003         0.999               0.215  1490.612
3     8.225                0.007         0.999               0.225   637.309
4     6.208                0.007         0.999               0.228   588.950
..      ...                  ...           ...                 ...       ...
195   0.817                0.052         0.990               1.147    16.549
196  16.143                0.032         0.999               1.188    26.206
197   0.787                0.046         0.996               1.194    18.012
198  21.687                0.035         0.998               1.308    21.775
199   0.779                0.048         0.987               1.393    14.667

[200 rows x 5 columns]
-----------------按照distance排序----------
     range  dissimilarity_value  cos_distance  Euclidean_distance  distance
0    6.222                0.003         0.999               0.215  1490.612
1    6.180                0.005         0.999               0.183   996.403
2    2.097                0.004         0.998               0.259   890.367
3    8.577                0.005         0.999               0.251   873.346
4    8.350                0.004         0.998               0.255   873.274
..     ...                  ...           ...                 ...       ...
195  0.778                0.047         0.994               1.096    19.121
196  0.787                0.046         0.996               1.194    18.012
197  0.783                0.055         0.996               1.080    16.921
198  0.817                0.052         0.990               1.147    16.549
199  0.779                0.048         0.987               1.393    14.667

[200 rows x 5 columns]
   distance
0  1490.612
1   996.403
2   890.367
3   228.793
4   227.509
5   225.741
6    16.921
7    16.549
8    14.667
   distance
0  1490.612
1   228.793
2    14.667
3   996.403
4   227.509
5    16.549
6   890.367
7   225.741
8    16.921
sgan数据生成时间花费为：1460.97
   spark.broadcast.blockSize  ...  spark.storage.memoryMapThreshold
0                       35.0  ...                               1.0
1                       36.0  ...                               1.0
2                       41.0  ...                               1.0
3                       36.0  ...                               1.0
4                       35.0  ...                               1.0
5                       41.0  ...                               1.0
6                       36.0  ...                               1.0
7                       36.0  ...                               1.0
8                       39.0  ...                               1.0

[9 rows x 27 columns]
需要通过formatConf处理的数据 : conf = spark.broadcast.blockSize	 value = 35.0
需要通过formatConf处理的数据 : conf = spark.broadcast.checksum	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.broadcast.compress	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.default.parallelism	 value = 279.0
需要通过formatConf处理的数据 : conf = spark.executor.cores	 value = 4.0
需要通过formatConf处理的数据 : conf = spark.executor.instances	 value = 8.0
需要通过formatConf处理的数据 : conf = spark.executor.memory	 value = 4.0
需要通过formatConf处理的数据 : conf = spark.executor.memoryOverhead	 value = 434.0
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer	 value = 128.0
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer.max	 value = 112.0
需要通过formatConf处理的数据 : conf = spark.locality.wait	 value = 7.0
需要通过formatConf处理的数据 : conf = spark.maxRemoteBlockSizeFetchToMem	 value = 1230936576.0
需要通过formatConf处理的数据 : conf = spark.memory.fraction	 value = 0.8399999737739563
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.enabled	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.size	 value = 891.0
需要通过formatConf处理的数据 : conf = spark.memory.storageFraction	 value = 0.8799999952316284
需要通过formatConf处理的数据 : conf = spark.rdd.compress	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxBlocksInFlightPerAddress	 value = 1485115136.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxReqsInFlight	 value = 1808978432.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxSizeInFlight	 value = 51.0
需要通过formatConf处理的数据 : conf = spark.scheduler.mode	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.scheduler.revive.interval	 value = 562.0
需要通过formatConf处理的数据 : conf = spark.shuffle.compress	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.shuffle.file.buffer	 value = 36.0
需要通过formatConf处理的数据 : conf = spark.shuffle.io.numConnectionsPerPeer	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.shuffle.sort.bypassMergeThreshold	 value = 343.0
需要通过formatConf处理的数据 : conf = spark.storage.memoryMapThreshold	 value = 1.0
configNum = 4	 run_cmd = /usr/local/home/zwr/wordcount-100G-ga.sh 4 /usr/local/home/yyq/bo/ganrs_bo_new/config/wordcount-100G
run_cmd命令执行成功
   spark.broadcast.blockSize  ...  runtime
0                       35.0  ...  159.559

[1 rows x 28 columns]
  spark.broadcast.blockSize  ...  runtime
0                        35  ...  221.911
1                        40  ...  373.212
2                        56  ...  831.529
3                      35.0  ...  159.559

[4 rows x 28 columns]
需要通过formatConf处理的数据 : conf = spark.broadcast.blockSize	 value = 36.0
需要通过formatConf处理的数据 : conf = spark.broadcast.checksum	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.broadcast.compress	 value = 1.0
================= config5 =================
2022年 02月 22日 星期二 10:49:57 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (129533) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (204130) - 没有那个进程
================= config6 =================
2022年 02月 22日 星期二 10:52:44 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (139814) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (222138) - 没有那个进程
需要通过formatConf处理的数据 : conf = spark.default.parallelism	 value = 260.0
需要通过formatConf处理的数据 : conf = spark.executor.cores	 value = 4.0
需要通过formatConf处理的数据 : conf = spark.executor.instances	 value = 8.0
需要通过formatConf处理的数据 : conf = spark.executor.memory	 value = 4.0
需要通过formatConf处理的数据 : conf = spark.executor.memoryOverhead	 value = 420.0
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer	 value = 127.0
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer.max	 value = 109.0
需要通过formatConf处理的数据 : conf = spark.locality.wait	 value = 7.0
需要通过formatConf处理的数据 : conf = spark.maxRemoteBlockSizeFetchToMem	 value = 1262467456.0
需要通过formatConf处理的数据 : conf = spark.memory.fraction	 value = 0.8500000238418579
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.enabled	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.size	 value = 935.0
需要通过formatConf处理的数据 : conf = spark.memory.storageFraction	 value = 0.8899999856948853
需要通过formatConf处理的数据 : conf = spark.rdd.compress	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxBlocksInFlightPerAddress	 value = 1538274176.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxReqsInFlight	 value = 1818238976.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxSizeInFlight	 value = 51.0
需要通过formatConf处理的数据 : conf = spark.scheduler.mode	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.scheduler.revive.interval	 value = 555.0
需要通过formatConf处理的数据 : conf = spark.shuffle.compress	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.shuffle.file.buffer	 value = 36.0
需要通过formatConf处理的数据 : conf = spark.shuffle.io.numConnectionsPerPeer	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.shuffle.sort.bypassMergeThreshold	 value = 350.0
需要通过formatConf处理的数据 : conf = spark.storage.memoryMapThreshold	 value = 1.0
configNum = 5	 run_cmd = /usr/local/home/zwr/wordcount-100G-ga.sh 5 /usr/local/home/yyq/bo/ganrs_bo_new/config/wordcount-100G
run_cmd命令执行成功
   spark.broadcast.blockSize  ...  runtime
0                       36.0  ...  159.011

[1 rows x 28 columns]
  spark.broadcast.blockSize  ...  runtime
0                        35  ...  221.911
1                        40  ...  373.212
2                        56  ...  831.529
3                      35.0  ...  159.559
4                      36.0  ...  159.011

[5 rows x 28 columns]
需要通过formatConf处理的数据 : conf = spark.broadcast.blockSize	 value = 41.0
需要通过formatConf处理的数据 : conf = spark.broadcast.checksum	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.broadcast.compress	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.default.parallelism	 value = 287.0
需要通过formatConf处理的数据 : conf = spark.executor.cores	 value = 4.0
需要通过formatConf处理的数据 : conf = spark.executor.instances	 value = 7.0
需要通过formatConf处理的数据 : conf = spark.executor.memory	 value = 4.0
需要通过formatConf处理的数据 : conf = spark.executor.memoryOverhead	 value = 489.0
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer	 value = 122.0
需要通过formatConf处理的数据 : conf = spark.kryoserializer.buffer.max	 value = 104.0
需要通过formatConf处理的数据 : conf = spark.locality.wait	 value = 7.0
需要通过formatConf处理的数据 : conf = spark.maxRemoteBlockSizeFetchToMem	 value = 1354656000.0
需要通过formatConf处理的数据 : conf = spark.memory.fraction	 value = 0.8299999833106995
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.enabled	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.memory.offHeap.size	 value = 822.0
需要通过formatConf处理的数据 : conf = spark.memory.storageFraction	 value = 0.8399999737739563
需要通过formatConf处理的数据 : conf = spark.rdd.compress	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxBlocksInFlightPerAddress	 value = 1476824064.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxReqsInFlight	 value = 1787918208.0
需要通过formatConf处理的数据 : conf = spark.reducer.maxSizeInFlight	 value = 52.0
需要通过formatConf处理的数据 : conf = spark.scheduler.mode	 value = 0.0
需要通过formatConf处理的数据 : conf = spark.scheduler.revive.interval	 value = 630.0
需要通过formatConf处理的数据 : conf = spark.shuffle.compress	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.shuffle.file.buffer	 value = 35.0
需要通过formatConf处理的数据 : conf = spark.shuffle.io.numConnectionsPerPeer	 value = 1.0
需要通过formatConf处理的数据 : conf = spark.shuffle.sort.bypassMergeThreshold	 value = 315.0
需要通过formatConf处理的数据 : conf = spark.storage.memoryMapThreshold	 value = 1.0
configNum = 6	 run_cmd = /usr/local/home/zwr/wordcount-100G-ga.sh 6 /usr/local/home/yyq/bo/ganrs_bo_new/config/wordcount-100G
run_cmd命令执行成功
   spark.broadcast.blockSize  ...  runtime
0                       41.0  ...  153.328

[1 rows x 28 columns]
  spark.broadcast.blockSize  ...  runtime
0                        35  ...  221.911
1                        40  ...  373.212
2                        56  ...  831.529
3                      35.0  ...  159.559
4                      36.0  ...  159.011
5                      41.0  ...  153.328

[6 rows x 28 columns]
  spark.broadcast.blockSize  ...  runtime
0                        35  ...  221.911
1                        40  ...  373.212
2                        56  ...  831.529
3                      35.0  ...  159.559
4                      36.0  ...  159.011
5                      41.0  ...  153.328

[6 rows x 28 columns]
Traceback (most recent call last):
  File "/usr/local/home/yyq/bo/ganrs_bo_new/ganrs_Bayesian_Optimization_server_cycle.py", line 278, in <module>
    initsamples = ganrs_samples_all(initsamples_df=dataset)
TypeError: ganrs_samples_all() missing 1 required positional argument: 'vital_params_list'
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/logs.json" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/generationConf.csv" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/target.png" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/dataset.csv" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/ganrs_bo_new/general_data.csv" 的文件状态(stat): 没有那个文件或目录
=============== finish wordcount-100G ===============
2022年 02月 22日 星期二 10:55:27 CST
=============== finish wordcount-100G ===============
