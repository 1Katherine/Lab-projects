nohup: 忽略输入
/usr/local/home/yyq/bo/feature_selection_bo
=============== start $1 ===============
2022年 03月 01日 星期二 18:56:38 CST
=============== start $1 ===============
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/config/terasort-20G" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/logs/" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/generationConf/" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/target/" 的文件状态(stat): 没有那个文件或目录
--niters = 30	 --ninits = 2
重要参数列表（将贝叶斯的x_probe按照重要参数列表顺序转成配置文件实际运行:
                                         vital_params
0                           spark.broadcast.blockSize
1                            spark.broadcast.checksum
2                            spark.broadcast.compress
3                   spark.cleaner.periodicGC.interval
4                     spark.cleaner.referenceTracking
5            spark.cleaner.referenceTracking.blocking
6    spark.cleaner.referenceTracking.blocking.shuffle
7    spark.cleaner.referenceTracking.cleanCheckpoints
8                           spark.default.parallelism
9                          spark.driver.maxResultSize
10                        spark.driver.memoryOverhead
11                               spark.executor.cores
12                   spark.executor.heartbeatInterval
13                           spark.executor.instances
14                              spark.executor.memory
15                           spark.files.fetchTimeout
16                      spark.files.maxPartitionBytes
17                        spark.files.openCostInBytes
18                              spark.files.overwrite
19                      spark.reducer.maxReqsInFlight
20                      spark.reducer.maxSizeInFlight
21                               spark.rpc.io.backLog
22                            spark.rpc.lookupTimeout
23                          spark.rpc.message.maxSize
24                               spark.rpc.retry.wait
25  spark.scheduler.blacklist.unschedulableTaskSet...
26    spark.scheduler.listenerbus.eventqueue.capacity
27        spark.scheduler.minRegisteredResourcesRatio
28                               spark.scheduler.mode
29                    spark.scheduler.revive.interval
30                 spark.serializer.objectStreamReset
31                             spark.shuffle.compress
32                          spark.shuffle.file.buffer
33                           spark.shuffle.io.backLog
34                        spark.shuffle.io.maxRetries
35             spark.shuffle.io.numConnectionsPerPeer
36                  spark.shuffle.io.preferDirectBufs
37            spark.shuffle.maxChunksBeingTransferred
38                       spark.shuffle.memoryFraction
39                          spark.files.useFetchCache
40  spark.hadoop.mapreduce.fileoutputcommitter.alg...
41                 spark.io.compression.lz4.blockSize
42              spark.io.compression.snappy.blockSize
43                       spark.kryo.referenceTracking
44                        spark.kryoserializer.buffer
45                    spark.kryoserializer.buffer.max
46                                spark.locality.wait
47                 spark.maxRemoteBlockSizeFetchToMem
48                              spark.memory.fraction
49                       spark.memory.offHeap.enabled
50                          spark.memory.offHeap.size
51                       spark.memory.storageFraction
52                         spark.memory.useLegacyMode
53                              spark.port.maxRetries
54                         spark.python.worker.memory
55                          spark.python.worker.reuse
56                                 spark.rdd.compress
57          spark.reducer.maxBlocksInFlightPerAddress
58             spark.shuffle.service.index.cache.size
59            spark.shuffle.sort.bypassMergeThreshold
60                       spark.shuffle.spill.compress
61                         spark.speculation.interval
62                       spark.speculation.multiplier
63                         spark.speculation.quantile
64                 spark.stage.maxConsecutiveAttempts
65                       spark.storage.memoryFraction
66                   spark.storage.memoryMapThreshold
67                spark.storage.replication.proactive
68                       spark.storage.unrollFraction
69               spark.streaming.backpressure.enabled
70                      spark.streaming.blockInterval
71      spark.streaming.receiver.writeAheadLog.enable
72           spark.streaming.stopGracefullyOnShutdown
73                          spark.streaming.unpersist
74                             spark.task.maxFailures
75                          spark.task.reaper.enabled
76                  spark.task.reaper.pollingInterval
77                       spark.task.reaper.threadDump
获取初始样本时，按照贝叶斯内部的key顺序传初始样本和已有的执行时间：
vital_params_name = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.cleaner.periodicGC.interval', 'spark.cleaner.referenceTracking', 'spark.cleaner.referenceTracking.blocking', 'spark.cleaner.referenceTracking.blocking.shuffle', 'spark.cleaner.referenceTracking.cleanCheckpoints', 'spark.default.parallelism', 'spark.driver.maxResultSize', 'spark.driver.memoryOverhead', 'spark.executor.cores', 'spark.executor.heartbeatInterval', 'spark.executor.instances', 'spark.executor.memory', 'spark.files.fetchTimeout', 'spark.files.maxPartitionBytes', 'spark.files.openCostInBytes', 'spark.files.overwrite', 'spark.files.useFetchCache', 'spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', 'spark.io.compression.lz4.blockSize', 'spark.io.compression.snappy.blockSize', 'spark.kryo.referenceTracking', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.memory.useLegacyMode', 'spark.port.maxRetries', 'spark.python.worker.memory', 'spark.python.worker.reuse', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.rpc.io.backLog', 'spark.rpc.lookupTimeout', 'spark.rpc.message.maxSize', 'spark.rpc.retry.wait', 'spark.scheduler.blacklist.unschedulableTaskSetTimeout', 'spark.scheduler.listenerbus.eventqueue.capacity', 'spark.scheduler.minRegisteredResourcesRatio', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.serializer.objectStreamReset', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.backLog', 'spark.shuffle.io.maxRetries', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.io.preferDirectBufs', 'spark.shuffle.maxChunksBeingTransferred', 'spark.shuffle.memoryFraction', 'spark.shuffle.service.index.cache.size', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.shuffle.spill.compress', 'spark.speculation.interval', 'spark.speculation.multiplier', 'spark.speculation.quantile', 'spark.stage.maxConsecutiveAttempts', 'spark.storage.memoryFraction', 'spark.storage.memoryMapThreshold', 'spark.storage.replication.proactive', 'spark.storage.unrollFraction', 'spark.streaming.backpressure.enabled', 'spark.streaming.blockInterval', 'spark.streaming.receiver.writeAheadLog.enable', 'spark.streaming.stopGracefullyOnShutdown', 'spark.streaming.unpersist', 'spark.task.maxFailures', 'spark.task.reaper.enabled', 'spark.task.reaper.pollingInterval', 'spark.task.reaper.threadDump']
进入current_niterations，current_niterations = 0
iterations == 1
迭代结束条件，当执行时间低于Tmax时，停止搜索，此时对应优化倍数为12倍。Tmax = 91.66666666666667
================= config1 =================
2022年 03月 01日 星期二 18:56:40 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (58532) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (1272) - 没有那个进程
================= config2 =================
2022年 03月 01日 星期二 19:30:11 CST
nohup: 忽略输入
/usr/local/home/yyq/bo/feature_selection_bo
=============== start $1 ===============
2022年 03月 01日 星期二 19:33:35 CST
=============== start $1 ===============
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/config/wordcount-100G" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/logs/" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/generationConf/" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/target/" 的文件状态(stat): 没有那个文件或目录
--niters = 30	 --ninits = 2
重要参数列表（将贝叶斯的x_probe按照重要参数列表顺序转成配置文件实际运行:
                                         vital_params
0                           spark.broadcast.blockSize
1                            spark.broadcast.checksum
2                            spark.broadcast.compress
3                   spark.cleaner.periodicGC.interval
4                     spark.cleaner.referenceTracking
5            spark.cleaner.referenceTracking.blocking
6    spark.cleaner.referenceTracking.blocking.shuffle
7    spark.cleaner.referenceTracking.cleanCheckpoints
8                           spark.default.parallelism
9                          spark.driver.maxResultSize
10                        spark.driver.memoryOverhead
11                               spark.executor.cores
12                   spark.executor.heartbeatInterval
13                           spark.executor.instances
14                              spark.executor.memory
15                           spark.files.fetchTimeout
16                      spark.files.maxPartitionBytes
17                        spark.files.openCostInBytes
18                              spark.files.overwrite
19                      spark.reducer.maxReqsInFlight
20                      spark.reducer.maxSizeInFlight
21                               spark.rpc.io.backLog
22                            spark.rpc.lookupTimeout
23                          spark.rpc.message.maxSize
24                               spark.rpc.retry.wait
25  spark.scheduler.blacklist.unschedulableTaskSet...
26    spark.scheduler.listenerbus.eventqueue.capacity
27        spark.scheduler.minRegisteredResourcesRatio
28                               spark.scheduler.mode
29                    spark.scheduler.revive.interval
30                 spark.serializer.objectStreamReset
31                             spark.shuffle.compress
32                          spark.shuffle.file.buffer
33                           spark.shuffle.io.backLog
34                        spark.shuffle.io.maxRetries
35             spark.shuffle.io.numConnectionsPerPeer
36                  spark.shuffle.io.preferDirectBufs
37            spark.shuffle.maxChunksBeingTransferred
38                       spark.shuffle.memoryFraction
39                          spark.files.useFetchCache
40  spark.hadoop.mapreduce.fileoutputcommitter.alg...
41                 spark.io.compression.lz4.blockSize
42              spark.io.compression.snappy.blockSize
43                       spark.kryo.referenceTracking
44                        spark.kryoserializer.buffer
45                    spark.kryoserializer.buffer.max
46                                spark.locality.wait
47                 spark.maxRemoteBlockSizeFetchToMem
48                              spark.memory.fraction
49                       spark.memory.offHeap.enabled
50                          spark.memory.offHeap.size
51                       spark.memory.storageFraction
52                         spark.memory.useLegacyMode
53                              spark.port.maxRetries
54                         spark.python.worker.memory
55                          spark.python.worker.reuse
56                                 spark.rdd.compress
57          spark.reducer.maxBlocksInFlightPerAddress
58             spark.shuffle.service.index.cache.size
59            spark.shuffle.sort.bypassMergeThreshold
60                       spark.shuffle.spill.compress
61                         spark.speculation.interval
62                       spark.speculation.multiplier
63                         spark.speculation.quantile
64                 spark.stage.maxConsecutiveAttempts
65                       spark.storage.memoryFraction
66                   spark.storage.memoryMapThreshold
67                spark.storage.replication.proactive
68                       spark.storage.unrollFraction
69               spark.streaming.backpressure.enabled
70                      spark.streaming.blockInterval
71      spark.streaming.receiver.writeAheadLog.enable
72           spark.streaming.stopGracefullyOnShutdown
73                          spark.streaming.unpersist
74                             spark.task.maxFailures
75                          spark.task.reaper.enabled
76                  spark.task.reaper.pollingInterval
77                       spark.task.reaper.threadDump
获取初始样本时，按照贝叶斯内部的key顺序传初始样本和已有的执行时间：
vital_params_name = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.cleaner.periodicGC.interval', 'spark.cleaner.referenceTracking', 'spark.cleaner.referenceTracking.blocking', 'spark.cleaner.referenceTracking.blocking.shuffle', 'spark.cleaner.referenceTracking.cleanCheckpoints', 'spark.default.parallelism', 'spark.driver.maxResultSize', 'spark.driver.memoryOverhead', 'spark.executor.cores', 'spark.executor.heartbeatInterval', 'spark.executor.instances', 'spark.executor.memory', 'spark.files.fetchTimeout', 'spark.files.maxPartitionBytes', 'spark.files.openCostInBytes', 'spark.files.overwrite', 'spark.files.useFetchCache', 'spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', 'spark.io.compression.lz4.blockSize', 'spark.io.compression.snappy.blockSize', 'spark.kryo.referenceTracking', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.memory.useLegacyMode', 'spark.port.maxRetries', 'spark.python.worker.memory', 'spark.python.worker.reuse', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.rpc.io.backLog', 'spark.rpc.lookupTimeout', 'spark.rpc.message.maxSize', 'spark.rpc.retry.wait', 'spark.scheduler.blacklist.unschedulableTaskSetTimeout', 'spark.scheduler.listenerbus.eventqueue.capacity', 'spark.scheduler.minRegisteredResourcesRatio', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.serializer.objectStreamReset', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.backLog', 'spark.shuffle.io.maxRetries', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.io.preferDirectBufs', 'spark.shuffle.maxChunksBeingTransferred', 'spark.shuffle.memoryFraction', 'spark.shuffle.service.index.cache.size', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.shuffle.spill.compress', 'spark.speculation.interval', 'spark.speculation.multiplier', 'spark.speculation.quantile', 'spark.stage.maxConsecutiveAttempts', 'spark.storage.memoryFraction', 'spark.storage.memoryMapThreshold', 'spark.storage.replication.proactive', 'spark.storage.unrollFraction', 'spark.streaming.backpressure.enabled', 'spark.streaming.blockInterval', 'spark.streaming.receiver.writeAheadLog.enable', 'spark.streaming.stopGracefullyOnShutdown', 'spark.streaming.unpersist', 'spark.task.maxFailures', 'spark.task.reaper.enabled', 'spark.task.reaper.pollingInterval', 'spark.task.reaper.threadDump']
进入current_niterations，current_niterations = 0
iterations == 1
迭代结束条件，当执行时间低于Tmax时，停止搜索，此时对应优化倍数为12倍。Tmax = 91.66666666666667
================= config1 =================
2022年 03月 01日 星期二 19:33:37 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
nohup: 忽略输入
/usr/local/home/yyq/bo/feature_selection_bo
=============== start $1 ===============
2022年 03月 01日 星期二 19:37:34 CST
=============== start $1 ===============
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/logs/" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/generationConf/" 的文件状态(stat): 没有那个文件或目录
mv: 无法获取"/usr/local/home/yyq/bo/feature_selection_bo/target/" 的文件状态(stat): 没有那个文件或目录
总共搜索的样本数niters = 30	 第一次随机采样的初始样本数 ninits = 2
重要参数列表（将贝叶斯的x_probe按照重要参数列表顺序转成配置文件实际运行:
                                         vital_params
0                           spark.broadcast.blockSize
1                            spark.broadcast.checksum
2                            spark.broadcast.compress
3                   spark.cleaner.periodicGC.interval
4                     spark.cleaner.referenceTracking
5            spark.cleaner.referenceTracking.blocking
6    spark.cleaner.referenceTracking.blocking.shuffle
7    spark.cleaner.referenceTracking.cleanCheckpoints
8                           spark.default.parallelism
9                          spark.driver.maxResultSize
10                        spark.driver.memoryOverhead
11                               spark.executor.cores
12                   spark.executor.heartbeatInterval
13                           spark.executor.instances
14                              spark.executor.memory
15                           spark.files.fetchTimeout
16                      spark.files.maxPartitionBytes
17                        spark.files.openCostInBytes
18                              spark.files.overwrite
19                      spark.reducer.maxReqsInFlight
20                      spark.reducer.maxSizeInFlight
21                               spark.rpc.io.backLog
22                            spark.rpc.lookupTimeout
23                          spark.rpc.message.maxSize
24                               spark.rpc.retry.wait
25  spark.scheduler.blacklist.unschedulableTaskSet...
26    spark.scheduler.listenerbus.eventqueue.capacity
27        spark.scheduler.minRegisteredResourcesRatio
28                               spark.scheduler.mode
29                    spark.scheduler.revive.interval
30                 spark.serializer.objectStreamReset
31                             spark.shuffle.compress
32                          spark.shuffle.file.buffer
33                           spark.shuffle.io.backLog
34                        spark.shuffle.io.maxRetries
35             spark.shuffle.io.numConnectionsPerPeer
36                  spark.shuffle.io.preferDirectBufs
37            spark.shuffle.maxChunksBeingTransferred
38                       spark.shuffle.memoryFraction
39                          spark.files.useFetchCache
40  spark.hadoop.mapreduce.fileoutputcommitter.alg...
41                 spark.io.compression.lz4.blockSize
42              spark.io.compression.snappy.blockSize
43                       spark.kryo.referenceTracking
44                        spark.kryoserializer.buffer
45                    spark.kryoserializer.buffer.max
46                                spark.locality.wait
47                 spark.maxRemoteBlockSizeFetchToMem
48                              spark.memory.fraction
49                       spark.memory.offHeap.enabled
50                          spark.memory.offHeap.size
51                       spark.memory.storageFraction
52                         spark.memory.useLegacyMode
53                              spark.port.maxRetries
54                         spark.python.worker.memory
55                          spark.python.worker.reuse
56                                 spark.rdd.compress
57          spark.reducer.maxBlocksInFlightPerAddress
58             spark.shuffle.service.index.cache.size
59            spark.shuffle.sort.bypassMergeThreshold
60                       spark.shuffle.spill.compress
61                         spark.speculation.interval
62                       spark.speculation.multiplier
63                         spark.speculation.quantile
64                 spark.stage.maxConsecutiveAttempts
65                       spark.storage.memoryFraction
66                   spark.storage.memoryMapThreshold
67                spark.storage.replication.proactive
68                       spark.storage.unrollFraction
69               spark.streaming.backpressure.enabled
70                      spark.streaming.blockInterval
71      spark.streaming.receiver.writeAheadLog.enable
72           spark.streaming.stopGracefullyOnShutdown
73                          spark.streaming.unpersist
74                             spark.task.maxFailures
75                          spark.task.reaper.enabled
76                  spark.task.reaper.pollingInterval
77                       spark.task.reaper.threadDump
获取初始样本时，按照贝叶斯内部的key顺序传初始样本和已有的执行时间：
vital_params_name = ['spark.broadcast.blockSize', 'spark.broadcast.checksum', 'spark.broadcast.compress', 'spark.cleaner.periodicGC.interval', 'spark.cleaner.referenceTracking', 'spark.cleaner.referenceTracking.blocking', 'spark.cleaner.referenceTracking.blocking.shuffle', 'spark.cleaner.referenceTracking.cleanCheckpoints', 'spark.default.parallelism', 'spark.driver.maxResultSize', 'spark.driver.memoryOverhead', 'spark.executor.cores', 'spark.executor.heartbeatInterval', 'spark.executor.instances', 'spark.executor.memory', 'spark.files.fetchTimeout', 'spark.files.maxPartitionBytes', 'spark.files.openCostInBytes', 'spark.files.overwrite', 'spark.files.useFetchCache', 'spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', 'spark.io.compression.lz4.blockSize', 'spark.io.compression.snappy.blockSize', 'spark.kryo.referenceTracking', 'spark.kryoserializer.buffer', 'spark.kryoserializer.buffer.max', 'spark.locality.wait', 'spark.maxRemoteBlockSizeFetchToMem', 'spark.memory.fraction', 'spark.memory.offHeap.enabled', 'spark.memory.offHeap.size', 'spark.memory.storageFraction', 'spark.memory.useLegacyMode', 'spark.port.maxRetries', 'spark.python.worker.memory', 'spark.python.worker.reuse', 'spark.rdd.compress', 'spark.reducer.maxBlocksInFlightPerAddress', 'spark.reducer.maxReqsInFlight', 'spark.reducer.maxSizeInFlight', 'spark.rpc.io.backLog', 'spark.rpc.lookupTimeout', 'spark.rpc.message.maxSize', 'spark.rpc.retry.wait', 'spark.scheduler.blacklist.unschedulableTaskSetTimeout', 'spark.scheduler.listenerbus.eventqueue.capacity', 'spark.scheduler.minRegisteredResourcesRatio', 'spark.scheduler.mode', 'spark.scheduler.revive.interval', 'spark.serializer.objectStreamReset', 'spark.shuffle.compress', 'spark.shuffle.file.buffer', 'spark.shuffle.io.backLog', 'spark.shuffle.io.maxRetries', 'spark.shuffle.io.numConnectionsPerPeer', 'spark.shuffle.io.preferDirectBufs', 'spark.shuffle.maxChunksBeingTransferred', 'spark.shuffle.memoryFraction', 'spark.shuffle.service.index.cache.size', 'spark.shuffle.sort.bypassMergeThreshold', 'spark.shuffle.spill.compress', 'spark.speculation.interval', 'spark.speculation.multiplier', 'spark.speculation.quantile', 'spark.stage.maxConsecutiveAttempts', 'spark.storage.memoryFraction', 'spark.storage.memoryMapThreshold', 'spark.storage.replication.proactive', 'spark.storage.unrollFraction', 'spark.streaming.backpressure.enabled', 'spark.streaming.blockInterval', 'spark.streaming.receiver.writeAheadLog.enable', 'spark.streaming.stopGracefullyOnShutdown', 'spark.streaming.unpersist', 'spark.task.maxFailures', 'spark.task.reaper.enabled', 'spark.task.reaper.pollingInterval', 'spark.task.reaper.threadDump']
进入current_niterations，current_niterations = 0
iterations == 1
迭代结束条件，当执行时间低于Tmax时，停止搜索，此时对应优化倍数为12倍。Tmax = 91.66666666666667
================= config1 =================
2022年 03月 01日 星期二 19:37:37 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
-------------------stop k8s-node02 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (67031) - 没有那个进程
-------------------stop k8s-node03 --------------
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
/usr/local/home/zwr/stop.sh: 第 3 行:kill: (10273) - 没有那个进程
================= config2 =================
2022年 03月 01日 星期二 19:40:16 CST
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stdout = io.open(c2pread, 'rb', bufsize)
/usr/local/home/python3/python3/lib/python3.8/subprocess.py:853: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
  self.stderr = io.open(errread, 'rb', bufsize)
cmd

end before line
finish
