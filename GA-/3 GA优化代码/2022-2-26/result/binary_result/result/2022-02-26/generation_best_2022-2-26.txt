
binary GA xgb ,sizePop=30 ,maxIter=30
['spark.kryoserializer.buffer', 'spark.sql.autoBroadcastJoinThreshold', 'spark.sql.shuffle.partitions', 'spark.scheduler.revive.interval', 'spark.executor.memoryOverhead', 'spark.shuffle.file.buffer', 'spark.executor.cores', 'spark.executor.instances', 'spark.executor.memory', 'spark.default.parallelism']
best_x : [1.160000e+02 8.482388e+06 2.590000e+02 6.800000e+02 8.280000e+02
 4.400000e+01 2.000000e+00 5.000000e+00 4.000000e+00 3.540000e+02]
best_y : [1469.0784]
generation_best 

{'spark.kryoserializer.buffer': 117.0, 'spark.sql.autoBroadcastJoinThreshold': 13639741.0, 'spark.sql.shuffle.partitions': 290.0, 'spark.scheduler.revive.interval': 960.0, 'spark.executor.memoryOverhead': 718.0, 'spark.shuffle.file.buffer': 40.0, 'spark.executor.cores': 3.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 7.0, 'spark.default.parallelism': 344.0, 'runtime': 1502.8517}
{'spark.kryoserializer.buffer': 117.0, 'spark.sql.autoBroadcastJoinThreshold': 13639741.0, 'spark.sql.shuffle.partitions': 290.0, 'spark.scheduler.revive.interval': 960.0, 'spark.executor.memoryOverhead': 697.0, 'spark.shuffle.file.buffer': 38.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 7.0, 'spark.default.parallelism': 420.0, 'runtime': 1503.2762}
{'spark.kryoserializer.buffer': 123.0, 'spark.sql.autoBroadcastJoinThreshold': 12329020.0, 'spark.sql.shuffle.partitions': 290.0, 'spark.scheduler.revive.interval': 949.0, 'spark.executor.memoryOverhead': 609.0, 'spark.shuffle.file.buffer': 38.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 5.0, 'spark.default.parallelism': 420.0, 'runtime': 1506.1025}
{'spark.kryoserializer.buffer': 117.0, 'spark.sql.autoBroadcastJoinThreshold': 13725185.0, 'spark.sql.shuffle.partitions': 300.0, 'spark.scheduler.revive.interval': 811.0, 'spark.executor.memoryOverhead': 697.0, 'spark.shuffle.file.buffer': 38.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 7.0, 'spark.default.parallelism': 420.0, 'runtime': 1503.276}
{'spark.kryoserializer.buffer': 108.0, 'spark.sql.autoBroadcastJoinThreshold': 9137665.0, 'spark.sql.shuffle.partitions': 300.0, 'spark.scheduler.revive.interval': 809.0, 'spark.executor.memoryOverhead': 835.0, 'spark.shuffle.file.buffer': 37.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 7.0, 'spark.executor.memory': 7.0, 'spark.default.parallelism': 223.0, 'runtime': 1497.551}
{'spark.kryoserializer.buffer': 126.0, 'spark.sql.autoBroadcastJoinThreshold': 9098001.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 621.0, 'spark.executor.memoryOverhead': 839.0, 'spark.shuffle.file.buffer': 46.0, 'spark.executor.cores': 3.0, 'spark.executor.instances': 8.0, 'spark.executor.memory': 7.0, 'spark.default.parallelism': 223.0, 'runtime': 1472.8466}
{'spark.kryoserializer.buffer': 126.0, 'spark.sql.autoBroadcastJoinThreshold': 9098001.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 621.0, 'spark.executor.memoryOverhead': 839.0, 'spark.shuffle.file.buffer': 46.0, 'spark.executor.cores': 3.0, 'spark.executor.instances': 8.0, 'spark.executor.memory': 7.0, 'spark.default.parallelism': 223.0, 'runtime': 1472.8466}
{'spark.kryoserializer.buffer': 108.0, 'spark.sql.autoBroadcastJoinThreshold': 9096869.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 621.0, 'spark.executor.memoryOverhead': 833.0, 'spark.shuffle.file.buffer': 37.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 7.0, 'spark.executor.memory': 7.0, 'spark.default.parallelism': 222.0, 'runtime': 1472.7554}
{'spark.kryoserializer.buffer': 108.0, 'spark.sql.autoBroadcastJoinThreshold': 9096869.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 621.0, 'spark.executor.memoryOverhead': 835.0, 'spark.shuffle.file.buffer': 37.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 7.0, 'spark.executor.memory': 5.0, 'spark.default.parallelism': 298.0, 'runtime': 1472.7554}
{'spark.kryoserializer.buffer': 108.0, 'spark.sql.autoBroadcastJoinThreshold': 9096869.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 621.0, 'spark.executor.memoryOverhead': 697.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 5.0, 'spark.default.parallelism': 298.0, 'runtime': 1472.7554}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8482388.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0103}
{'spark.kryoserializer.buffer': 128.0, 'spark.sql.autoBroadcastJoinThreshold': 8482388.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0103}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 9093589.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0103}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 9094868.0, 'spark.sql.shuffle.partitions': 258.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0103}
{'spark.kryoserializer.buffer': 110.0, 'spark.sql.autoBroadcastJoinThreshold': 8482388.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8482388.0, 'spark.sql.shuffle.partitions': 259.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 354.0, 'runtime': 1469.0784}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8482388.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 3.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8482388.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 820.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8482388.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 820.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 113.0, 'spark.sql.autoBroadcastJoinThreshold': 8487508.0, 'spark.sql.shuffle.partitions': 295.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 113.0, 'spark.sql.autoBroadcastJoinThreshold': 8482368.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 820.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8479785.0, 'spark.sql.shuffle.partitions': 295.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 704.0, 'spark.shuffle.file.buffer': 48.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 5.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8482388.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 820.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 113.0, 'spark.sql.autoBroadcastJoinThreshold': 8487468.0, 'spark.sql.shuffle.partitions': 295.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8482389.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 820.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 5.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 113.0, 'spark.sql.autoBroadcastJoinThreshold': 8487468.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 828.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 113.0, 'spark.sql.autoBroadcastJoinThreshold': 8815188.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 830.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 6.0, 'spark.executor.memory': 3.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 125.0, 'spark.sql.autoBroadcastJoinThreshold': 8487508.0, 'spark.sql.shuffle.partitions': 295.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 845.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 5.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8487478.0, 'spark.sql.shuffle.partitions': 295.0, 'spark.scheduler.revive.interval': 680.0, 'spark.executor.memoryOverhead': 845.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 1.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 5.0, 'spark.default.parallelism': 367.0, 'runtime': 1470.0078}
{'spark.kryoserializer.buffer': 116.0, 'spark.sql.autoBroadcastJoinThreshold': 8486825.0, 'spark.sql.shuffle.partitions': 296.0, 'spark.scheduler.revive.interval': 649.0, 'spark.executor.memoryOverhead': 820.0, 'spark.shuffle.file.buffer': 44.0, 'spark.executor.cores': 2.0, 'spark.executor.instances': 5.0, 'spark.executor.memory': 4.0, 'spark.default.parallelism': 368.0, 'runtime': 1470.0078}
